{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f019663a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd80e337a60b482eba26b3327e6233d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8904899461c846f8ac8d53499dae293f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train 4.7390, val 3.5797\n",
      "Generated:  FINDINGS: Noative change change both. jointsopen \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm import create_model\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Logging: INFO+ to training.log\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) Your FinalSamplesDataset (verbatim)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform, patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token  = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        input_text = f\"{self.tokenizer.bos_token} FINDINGS: {clean} {self.tokenizer.eos_token}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        input_ids      = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids: {input_ids.shape}, attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0 = max(x-crop_size[0]//2,0); y0 = max(y-crop_size[1]//2,0)\n",
    "                    x1 = min(x+crop_size[0]//2,arr.shape[1]); y1 = min(y+crop_size[1]//2,arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size: lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17: lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left,right = [],[]\n",
    "        if len(paths)==1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth): return pad17([])+pad17([])\n",
    "            arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side,pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        if side=='left': left=lst\n",
    "                        else:            right=lst\n",
    "\n",
    "        if left and not right: right=[cv2.flip(p,1) for p in left]\n",
    "        if right and not left: left =[cv2.flip(p,1) for p in right]\n",
    "        if not left and not right: return pad17([])+pad17([])\n",
    "\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_',' ')\n",
    "        return re.sub(r'\\s+',' ', text).strip()\n",
    "\n",
    "# prepare_data function for binary balancing\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    from collections import defaultdict\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in data.values():\n",
    "        lbl = entry.get('class_label','').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl]+=1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    if is_binary:\n",
    "        class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "        min_count = min(class_counts.values())\n",
    "        combined = []\n",
    "        for lbl in target_classes:\n",
    "            combined += random.sample(data_by_class[lbl], min_count)\n",
    "        return combined, class_counts, class_counts\n",
    "    else:\n",
    "        return list(data.values()), {}, {}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) Transforms\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) Collate fn\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    # pad the patch sequences\n",
    "    patches = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in patches)\n",
    "    padded = []\n",
    "    for p in patches:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        padded.append(p)\n",
    "    patches = torch.stack(padded, 0)\n",
    "\n",
    "    # gather inputs\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "\n",
    "    # build shifted labels without in-place assignment\n",
    "    labels = []\n",
    "    for i in ids:\n",
    "        seq = i.tolist()            # full list of token IDs, e.g. [BOS, ..., EOS]\n",
    "        shifted = seq[1:] + [-100]  # drop the first, pad last with -100\n",
    "        labels.append(torch.tensor(shifted, dtype=torch.long))\n",
    "\n",
    "    # pad them all\n",
    "    input_ids = nn.utils.rnn.pad_sequence(ids,    batch_first=True,\n",
    "                                          padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence(masks, batch_first=True,\n",
    "                                               padding_value=0)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True,\n",
    "                                       padding_value=-100)\n",
    "\n",
    "    return imgs, patches, input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) VisionGPT2Model + submodules\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size*self.n_heads*3)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "        self.c_proj= nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "    def forward(self,x):\n",
    "        b,t,c = x.size()\n",
    "        q,k,v = self.c_attn(x).chunk(3,-1)\n",
    "        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = (q@k.transpose(-2,-1))*self.scale\n",
    "        att = att.masked_fill(self.mask[:,:,:t,:t]==0, float('-inf'))\n",
    "        att = F.softmax(att,-1); att = self.attn_dropout(att)\n",
    "        y = att@v; y = y.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.q= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.k= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.c_proj= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.normal_(m.weight,0,0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    def forward(self, q,k,v):\n",
    "        b,qt,_=q.size()\n",
    "        q_ = self.q(q).view(b,qt,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k_ = self.k(k).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v_ = self.v(v).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = F.softmax((q_@k_.transpose(-2,-1))*self.scale, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att@v_; y = y.permute(0,2,1,3).contiguous().view(b,qt,self.embed_dim)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.embed_dim, config.embed_dim*config.mlp_ratio)\n",
    "        self.act=nn.GELU()\n",
    "        self.c_proj=nn.Linear(config.embed_dim*config.mlp_ratio, config.embed_dim)\n",
    "        self.dropout=nn.Dropout(config.mlp_dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln_1=nn.LayerNorm(config.embed_dim)\n",
    "        self.attn=GPT2Attention(config)\n",
    "        self.ln_2=nn.LayerNorm(config.embed_dim)\n",
    "        self.cross=GPT2CrossAttention(config)\n",
    "        self.ln_3=nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp=GPT2MLP(config)\n",
    "    def forward(self,x,enc):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross(self.ln_2(x), enc, enc)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x\n",
    "\n",
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        vit = create_model('vit_base_patch16_224',pretrained=True,num_classes=0)\n",
    "        self.patch_embed=vit.patch_embed\n",
    "        self.cls_token=vit.cls_token\n",
    "        self.pos_embed=vit.pos_embed\n",
    "        self.pos_drop=nn.Dropout(0.)\n",
    "        self.vit_blocks=vit.blocks[:config.depth]\n",
    "\n",
    "        self.config=config\n",
    "        self.wte=nn.Embedding(config.vocab_size,config.embed_dim)\n",
    "        self.wpe=nn.Embedding(config.seq_len,config.embed_dim)\n",
    "        self.drop=nn.Dropout(config.emb_dropout)\n",
    "        self.h=nn.ModuleList([GPT2Block(config) for _ in range(config.depth)])\n",
    "        self.ln_f=nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head=nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.wte.weight\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,config):\n",
    "        model=cls(config)\n",
    "        hf=GPT2LMHeadModel.from_pretrained('gpt2').state_dict()\n",
    "        sd=model.state_dict()\n",
    "        for k,v in hf.items():\n",
    "            if k in sd and sd[k].shape==v.shape:\n",
    "                sd[k].copy_(v)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "\n",
    "    def forward(self,image,input_ids,labels=None):\n",
    "        x=self.patch_embed(image)\n",
    "        x=torch.cat([self.cls_token.expand(x.size(0),-1,-1),x],1)\n",
    "        x=x+self.pos_embed; x=self.pos_drop(x)\n",
    "        for blk in self.vit_blocks: x=blk(x)\n",
    "\n",
    "        b,t=input_ids.size()\n",
    "        tok=self.wte(input_ids)\n",
    "        pos=torch.arange(t,device=input_ids.device).unsqueeze(0)\n",
    "        pos=self.wpe(pos)\n",
    "        h=self.drop(tok+pos)\n",
    "\n",
    "        for blk in self.h: h=blk(h,x)\n",
    "        h=self.ln_f(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits=self.lm_head(h)\n",
    "            loss=F.cross_entropy(lm_logits.view(-1,lm_logits.size(-1)),\n",
    "                                 labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return self.lm_head(h[:,-1,:])\n",
    "\n",
    "    def generate(self,image,seq,max_tokens=50,temperature=1.0,deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            logits=self(image,seq)/temperature\n",
    "            probs=F.softmax(logits,-1)\n",
    "            if deterministic:\n",
    "                nxt=torch.argmax(probs,dim=-1,keepdim=True)\n",
    "            else:\n",
    "                nxt=torch.multinomial(probs,1)\n",
    "            seq=torch.cat([seq,nxt],1)\n",
    "            if nxt.item()==tokenizer.eos_token_id: break\n",
    "        return seq\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) Trainer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class Trainer:\n",
    "    def __init__(self,model_config,train_config,dls):\n",
    "        self.device=train_config.device\n",
    "        self.model=VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        # freeze pretrained for first epochs\n",
    "        for p in self.model.parameters(): p.requires_grad=False\n",
    "        self.model.h.requires_grad_(True)\n",
    "\n",
    "        self.tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token=self.tokenizer.eos_token\n",
    "\n",
    "        self.optim=torch.optim.Adam(self.model.parameters(),lr=train_config.lr)\n",
    "        self.train_dl,self.val_dl=dls\n",
    "        self.train_config=train_config\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train(); total=0\n",
    "        for img,patches,ids,mask,labels in tqdm(self.train_dl,desc=\"Train\"):\n",
    "            img,ids,labels=img.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            loss=self.model(img,ids,labels)\n",
    "            self.optim.zero_grad(); loss.backward(); self.optim.step()\n",
    "            total+=loss.item()\n",
    "        return total/len(self.train_dl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self):\n",
    "        self.model.eval(); total=0\n",
    "        for img,patches,ids,mask,labels in tqdm(self.val_dl,desc=\"Val\"):\n",
    "            img,ids,labels=img.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            total+=self.model(img,ids,labels).item()\n",
    "        return total/len(self.val_dl)\n",
    "\n",
    "    def fit(self):\n",
    "        best=1e9\n",
    "        os.makedirs(self.train_config.model_path,exist_ok=True)\n",
    "        for ep in range(self.train_config.epochs):\n",
    "            tl=self.train_one_epoch(); vl=self.valid_one_epoch()\n",
    "            print(f\"Epoch {ep}: train {tl:.4f}, val {vl:.4f}\")\n",
    "            if vl<best:\n",
    "                best=vl\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.train_config.model_path,'best.pt'))\n",
    "\n",
    "    def generate_caption(self,image_path,temp=1.0,det=False):\n",
    "        seq=torch.tensor([[self.tokenizer.bos_token_id]],\n",
    "                         device=self.device)\n",
    "        img=np.array(Image.open(image_path).convert('RGB'))\n",
    "        img=train_transform(Image.fromarray(img)).unsqueeze(0).to(self.device)\n",
    "        out=self.model.generate(img,seq,temperature=temp,deterministic=det)\n",
    "        return self.tokenizer.decode(out[0].cpu(),skip_special_tokens=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) Main\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if __name__==\"__main__\":\n",
    "    model_config=SimpleNamespace(\n",
    "        vocab_size=50257, embed_dim=768, num_heads=12,\n",
    "        seq_len=512, depth=12,\n",
    "        attention_dropout=0.1, residual_dropout=0.1,\n",
    "        mlp_ratio=4, mlp_dropout=0.1, emb_dropout=0.1\n",
    "    )\n",
    "    train_config=SimpleNamespace(\n",
    "        epochs=1,\n",
    "        lr=1e-4,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model_path='captioner'\n",
    "    )\n",
    "\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "    cfg=SimpleNamespace(); cfg.DATASET=SimpleNamespace()\n",
    "    cfg.DATASET.JSON='final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.TARGET_CLASSES=['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE=False; cfg.DATASET.AUGMENT=False\n",
    "\n",
    "    ds=FinalSamplesDataset(cfg, image_transform=train_transform, patch_transform=patch_transform)\n",
    "    ds.tokenizer=tokenizer; ds.eos_token=tokenizer.eos_token\n",
    "\n",
    "    n=len(ds); n_train=int(0.8*n); n_val=int(0.1*n)\n",
    "    train_ds,val_ds,_=random_split(ds,[n_train,n_val,n-n_train-n_val])\n",
    "\n",
    "    train_dl=DataLoader(train_ds,batch_size=8,shuffle=True,collate_fn=collate_fn,pin_memory=True)\n",
    "    val_dl  =DataLoader(val_ds,  batch_size=8,shuffle=False,collate_fn=collate_fn,pin_memory=True)\n",
    "\n",
    "    trainer=Trainer(model_config,train_config,(train_dl,val_dl))\n",
    "    trainer.fit()\n",
    "\n",
    "    # example\n",
    "    example_path = ds.data[0]['file_path']\n",
    "    print(\"Generated:\", trainer.generate_caption(example_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb5df42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d7eae6e91244d1ab0f1da79d83dda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f5ad12f16049239febf3f181486875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 — train 4.5818, val 3.4920\n",
      "\n",
      "=== 20 Random Test Examples ===\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 515\u001b[0m\n\u001b[1;32m    513\u001b[0m item \u001b[38;5;241m=\u001b[39m test_ds[idx]\n\u001b[1;32m    514\u001b[0m actual \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_report\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 515\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    516\u001b[0m generated \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mgenerate_caption(img_path, temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, det\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file_path'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm import create_model\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Logging: INFO+ to training.log\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) Your FinalSamplesDataset (verbatim)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform, patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token  = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        input_text = f\"{self.tokenizer.bos_token} FINDINGS: {clean} {self.tokenizer.eos_token}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        input_ids      = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids: {input_ids.shape}, attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0 = max(x-crop_size[0]//2,0); y0 = max(y-crop_size[1]//2,0)\n",
    "                    x1 = min(x+crop_size[0]//2,arr.shape[1]); y1 = min(y+crop_size[1]//2,arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size: lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17: lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left,right = [],[]\n",
    "        if len(paths)==1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth): return pad17([])+pad17([])\n",
    "            arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side,pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        if side=='left': left=lst\n",
    "                        else:            right=lst\n",
    "\n",
    "        if left and not right: right=[cv2.flip(p,1) for p in left]\n",
    "        if right and not left: left =[cv2.flip(p,1) for p in right]\n",
    "        if not left and not right: return pad17([])+pad17([])\n",
    "\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_',' ')\n",
    "        return re.sub(r'\\s+',' ', text).strip()\n",
    "\n",
    "# prepare_data function for binary balancing\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in data.values():\n",
    "        lbl = entry.get('class_label','').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl]+=1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    if is_binary:\n",
    "        class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "        min_count = min(class_counts.values())\n",
    "        combined = []\n",
    "        for lbl in target_classes:\n",
    "            combined += random.sample(data_by_class[lbl], min_count)\n",
    "        return combined, class_counts, class_counts\n",
    "    else:\n",
    "        return list(data.values()), {}, {}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) Transforms\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) Collate fn\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    # pad the patch sequences\n",
    "    patches = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in patches)\n",
    "    padded = []\n",
    "    for p in patches:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        padded.append(p)\n",
    "    patches = torch.stack(padded, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "\n",
    "    labels = []\n",
    "    for i in ids:\n",
    "        seq = i.tolist()\n",
    "        shifted = seq[1:] + [-100]\n",
    "        labels.append(torch.tensor(shifted, dtype=torch.long))\n",
    "\n",
    "    input_ids = nn.utils.rnn.pad_sequence(ids,    batch_first=True,\n",
    "                                          padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence(masks, batch_first=True,\n",
    "                                               padding_value=0)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True,\n",
    "                                       padding_value=-100)\n",
    "\n",
    "    return imgs, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) VisionGPT2Model + submodules\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size*self.n_heads*3)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "        self.c_proj= nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "    def forward(self,x):\n",
    "        b,t,c = x.size()\n",
    "        q,k,v = self.c_attn(x).chunk(3,-1)\n",
    "        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = (q@k.transpose(-2,-1))*self.scale\n",
    "        att = att.masked_fill(self.mask[:,:,:t,:t]==0, float('-inf'))\n",
    "        att = F.softmax(att,-1); att = self.attn_dropout(att)\n",
    "        y = att@v; y = y.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.q= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.k= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.c_proj= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.normal_(m.weight,0,0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    def forward(self, q,k,v):\n",
    "        b,qt,_=q.size()\n",
    "        q_ = self.q(q).view(b,qt,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k_ = self.k(k).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v_ = self.v(v).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = F.softmax((q_@k_.transpose(-2,-1))*self.scale, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att@v_; y = y.permute(0,2,1,3).contiguous().view(b,qt,self.embed_dim)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.embed_dim, config.embed_dim*config.mlp_ratio)\n",
    "        self.act=nn.GELU()\n",
    "        self.c_proj=nn.Linear(config.embed_dim*config.mlp_ratio, config.embed_dim)\n",
    "        self.dropout=nn.Dropout(config.mlp_dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln_1=nn.LayerNorm(config.embed_dim)\n",
    "        self.attn=GPT2Attention(config)\n",
    "        self.ln_2=nn.LayerNorm(config.embed_dim)\n",
    "        self.cross=GPT2CrossAttention(config)\n",
    "        self.ln_3=nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp=GPT2MLP(config)\n",
    "    def forward(self,x,enc):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross(self.ln_2(x), enc, enc)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x\n",
    "\n",
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        vit = create_model('vit_base_patch16_224',pretrained=True,num_classes=0)\n",
    "        self.patch_embed=vit.patch_embed\n",
    "        self.cls_token=vit.cls_token\n",
    "        self.pos_embed=vit.pos_embed\n",
    "        self.pos_drop=nn.Dropout(0.)\n",
    "        self.vit_blocks=vit.blocks[:config.depth]\n",
    "\n",
    "        self.config=config\n",
    "        self.wte=nn.Embedding(config.vocab_size,config.embed_dim)\n",
    "        self.wpe=nn.Embedding(config.seq_len,config.embed_dim)\n",
    "        self.drop=nn.Dropout(config.emb_dropout)\n",
    "        self.h=nn.ModuleList([GPT2Block(config) for _ in range(config.depth)])\n",
    "        self.ln_f=nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head=nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.wte.weight\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,config):\n",
    "        model=cls(config)\n",
    "        hf=GPT2LMHeadModel.from_pretrained('gpt2').state_dict()\n",
    "        sd=model.state_dict()\n",
    "        for k,v in hf.items():\n",
    "            if k in sd and sd[k].shape==v.shape:\n",
    "                sd[k].copy_(v)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "\n",
    "    def forward(self,image,input_ids,labels=None):\n",
    "        x=self.patch_embed(image)\n",
    "        x=torch.cat([self.cls_token.expand(x.size(0),-1,-1),x],1)\n",
    "        x=x+self.pos_embed; x=self.pos_drop(x)\n",
    "        for blk in self.vit_blocks: x=blk(x)\n",
    "\n",
    "        b,t=input_ids.size()\n",
    "        tok=self.wte(input_ids)\n",
    "        pos=torch.arange(t,device=input_ids.device).unsqueeze(0)\n",
    "        pos=self.wpe(pos)\n",
    "        h=self.drop(tok+pos)\n",
    "\n",
    "        for blk in self.h: h=blk(h,x)\n",
    "        h=self.ln_f(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits=self.lm_head(h)\n",
    "            loss=F.cross_entropy(lm_logits.view(-1,lm_logits.size(-1)),\n",
    "                                 labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return self.lm_head(h[:,-1,:])\n",
    "\n",
    "    def generate(self,image,seq,max_tokens=50,temperature=1.0,deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            logits=self(image,seq)/temperature\n",
    "            probs=F.softmax(logits,-1)\n",
    "            if deterministic:\n",
    "                nxt=torch.argmax(probs,dim=-1,keepdim=True)\n",
    "            else:\n",
    "                nxt=torch.multinomial(probs,1)\n",
    "            seq=torch.cat([seq,nxt],1)\n",
    "            if nxt.item()==tokenizer.eos_token_id: break\n",
    "        return seq\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) Trainer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class Trainer:\n",
    "    def __init__(self,model_config,train_config,dls):\n",
    "        self.device=train_config.device\n",
    "        self.model=VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        # freeze pretrained for first epochs\n",
    "        for p in self.model.parameters(): p.requires_grad=False\n",
    "        self.model.h.requires_grad_(True)\n",
    "\n",
    "        self.tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token=self.tokenizer.eos_token\n",
    "\n",
    "        self.optim=torch.optim.Adam(self.model.parameters(),lr=train_config.lr)\n",
    "        self.train_dl,self.val_dl=dls\n",
    "        self.train_config=train_config\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.train_dl,desc=\"Train\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            loss=self.model(imgs,ids,labels)\n",
    "            self.optim.zero_grad(); loss.backward(); self.optim.step()\n",
    "            total+=loss.item()\n",
    "        return total/len(self.train_dl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self):\n",
    "        self.model.eval(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.val_dl,desc=\"Val\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            total+=self.model(imgs,ids,labels).item()\n",
    "        return total/len(self.val_dl)\n",
    "\n",
    "    def fit(self):\n",
    "        best=1e9\n",
    "        os.makedirs(self.train_config.model_path,exist_ok=True)\n",
    "        for ep in range(self.train_config.epochs):\n",
    "            tl=self.train_one_epoch(); vl=self.valid_one_epoch()\n",
    "            print(f\"Epoch {ep+1}/{self.train_config.epochs} — train {tl:.4f}, val {vl:.4f}\")\n",
    "            if vl<best:\n",
    "                best=vl\n",
    "                torch.save(self.model.state_dict(),\n",
    "                           os.path.join(self.train_config.model_path,'best.pt'))\n",
    "\n",
    "    def generate_caption(self,image_path,temp=1.0,det=False):\n",
    "        seq=torch.tensor([[self.tokenizer.bos_token_id]],\n",
    "                         device=self.device)\n",
    "        img=np.array(Image.open(image_path).convert('RGB'))\n",
    "        img=train_transform(Image.fromarray(img)).unsqueeze(0).to(self.device)\n",
    "        out=self.model.generate(img,seq,temperature=temp,deterministic=det)\n",
    "        return self.tokenizer.decode(out[0].cpu(),skip_special_tokens=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) Main\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if __name__==\"__main__\":\n",
    "    # configs\n",
    "    model_config=SimpleNamespace(\n",
    "        vocab_size=50257, embed_dim=768, num_heads=12,\n",
    "        seq_len=512, depth=12,\n",
    "        attention_dropout=0.1, residual_dropout=0.1,\n",
    "        mlp_ratio=4, mlp_dropout=0.1, emb_dropout=0.1\n",
    "    )\n",
    "    train_config=SimpleNamespace(\n",
    "        epochs=1,\n",
    "        lr=1e-4,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model_path='captioner'\n",
    "    )\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "    # dataset config\n",
    "    cfg=SimpleNamespace()\n",
    "    cfg.DATASET=SimpleNamespace()\n",
    "    cfg.DATASET.JSON='final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.TARGET_CLASSES=['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE=False\n",
    "    cfg.DATASET.AUGMENT=False\n",
    "\n",
    "    # dataset + splits\n",
    "    ds=FinalSamplesDataset(cfg,\n",
    "                           image_transform=train_transform,\n",
    "                           patch_transform=patch_transform)\n",
    "    ds.tokenizer=tokenizer\n",
    "    ds.eos_token=tokenizer.eos_token\n",
    "\n",
    "    n=len(ds)\n",
    "    n_train=int(0.8*n)\n",
    "    n_val=int(0.1*n)\n",
    "    n_test=n-n_train-n_val\n",
    "    train_ds,val_ds,test_ds=random_split(\n",
    "        ds,[n_train,n_val,n_test],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_dl=DataLoader(train_ds,batch_size=8,shuffle=True,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "    val_dl  =DataLoader(val_ds,  batch_size=8,shuffle=False,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "\n",
    "    # train\n",
    "    trainer=Trainer(model_config,train_config,(train_dl,val_dl))\n",
    "    trainer.fit()\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 7) Sample 20 random examples from test set\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    print(\"\\n=== 20 Random Test Examples ===\\n\")\n",
    "    random.seed(42)\n",
    "    sample_idxs = random.sample(range(len(test_ds)), min(20, len(test_ds)))\n",
    "    for idx in sample_idxs:\n",
    "        item = test_ds[idx]\n",
    "        actual = item['cleaned_report']\n",
    "        img_path = item['file_path']\n",
    "        generated = trainer.generate_caption(img_path, temp=1.0, det=False)\n",
    "        print(f\"--- Example {idx} ---\")\n",
    "        print(f\"Actual cleaned report : {actual}\")\n",
    "        print(f\"Generated report      : {generated}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f8729bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3144ae8014a1451ba839d4b80e76e1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e895b741aa48428ffb33cb4c26d274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — train 4.5675, val 3.5786\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ef8e8f2b5142bb830b241c7b4f898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a242a3562143bfb489bfde91b24bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 — train 3.4043, val 2.9481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dc4b3b6ae247c29d8e92b3f4144993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ebefbd814142d69d67d5a1fa539328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 — train 2.9327, val 2.5357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59b120e5c44416c99eea889f47c1fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2895d72d9f4e9fa15254dbced1d3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 — train 2.5817, val 2.3052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ca3aad5f7b406b9238199946c861c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b2378a5cad4ac3aec87ab22e7912b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 — train 2.3159, val 2.1831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615e527458bc430ab875a43c3f889688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7a1a68b6ad4fc8bb3e272b1c8e5c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 — train 2.1014, val 2.0583\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dd4d8f746d41a89c431781a16636e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4fcfc2eb5e4d64ac0c845103c7b75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 — train 1.9108, val 1.9986\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4253bc7b10444848ec4fdecd83e8502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be899daeeebb48bb8182391f2e4bb4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 — train 1.6864, val 1.9894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ae8a311ca141d9a0e4168c8b96f11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ba69d9727b4f2ba8ab45da90ea49e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 — train 1.5492, val 2.0059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2baf84db0a8469a9ec01ac76c2b75be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8323a6550e471a811782800808befb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 — train 1.3596, val 2.0892\n",
      "\n",
      "=== 20 Random Test Examples ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efe84e4091f425f88939991bbb28d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94229155d37494582b2ab579ffc27bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 163 (orig #399) ---\n",
      "Actual cleaned report : both hands, feet, and knees, FBs. both 3rd MT bone, erosive change. both hands, erosions. --> RA, suggested. Rt. knee RA involvement.\n",
      "Generated report      :  FINDINGS: RA inflammatory arthritis in RA 3lyatarsalosis, possible inflammatory at Lt 2nd MTP joint erosions. --> RAtes \n",
      "Semantic similarity   : 0.5571\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b85085b3464c38832e061d2c3fd451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a958d82e12e44381a2139e3d50c750b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 28 (orig #1421) ---\n",
      "Actual cleaned report : C.I.> left 1st MTP joint Joint space narrowing in the IP joint, left big toe --> Probable gout arthritis Joint space narrowing in the IP joint, left big toe --> Probable gout arthritis\n",
      "Generated report      :  FINDINGS: bone . soft tissue . No bony abnormality \n",
      "Semantic similarity   : 0.3156\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db09b562f4d4763b1f5cd6b8b8025dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11827142a324b848ee2fb086e28016e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 6 (orig #438) ---\n",
      "Actual cleaned report : no significant bony abnormality\n",
      "Generated report      :  FINDINGS: Rt. 1st MTP joint -> soft tissue swelling, --> Rtar, both 1st MTP joint -> a t ingoly strongly \n",
      "Semantic similarity   : 0.3414\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c364d4aed049aab2ea22b4c4fc18e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e144c1a36044989a39cf9ec08642eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 189 (orig #1485) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.9006\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de837a14e2845a7b9e54bc4f5a99992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738da823a68f4199ae7ec5ceaf5696d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 70 (orig #1203) ---\n",
      "Actual cleaned report : no significant bony lesion on radiographs. no significant bony lesion on radiographs.\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.2361\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5aa41691a54200a7b6f8783d04fe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8872b7adfbd46f0b3c8a4eaaa8407bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 62 (orig #2249) ---\n",
      "Actual cleaned report : degenerative change\n",
      "Generated report      :  FINDINGS: - mild degenerative change \n",
      "Semantic similarity   : 0.8014\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5084bf4b774f443db2804eab8d4a7ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba56cb90fa844474b48200fa9ec9ed57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 57 (orig #727) ---\n",
      "Actual cleaned report : No bony abnormality. No bony abnormality.\n",
      "Generated report      :  FINDINGS: bone . soft tissue . No bony abnormality \n",
      "Semantic similarity   : 0.7980\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9a680f3b5c44289026b9de20d2f670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9a60a962474c71b04c6b4e5f8cc65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 35 (orig #2192) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.9006\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecebedd2c9474d52a717f4fffd9c600c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64d90008a7048a2a2cca15d07b0fd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 188 (orig #1000) ---\n",
      "Actual cleaned report : No bony abnormality\n",
      "Generated report      :  FINDINGS: possible bone erosion, right 1st MTP joint. Possibleicious gout, Rt, \n",
      "Semantic similarity   : 0.4039\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a144d3515b4d48dbb92efe677f31ac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4774e041d3f3411b91672a14c20b31e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 26 (orig #1286) ---\n",
      "Actual cleaned report : both ankle OA. both knee, minimal OA Rt. 1st MTP joint, R/O gout.\n",
      "Generated report      :  FINDINGS: no significant bony abnormality \n",
      "Semantic similarity   : 0.2618\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4008cfb9cc447b49006f9ddd396eb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a06605327c842668a682e16817d37d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 173 (orig #923) ---\n",
      "Actual cleaned report : Tiny osteophyte of right 1st MTP joint. Subtle radiolucent lesion of rigth 1st proximal phalangeal bone base. --- suspicious subchondral cyst. Accessory navicular bone, both.\n",
      "Generated report      :  FINDINGS: No bony lesion. no bony lesion. \n",
      "Semantic similarity   : 0.5068\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d78acc5c2349cda41291b3fbc081b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cf1a89b1e945a1bafdd0998f66d9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 228 (orig #113) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: no significant bony lesion on radiographs. \n",
      "Semantic similarity   : 0.1811\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1fb162a3004f6d862de3ebe9f75b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee560e5396146edb874866f4b1256a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 139 (orig #1120) ---\n",
      "Actual cleaned report : degenerative change. osteopenia. degenerative change. osteopenia.\n",
      "Generated report      :  FINDINGS: both 1st MTP joint soft tissue swelling. --> gout arthritis. \n",
      "Semantic similarity   : 0.1737\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b66b772206840b28abb7c4ecba82d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcea532eb096499d8f7d02356eeafe3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 22 (orig #514) ---\n",
      "Actual cleaned report : mild degenerative change\n",
      "Generated report      :  FINDINGS: no significant bony lesion on radiographs. \n",
      "Semantic similarity   : 0.1880\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d345107ebc46b496e62beee903a414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cbebe5c8ab4a02b725b2184253f99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 151 (orig #217) ---\n",
      "Actual cleaned report : ulnar negative variance both Lt. 1st toe, corrective osteotomy.\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.2832\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eac8cd400449a9a93597be62e9186b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e5d28397fd4c31bc7259c9f8219d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 108 (orig #732) ---\n",
      "Actual cleaned report : Hallux valgus, right. Possible osteoarthritis in both 1st IP and right 1st, 2nd MTP joint. Osteophyte in talus neck, both.\n",
      "Generated report      :  FINDINGS: no bony lesion. \n",
      "Semantic similarity   : 0.4846\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0405049a0f496b90ba8bdc08acfdd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9a6733b8ca4cacbce7ab7e8273cae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 8 (orig #814) ---\n",
      "Actual cleaned report : No bony abnormality\n",
      "Generated report      :  FINDINGS: gout, both 1st MTP joints. \n",
      "Semantic similarity   : 0.2670\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a587b666751542e08ee8026ab569606c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2888dcd2e6304c36b8824958a5b571ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 7 (orig #2219) ---\n",
      "Actual cleaned report : No significant interval change\n",
      "Generated report      :  FINDINGS: joint space narrowing around erosions at Rt 1st MTP joint -> erosion. Lt 1st MTP joint -> Rt 1st MTP joint -> Lt -> possible -> -> r/o arthritis. : pl ->\n",
      "Semantic similarity   : 0.0888\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ef0bb2229b4409a767f6ec447f4ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57fe2fb10c04872bae80fd74c5c13c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 23 (orig #1698) ---\n",
      "Actual cleaned report : both feet, R/O gout\n",
      "Generated report      :  FINDINGS: old gout arthritis, post almost soft tissue swelling at early, schondral cystic 1st PP IP head \n",
      "Semantic similarity   : 0.3881\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87da4b187b844be93c97ed3db7daa13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d295b55c7e04213b97a18e2c5fdc487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 55 (orig #480) ---\n",
      "Actual cleaned report : mild soft tissue swelling, Rt lateral ankle\n",
      "Generated report      :  FINDINGS: no significant bony lesion on radiographs. \n",
      "Semantic similarity   : 0.2843\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm import create_model\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Logging: INFO+ to training.log\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) Your FinalSamplesDataset (verbatim)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform, patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token  = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        input_text = f\"{self.tokenizer.bos_token} FINDINGS: {clean} {self.tokenizer.eos_token}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        input_ids      = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids: {input_ids.shape}, attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0 = max(x-crop_size[0]//2,0); y0 = max(y-crop_size[1]//2,0)\n",
    "                    x1 = min(x+crop_size[0]//2,arr.shape[1]); y1 = min(y+crop_size[1]//2,arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size: lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17: lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left,right = [],[]\n",
    "        if len(paths)==1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth): return pad17([])+pad17([])\n",
    "            arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side,pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        if side=='left': left=lst\n",
    "                        else:            right=lst\n",
    "\n",
    "        if left and not right: right=[cv2.flip(p,1) for p in left]\n",
    "        if right and not left: left =[cv2.flip(p,1) for p in right]\n",
    "        if not left and not right: return pad17([])+pad17([])\n",
    "\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_',' ')\n",
    "        return re.sub(r'\\s+',' ', text).strip()\n",
    "\n",
    "# prepare_data for binary tasks\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in data.values():\n",
    "        lbl = entry.get('class_label','').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl]+=1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    if is_binary:\n",
    "        class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "        min_count = min(class_counts.values())\n",
    "        combined = []\n",
    "        for lbl in target_classes:\n",
    "            combined += random.sample(data_by_class[lbl], min_count)\n",
    "        return combined, class_counts, class_counts\n",
    "    else:\n",
    "        return list(data.values()), {}, {}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) Transforms\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) Collate fn\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    patches = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in patches)\n",
    "    padded = []\n",
    "    for p in patches:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        padded.append(p)\n",
    "    patches = torch.stack(padded, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "\n",
    "    labels = []\n",
    "    for i in ids:\n",
    "        seq = i.tolist()\n",
    "        shifted = seq[1:] + [-100]\n",
    "        labels.append(torch.tensor(shifted, dtype=torch.long))\n",
    "\n",
    "    input_ids = nn.utils.rnn.pad_sequence(ids,    batch_first=True,\n",
    "                                          padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence(masks, batch_first=True,\n",
    "                                               padding_value=0)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True,\n",
    "                                       padding_value=-100)\n",
    "\n",
    "    return imgs, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) VisionGPT2Model + submodules (same as reference)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size*self.n_heads*3)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "        self.c_proj= nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "    def forward(self,x):\n",
    "        b,t,c = x.size()\n",
    "        q,k,v = self.c_attn(x).chunk(3,-1)\n",
    "        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = (q@k.transpose(-2,-1))*self.scale\n",
    "        att = att.masked_fill(self.mask[:,:,:t,:t]==0, float('-inf'))\n",
    "        att = F.softmax(att,-1); att = self.attn_dropout(att)\n",
    "        y = att@v; y = y.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.q= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.k= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.c_proj= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.normal_(m.weight,0,0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    def forward(self, q,k,v):\n",
    "        b,qt,_=q.size()\n",
    "        q_ = self.q(q).view(b,qt,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k_ = self.k(k).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v_ = self.v(v).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = F.softmax((q_@k_.transpose(-2,-1))*self.scale, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att@v_; y = y.permute(0,2,1,3).contiguous().view(b,qt,self.embed_dim)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.embed_dim, config.embed_dim*config.mlp_ratio)\n",
    "        self.act=nn.GELU()\n",
    "        self.c_proj=nn.Linear(config.embed_dim*config.mlp_ratio, config.embed_dim)\n",
    "        self.dropout=nn.Dropout(config.mlp_dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln_1=nn.LayerNorm(config.embed_dim)\n",
    "        self.attn=GPT2Attention(config)\n",
    "        self.ln_2=nn.LayerNorm(config.embed_dim)\n",
    "        self.cross=GPT2CrossAttention(config)\n",
    "        self.ln_3=nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp=GPT2MLP(config)\n",
    "    def forward(self,x,enc):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross(self.ln_2(x), enc, enc)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x\n",
    "\n",
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        vit = create_model('vit_base_patch16_224',pretrained=True,num_classes=0)\n",
    "        self.patch_embed=vit.patch_embed\n",
    "        self.cls_token=vit.cls_token\n",
    "        self.pos_embed=vit.pos_embed\n",
    "        self.pos_drop=nn.Dropout(0.)\n",
    "        self.vit_blocks=vit.blocks[:config.depth]\n",
    "\n",
    "        self.config=config\n",
    "        self.wte=nn.Embedding(config.vocab_size,config.embed_dim)\n",
    "        self.wpe=nn.Embedding(config.seq_len,config.embed_dim)\n",
    "        self.drop=nn.Dropout(config.emb_dropout)\n",
    "        self.h=nn.ModuleList([GPT2Block(config) for _ in range(config.depth)])\n",
    "        self.ln_f=nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head=nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.wte.weight\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,config):\n",
    "        model=cls(config)\n",
    "        hf=GPT2LMHeadModel.from_pretrained('gpt2').state_dict()\n",
    "        sd=model.state_dict()\n",
    "        for k,v in hf.items():\n",
    "            if k in sd and sd[k].shape==v.shape:\n",
    "                sd[k].copy_(v)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "\n",
    "    def forward(self,image,input_ids,labels=None):\n",
    "        x=self.patch_embed(image)\n",
    "        x=torch.cat([self.cls_token.expand(x.size(0),-1,-1),x],1)\n",
    "        x=x+self.pos_embed; x=self.pos_drop(x)\n",
    "        for blk in self.vit_blocks: x=blk(x)\n",
    "\n",
    "        b,t=input_ids.size()\n",
    "        tok=self.wte(input_ids)\n",
    "        pos=torch.arange(t,device=input_ids.device).unsqueeze(0)\n",
    "        pos=self.wpe(pos)\n",
    "        h=self.drop(tok+pos)\n",
    "\n",
    "        for blk in self.h: h=blk(h,x)\n",
    "        h=self.ln_f(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits=self.lm_head(h)\n",
    "            loss=F.cross_entropy(lm_logits.view(-1,lm_logits.size(-1)),\n",
    "                                 labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return self.lm_head(h[:,-1,:])\n",
    "\n",
    "    def generate(self,image,seq,max_tokens=50,temperature=1.0,deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            logits=self(image,seq)/temperature\n",
    "            probs=F.softmax(logits,-1)\n",
    "            if deterministic:\n",
    "                nxt=torch.argmax(probs,dim=-1,keepdim=True)\n",
    "            else:\n",
    "                nxt=torch.multinomial(probs,1)\n",
    "            seq=torch.cat([seq,nxt],1)\n",
    "            if nxt.item()==tokenizer.eos_token_id: break\n",
    "        return seq\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) Trainer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class Trainer:\n",
    "    def __init__(self,model_config,train_config,dls):\n",
    "        self.device=train_config.device\n",
    "        self.model=VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        # freeze pretrained for first epochs\n",
    "        for p in self.model.parameters(): p.requires_grad=False\n",
    "        self.model.h.requires_grad_(True)\n",
    "\n",
    "        self.tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token=self.tokenizer.eos_token\n",
    "\n",
    "        self.optim=torch.optim.Adam(self.model.parameters(),lr=train_config.lr)\n",
    "        self.train_dl,self.val_dl=dls\n",
    "        self.train_config=train_config\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.train_dl,desc=\"Train\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            loss=self.model(imgs,ids,labels)\n",
    "            self.optim.zero_grad(); loss.backward(); self.optim.step()\n",
    "            total+=loss.item()\n",
    "        return total/len(self.train_dl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self):\n",
    "        self.model.eval(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.val_dl,desc=\"Val\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            total+=self.model(imgs,ids,labels).item()\n",
    "        return total/len(self.val_dl)\n",
    "\n",
    "    def fit(self):\n",
    "        best=1e9\n",
    "        os.makedirs(self.train_config.model_path,exist_ok=True)\n",
    "        for ep in range(self.train_config.epochs):\n",
    "            tl=self.train_one_epoch(); vl=self.valid_one_epoch()\n",
    "            print(f\"Epoch {ep+1}/{self.train_config.epochs} — train {tl:.4f}, val {vl:.4f}\")\n",
    "            if vl<best:\n",
    "                best=vl\n",
    "                torch.save(self.model.state_dict(),\n",
    "                           os.path.join(self.train_config.model_path,'best.pt'))\n",
    "\n",
    "    def generate_caption(self,image_path,temp=1.0,det=False):\n",
    "        seq=torch.tensor([[self.tokenizer.bos_token_id]],\n",
    "                         device=self.device)\n",
    "        img=np.array(Image.open(image_path).convert('RGB'))\n",
    "        img=train_transform(Image.fromarray(img)).unsqueeze(0).to(self.device)\n",
    "        out=self.model.generate(img,seq,temperature=temp,deterministic=det)\n",
    "        return self.tokenizer.decode(out[0].cpu(),skip_special_tokens=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) Main\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if __name__==\"__main__\":\n",
    "    # configs\n",
    "    model_config=SimpleNamespace(\n",
    "        vocab_size=50257, embed_dim=768, num_heads=12,\n",
    "        seq_len=512, depth=12,\n",
    "        attention_dropout=0.1, residual_dropout=0.1,\n",
    "        mlp_ratio=4, mlp_dropout=0.1, emb_dropout=0.1\n",
    "    )\n",
    "    train_config=SimpleNamespace(\n",
    "        epochs=10,\n",
    "        lr=1e-4,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model_path='captioner'\n",
    "    )\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "    # dataset config\n",
    "    cfg=SimpleNamespace()\n",
    "    cfg.DATASET=SimpleNamespace()\n",
    "    cfg.DATASET.JSON='final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.TARGET_CLASSES=['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE=False\n",
    "    cfg.DATASET.AUGMENT=False\n",
    "\n",
    "    # dataset + splits\n",
    "    ds=FinalSamplesDataset(cfg,\n",
    "                           image_transform=train_transform,\n",
    "                           patch_transform=patch_transform)\n",
    "    ds.tokenizer=tokenizer\n",
    "    ds.eos_token=tokenizer.eos_token\n",
    "\n",
    "    n=len(ds)\n",
    "    n_train=int(0.8*n)\n",
    "    n_val=int(0.1*n)\n",
    "    n_test=n-n_train-n_val\n",
    "    train_ds,val_ds,test_ds=random_split(\n",
    "        ds,[n_train,n_val,n_test],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_dl=DataLoader(train_ds,batch_size=8,shuffle=True,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "    val_dl  =DataLoader(val_ds,  batch_size=8,shuffle=False,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "\n",
    "    # train\n",
    "    trainer=Trainer(model_config,train_config,(train_dl,val_dl))\n",
    "    trainer.fit()\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 7) Sample 20 random examples from test set + compute semantic sim\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    print(\"\\n=== 20 Random Test Examples ===\\n\")\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    random.seed(42)\n",
    "    sample_idxs = random.sample(range(len(test_ds)), min(20, len(test_ds)))\n",
    "    for idx in sample_idxs:\n",
    "        # original dataset index\n",
    "        orig_idx = test_ds.indices[idx]\n",
    "        # get cleaned report\n",
    "        item = test_ds[idx]  # this returns the dict from __getitem__\n",
    "        actual = item['cleaned_report']\n",
    "        # generate\n",
    "        img_path = ds.data[orig_idx]['file_path']\n",
    "        generated = trainer.generate_caption(img_path, temp=1.0, det=False)\n",
    "        # semantic similarity\n",
    "        emb_actual = stm.encode(actual, convert_to_tensor=True)\n",
    "        emb_gen    = stm.encode(generated, convert_to_tensor=True)\n",
    "        sim = F.cosine_similarity(emb_actual, emb_gen, dim=0).item()\n",
    "        print(f\"--- Example {idx} (orig #{orig_idx}) ---\")\n",
    "        print(f\"Actual cleaned report : {actual}\")\n",
    "        print(f\"Generated report      : {generated}\")\n",
    "        print(f\"Semantic similarity   : {sim:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e29c7001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7995e14683b42c18cb09a46ea018e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808fa725d66b4d178c716e758759f25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — train 4.6282, val 3.5355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba4ce7aa52d4d28afbb5ebc0b4b5097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8328630c645e4ecf97d3bcc0fb65d479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 — train 3.3952, val 2.8617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78452213d1542a7b2126c352e91be47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9666850d593d43cab162cb6cf696a9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 — train 2.9166, val 2.5522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d96453eaee4564b253c5e40e74b8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3481f8a7df354dadbb123dcfe8a7eb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 — train 2.5812, val 2.3379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbcdbf2b1374f26b308d5be2e9bfbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16556ba4d70541f4bf4be216a7ac7435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 — train 2.3017, val 2.2213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad21259f5414f11a1b0e6765926e98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680eb59382a343e397f53d3b0110646d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 — train 2.0991, val 2.0864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0137cbac00ea4b8e820371ff96df4c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b754b4a43a604a72b0ddd94be62216e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 — train 1.8931, val 2.0628\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72fdc58f090496fb7e131c920349898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a61ede035341ef80682145c9482e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 — train 1.7060, val 1.9804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc5f9d6438a4b86b8a90036af3e1dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257d536485ec460997542f62ca8640d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 — train 1.5128, val 2.0403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebba2c9e0113402791c01fbe2d81a9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab31895d48747849eedf260560c54ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 — train 1.3749, val 2.1235\n",
      "\n",
      "=== 20 Random Test Examples ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5837b0249edc4e86a9a063ff8196179b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020299d0a8a5454db5c94dea192d3b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 163 (orig #399) ---\n",
      "Actual cleaned report : both hands, feet, and knees, FBs. both 3rd MT bone, erosive change. both hands, erosions. --> RA, suggested. Rt. knee RA involvement.\n",
      "Generated report      :  FINDINGS: both feet, ul, pes cav \n",
      "Semantic similarity   : 0.5029\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14dcc620f0840e1b8d606cb17c2728e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bd00f9b24849bab507f0d40278dab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 28 (orig #1421) ---\n",
      "Actual cleaned report : C.I.> left 1st MTP joint Joint space narrowing in the IP joint, left big toe --> Probable gout arthritis Joint space narrowing in the IP joint, left big toe --> Probable gout arthritis\n",
      "Generated report      :  FINDINGS: small OA, both knee joints. Lt 1st MTP joint, \n",
      "Semantic similarity   : 0.5668\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb9ab4d93c34332be24d8dec471429b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f506d99ae6943768afbff24e1abe7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 6 (orig #438) ---\n",
      "Actual cleaned report : no significant bony abnormality\n",
      "Generated report      :  FINDINGS: - joint space narrowing swelling. Lt. e 3st MTP joint, l ankle arthritis R/O gout arthritis OA arthritis arthritis. \n",
      "Semantic similarity   : 0.3876\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8734d15d4e0c4997ae2e106e56613de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee41a00454434e0ea091ddc542758493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 189 (orig #1485) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: no bony lesion. \n",
      "Semantic similarity   : 0.2340\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f89a9992efa4018a3d6c5f9f5add614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26089e5509e64714ac4485ae0cde2c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 70 (orig #1203) ---\n",
      "Actual cleaned report : no significant bony lesion on radiographs. no significant bony lesion on radiographs.\n",
      "Generated report      :  FINDINGS: . perros. left left wrists. \n",
      "Semantic similarity   : 0.2701\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b08eec3071343ea8068b6b6188731b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ab49a8ca13437c819ee23b77c46603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 62 (orig #2249) ---\n",
      "Actual cleaned report : degenerative change\n",
      "Generated report      :  FINDINGS: diffuse osteopenia \n",
      "Semantic similarity   : 0.2185\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f876651e283c486aa6205a179bb4b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac4e2528ed147ec868b0408a0ce6ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 57 (orig #727) ---\n",
      "Actual cleaned report : No bony abnormality. No bony abnormality.\n",
      "Generated report      :  FINDINGS: No bony abnormality. \n",
      "Semantic similarity   : 0.8773\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30add3ebaee94322a3d0110ae9a68658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0723805188dd4ed7bb68eeff2fee38b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 35 (orig #2192) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: no bony abnormality \n",
      "Semantic similarity   : 0.2654\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8861d4dec349c68bb52beec977b87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9ec4fcf0164b0e877e85e30bc1f11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 188 (orig #1000) ---\n",
      "Actual cleaned report : No bony abnormality\n",
      "Generated report      :  FINDINGS: bone . soft tissue . No bony abnormality \n",
      "Semantic similarity   : 0.8238\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8ca083015b4bb892d64db8047de9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788a03830e9f487da26b1e3bc69ccfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 26 (orig #1286) ---\n",
      "Actual cleaned report : both ankle OA. both knee, minimal OA Rt. 1st MTP joint, R/O gout.\n",
      "Generated report      :  FINDINGS: degenerative change, foot, ankle. \n",
      "Semantic similarity   : 0.4371\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bdeed5ed924e1f84927d3b7c22eb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a996352e5d4440a359b3d638ae8f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 173 (orig #923) ---\n",
      "Actual cleaned report : Tiny osteophyte of right 1st MTP joint. Subtle radiolucent lesion of rigth 1st proximal phalangeal bone base. --- suspicious subchondral cyst. Accessory navicular bone, both.\n",
      "Generated report      :  FINDINGS: severe joint space narrowing with 1st MTP joint \n",
      "Semantic similarity   : 0.5772\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24eb8e3d877434896a5197e88f259c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ee5e0e49b2437da17ea5a2ef62a63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 228 (orig #113) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: . Rectux valgus D right \n",
      "Semantic similarity   : 0.2427\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269e35a40d324b6eb8d8d197c62dd522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e6130623e445efb20485c04dcf0d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 139 (orig #1120) ---\n",
      "Actual cleaned report : degenerative change. osteopenia. degenerative change. osteopenia.\n",
      "Generated report      :  FINDINGS: no bony abnormality \n",
      "Semantic similarity   : 0.3603\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f75a62c812041609691e1c042eb3943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c875955b4ba46d8b4c2bf1214ba3559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 22 (orig #514) ---\n",
      "Actual cleaned report : mild degenerative change\n",
      "Generated report      :  FINDINGS: degenerative change, foot OA \n",
      "Semantic similarity   : 0.6747\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e374484792642a0b7e6a9407baeac02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74019fa7f5714136a96922710efdbc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 151 (orig #217) ---\n",
      "Actual cleaned report : ulnar negative variance both Lt. 1st toe, corrective osteotomy.\n",
      "Generated report      :  FINDINGS: . ------------------------------------------------------------------------ No bony abnormality. \n",
      "Semantic similarity   : 0.4856\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc620a2d41f4b119c664170722f800e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b5df0cfaa34894b1293e4104a97e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 108 (orig #732) ---\n",
      "Actual cleaned report : Hallux valgus, right. Possible osteoarthritis in both 1st IP and right 1st, 2nd MTP joint. Osteophyte in talus neck, both.\n",
      "Generated report      :  FINDINGS: both ankle soft tissue swelling. \n",
      "Semantic similarity   : 0.3932\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9b4e5bf49a4196a2a5887b1ee28084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dc2317341f457d917e79b8d89e59b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 8 (orig #814) ---\n",
      "Actual cleaned report : No bony abnormality\n",
      "Generated report      :  FINDINGS: Both 1st MTP joint \n",
      "Semantic similarity   : 0.2987\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913416a5e765422581ab2e0265dcf2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5126de88d9e4797ace9b75927915034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 7 (orig #2219) ---\n",
      "Actual cleaned report : No significant interval change\n",
      "Generated report      :  FINDINGS: - flat degenerative change \n",
      "Semantic similarity   : 0.1486\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cc24a963384f938629c35b5b3b594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a76fff9e1d644f1af97c0c45f4c166c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 23 (orig #1698) ---\n",
      "Actual cleaned report : both feet, R/O gout\n",
      "Generated report      :  FINDINGS: no bony abnormality \n",
      "Semantic similarity   : 0.1776\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c930d51c0be74649b8fae8f80d54bab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575cce4d2fbf4eb09c07310b080e7141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 55 (orig #480) ---\n",
      "Actual cleaned report : mild soft tissue swelling, Rt lateral ankle\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.0978\n",
      "\n",
      "Overall average semantic similarity over 20 examples: 0.4020\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm import create_model\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Logging: INFO+ to training.log\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) Your FinalSamplesDataset (verbatim)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform, patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token  = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        input_text = f\"{self.tokenizer.bos_token} FINDINGS: {clean} {self.tokenizer.eos_token}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        input_ids      = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids: {input_ids.shape}, attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0 = max(x-crop_size[0]//2,0); y0 = max(y-crop_size[1]//2,0)\n",
    "                    x1 = min(x+crop_size[0]//2,arr.shape[1]); y1 = min(y+crop_size[1]//2,arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size: lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17: lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left,right = [],[]\n",
    "        if len(paths)==1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth): return pad17([])+pad17([])\n",
    "            arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side,pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        if side=='left': left=lst\n",
    "                        else:            right=lst\n",
    "\n",
    "        if left and not right: right=[cv2.flip(p,1) for p in left]\n",
    "        if right and not left: left =[cv2.flip(p,1) for p in right]\n",
    "        if not left and not right: return pad17([])+pad17([])\n",
    "\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_',' ')\n",
    "        return re.sub(r'\\s+',' ', text).strip()\n",
    "\n",
    "# prepare_data for binary tasks\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in data.values():\n",
    "        lbl = entry.get('class_label','').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl]+=1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    if is_binary:\n",
    "        class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "        min_count = min(class_counts.values())\n",
    "        combined = []\n",
    "        for lbl in target_classes:\n",
    "            combined += random.sample(data_by_class[lbl], min_count)\n",
    "        return combined, class_counts, class_counts\n",
    "    else:\n",
    "        return list(data.values()), {}, {}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) Transforms\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) Collate fn\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    patches = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in patches)\n",
    "    padded = []\n",
    "    for p in patches:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        padded.append(p)\n",
    "    patches = torch.stack(padded, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "\n",
    "    labels = []\n",
    "    for i in ids:\n",
    "        seq = i.tolist()\n",
    "        shifted = seq[1:] + [-100]\n",
    "        labels.append(torch.tensor(shifted, dtype=torch.long))\n",
    "\n",
    "    input_ids = nn.utils.rnn.pad_sequence(ids,    batch_first=True,\n",
    "                                          padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence(masks, batch_first=True,\n",
    "                                               padding_value=0)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True,\n",
    "                                       padding_value=-100)\n",
    "\n",
    "    return imgs, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) VisionGPT2Model + submodules (same as reference)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size*self.n_heads*3)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "        self.c_proj= nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "    def forward(self,x):\n",
    "        b,t,c = x.size()\n",
    "        q,k,v = self.c_attn(x).chunk(3,-1)\n",
    "        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = (q@k.transpose(-2,-1))*self.scale\n",
    "        att = att.masked_fill(self.mask[:,:,:t,:t]==0, float('-inf'))\n",
    "        att = F.softmax(att,-1); att = self.attn_dropout(att)\n",
    "        y = att@v; y = y.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.q= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.k= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.c_proj= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.normal_(m.weight,0,0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    def forward(self, q,k,v):\n",
    "        b,qt,_=q.size()\n",
    "        q_ = self.q(q).view(b,qt,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k_ = self.k(k).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v_ = self.v(v).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = F.softmax((q_@k_.transpose(-2,-1))*self.scale, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att@v_; y = y.permute(0,2,1,3).contiguous().view(b,qt,self.embed_dim)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.embed_dim, config.embed_dim*config.mlp_ratio)\n",
    "        self.act=nn.GELU()\n",
    "        self.c_proj=nn.Linear(config.embed_dim*config.mlp_ratio, config.embed_dim)\n",
    "        self.dropout=nn.Dropout(config.mlp_dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln_1=nn.LayerNorm(config.embed_dim)\n",
    "        self.attn=GPT2Attention(config)\n",
    "        self.ln_2=nn.LayerNorm(config.embed_dim)\n",
    "        self.cross=GPT2CrossAttention(config)\n",
    "        self.ln_3=nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp=GPT2MLP(config)\n",
    "    def forward(self,x,enc):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross(self.ln_2(x), enc, enc)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x\n",
    "\n",
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        vit = create_model('vit_base_patch16_224',pretrained=True,num_classes=0)\n",
    "        self.patch_embed=vit.patch_embed\n",
    "        self.cls_token=vit.cls_token\n",
    "        self.pos_embed=vit.pos_embed\n",
    "        self.pos_drop=nn.Dropout(0.)\n",
    "        self.vit_blocks=vit.blocks[:config.depth]\n",
    "\n",
    "        self.config=config\n",
    "        self.wte=nn.Embedding(config.vocab_size,config.embed_dim)\n",
    "        self.wpe=nn.Embedding(config.seq_len,config.embed_dim)\n",
    "        self.drop=nn.Dropout(config.emb_dropout)\n",
    "        self.h=nn.ModuleList([GPT2Block(config) for _ in range(config.depth)])\n",
    "        self.ln_f=nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head=nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.wte.weight\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,config):\n",
    "        model=cls(config)\n",
    "        hf=GPT2LMHeadModel.from_pretrained('gpt2').state_dict()\n",
    "        sd=model.state_dict()\n",
    "        for k,v in hf.items():\n",
    "            if k in sd and sd[k].shape==v.shape:\n",
    "                sd[k].copy_(v)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "\n",
    "    def forward(self,image,input_ids,labels=None):\n",
    "        x=self.patch_embed(image)\n",
    "        x=torch.cat([self.cls_token.expand(x.size(0),-1,-1),x],1)\n",
    "        x=x+self.pos_embed; x=self.pos_drop(x)\n",
    "        for blk in self.vit_blocks: x=blk(x)\n",
    "\n",
    "        b,t=input_ids.size()\n",
    "        tok=self.wte(input_ids)\n",
    "        pos=torch.arange(t,device=input_ids.device).unsqueeze(0)\n",
    "        pos=self.wpe(pos)\n",
    "        h=self.drop(tok+pos)\n",
    "\n",
    "        for blk in self.h: h=blk(h,x)\n",
    "        h=self.ln_f(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits=self.lm_head(h)\n",
    "            loss=F.cross_entropy(lm_logits.view(-1,lm_logits.size(-1)),\n",
    "                                 labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return self.lm_head(h[:,-1,:])\n",
    "\n",
    "    def generate(self,image,seq,max_tokens=50,temperature=1.0,deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            logits=self(image,seq)/temperature\n",
    "            probs=F.softmax(logits,-1)\n",
    "            if deterministic:\n",
    "                nxt=torch.argmax(probs,dim=-1,keepdim=True)\n",
    "            else:\n",
    "                nxt=torch.multinomial(probs,1)\n",
    "            seq=torch.cat([seq,nxt],1)\n",
    "            if nxt.item()==tokenizer.eos_token_id: break\n",
    "        return seq\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) Trainer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class Trainer:\n",
    "    def __init__(self,model_config,train_config,dls):\n",
    "        self.device=train_config.device\n",
    "        self.model=VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        # freeze pretrained for first epochs\n",
    "        for p in self.model.parameters(): p.requires_grad=False\n",
    "        self.model.h.requires_grad_(True)\n",
    "\n",
    "        self.tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token=self.tokenizer.eos_token\n",
    "\n",
    "        self.optim=torch.optim.Adam(self.model.parameters(),lr=train_config.lr)\n",
    "        self.train_dl,self.val_dl=dls\n",
    "        self.train_config=train_config\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.train_dl,desc=\"Train\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            loss=self.model(imgs,ids,labels)\n",
    "            self.optim.zero_grad(); loss.backward(); self.optim.step()\n",
    "            total+=loss.item()\n",
    "        return total/len(self.train_dl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self):\n",
    "        self.model.eval(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.val_dl,desc=\"Val\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            total+=self.model(imgs,ids,labels).item()\n",
    "        return total/len(self.val_dl)\n",
    "\n",
    "    def fit(self):\n",
    "        best=1e9\n",
    "        os.makedirs(self.train_config.model_path,exist_ok=True)\n",
    "        for ep in range(self.train_config.epochs):\n",
    "            tl=self.train_one_epoch(); vl=self.valid_one_epoch()\n",
    "            print(f\"Epoch {ep+1}/{self.train_config.epochs} — train {tl:.4f}, val {vl:.4f}\")\n",
    "            if vl<best:\n",
    "                best=vl\n",
    "                torch.save(self.model.state_dict(),\n",
    "                           os.path.join(self.train_config.model_path,'best.pt'))\n",
    "\n",
    "    def generate_caption(self,image_path,temp=1.0,det=False):\n",
    "        seq=torch.tensor([[self.tokenizer.bos_token_id]],\n",
    "                         device=self.device)\n",
    "        img=np.array(Image.open(image_path).convert('RGB'))\n",
    "        img=train_transform(Image.fromarray(img)).unsqueeze(0).to(self.device)\n",
    "        out=self.model.generate(img,seq,temperature=temp,deterministic=det)\n",
    "        return self.tokenizer.decode(out[0].cpu(),skip_special_tokens=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) Main\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if __name__==\"__main__\":\n",
    "    # configs\n",
    "    model_config=SimpleNamespace(\n",
    "        vocab_size=50257, embed_dim=768, num_heads=12,\n",
    "        seq_len=512, depth=12,\n",
    "        attention_dropout=0.1, residual_dropout=0.1,\n",
    "        mlp_ratio=4, mlp_dropout=0.1, emb_dropout=0.1\n",
    "    )\n",
    "    train_config=SimpleNamespace(\n",
    "        epochs=10,\n",
    "        lr=1e-4,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model_path='captioner'\n",
    "    )\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "    # dataset config\n",
    "    cfg=SimpleNamespace()\n",
    "    cfg.DATASET=SimpleNamespace()\n",
    "    cfg.DATASET.JSON='final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.TARGET_CLASSES=['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE=False\n",
    "    cfg.DATASET.AUGMENT=False\n",
    "\n",
    "    # dataset + splits\n",
    "    ds=FinalSamplesDataset(cfg,\n",
    "                           image_transform=train_transform,\n",
    "                           patch_transform=patch_transform)\n",
    "    ds.tokenizer=tokenizer\n",
    "    ds.eos_token=tokenizer.eos_token\n",
    "\n",
    "    n=len(ds)\n",
    "    n_train=int(0.8*n)\n",
    "    n_val=int(0.1*n)\n",
    "    n_test=n-n_train-n_val\n",
    "    train_ds,val_ds,test_ds=random_split(\n",
    "        ds,[n_train,n_val,n_test],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_dl=DataLoader(train_ds,batch_size=8,shuffle=True,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "    val_dl  =DataLoader(val_ds,  batch_size=8,shuffle=False,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "\n",
    "    # train\n",
    "    trainer=Trainer(model_config,train_config,(train_dl,val_dl))\n",
    "    trainer.fit()\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 7) Sample 20 random examples from test set + compute semantic sim\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    print(\"\\n=== 20 Random Test Examples ===\\n\")\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    random.seed(42)\n",
    "    sample_idxs = random.sample(range(len(test_ds)), min(20, len(test_ds)))\n",
    "    sims = []\n",
    "    for idx in sample_idxs:\n",
    "        orig_idx = test_ds.indices[idx]\n",
    "        item = test_ds[idx]\n",
    "        actual = item['cleaned_report']\n",
    "        img_path = ds.data[orig_idx]['file_path']\n",
    "        generated = trainer.generate_caption(img_path, temp=1.0, det=False)\n",
    "        emb_actual = stm.encode(actual, convert_to_tensor=True)\n",
    "        emb_gen    = stm.encode(generated, convert_to_tensor=True)\n",
    "        sim = F.cosine_similarity(emb_actual, emb_gen, dim=0).item()\n",
    "        sims.append(sim)\n",
    "        print(f\"--- Example {idx} (orig #{orig_idx}) ---\")\n",
    "        print(f\"Actual cleaned report : {actual}\")\n",
    "        print(f\"Generated report      : {generated}\")\n",
    "        print(f\"Semantic similarity   : {sim:.4f}\\n\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 8) Overall semantic similarity\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    overall_sim = sum(sims) / len(sims) if sims else 0.0\n",
    "    print(f\"Overall average semantic similarity over {len(sims)} examples: {overall_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93813bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7016ca5dd7224b069d117a0e06ae0058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4be80e315d42c5b565a4670d8932c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 — train 4.6492, val 3.5890\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7139b4fe81c54e6ebe68c05d2ac3e748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9eaab805304f98956344086a23e887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 — train 3.4171, val 2.8898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156fe24b998749b8ba975b6f50b556c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98df58446bfc4d7680f0632347f42faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 — train 2.9419, val 2.5857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877b2e3bc09e4528b1d516075393bc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a884a8a5a44047998b3b4214d81658f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 — train 2.6044, val 2.3441\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c333a9dcfdb4b32bb847924268b5e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271f3b3383214ec794cc4822ee7a8b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 — train 2.3568, val 2.2085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedae9029960460fb24f502fb4795a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a789170d89441bfb3ee0fc8148db74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 — train 2.1367, val 2.1751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2391040be0456388a240c79fb6d792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411a8a6307f84827be8ec83ff67c609a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 — train 1.9626, val 2.0460\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734eed08ee7f46559da197e1b13866fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d24a3db5134b8698b21740d8db1628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 — train 1.7372, val 2.0332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea223d615234e118e56cd97f67129aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf1f3355191427ea27cb592dc3f97c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 — train 1.5561, val 2.0419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43703fd78f8e4b839387a77b92104ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cf6466f90046979dc37e18383eeefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 — train 1.4165, val 2.0136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2820c0219a1444898c058727220a8119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadc85f411b842e68f8da9a0f9161da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 — train 1.2203, val 2.1017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6d1f2c373f407ab3bacf5402382231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7175881a8724eabb1734dc901182527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 — train 1.0567, val 2.1870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6337f6a97bca46428f8aade74d513c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d870318dbf42bb9e01b7ab83ddac6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 — train 0.9337, val 2.2084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f5af5537b046a191cba8375cde460f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f968fd6cce145cb878566a217c67655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 — train 0.7878, val 2.3663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e54eddf23246debd17af2a01ed87bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f7f01c6ec647c98b33af7a81a9223d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 — train 0.6801, val 2.4118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1712965261d24be5b6194cfa17bdd211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ca4b0fa8aa466b8ef16b94add58adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 — train 0.5499, val 2.4762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f913bd3cdf47b896d5d2e34f1a6ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf92066193f452bb0db296cc77b5910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 — train 0.4326, val 2.6268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadadf2862de41bb83ed2df6d845d60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a9d45d0acd44dd809b386abb8ded2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 — train 0.3949, val 2.6823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ebb1aeee6241a0b311b053b73b25af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74529d26b7684c459daed8eb8e4b4e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 — train 0.3430, val 2.6620\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd9ac14281f4304ba38153178e277f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8a26fdc7de4f52882e9e1db3bef3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 — train 0.2593, val 2.8576\n",
      "\n",
      "=== 20 Random Test Examples ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72847954272c4d2685bfd8482ca1e0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7bb85fb3b243629794c8c79fa0348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 163 (orig #399) ---\n",
      "Actual cleaned report : both hands, feet, and knees, FBs. both 3rd MT bone, erosive change. both hands, erosions. --> RA, suggested. Rt. knee RA involvement.\n",
      "Generated report      :  FINDINGS: periarticular osteopenia, both feet. \n",
      "Semantic similarity   : 0.4624\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65901bff01644f5b8a88c90de784428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae94bbadad7b43e4ad7bfe9b9cef38c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 28 (orig #1421) ---\n",
      "Actual cleaned report : C.I.> left 1st MTP joint Joint space narrowing in the IP joint, left big toe --> Probable gout arthritis Joint space narrowing in the IP joint, left big toe --> Probable gout arthritis\n",
      "Generated report      :  FINDINGS: Interout, both calcaneous R/O soft tissue swelling. \n",
      "Semantic similarity   : 0.3228\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ec6845370c4f6ea912a40db960fa53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e290a186eccd4df5a53cde15609bc3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 6 (orig #438) ---\n",
      "Actual cleaned report : no significant bony abnormality\n",
      "Generated report      :  FINDINGS: no significant bony lesion on radiographs. \n",
      "Semantic similarity   : 0.7430\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42669e25068945b7974ff31bb4fcd379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bc24351e834b948eea9893e810458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 189 (orig #1485) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: bilateral plan soft tissue . No bony abnormality \n",
      "Semantic similarity   : 0.2420\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f187e5487a6348458b937fd49196f13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a35b6767b24e9a8e3ffd855681ceec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 70 (orig #1203) ---\n",
      "Actual cleaned report : no significant bony lesion on radiographs. no significant bony lesion on radiographs.\n",
      "Generated report      :  FINDINGS: degenerative change of both feet. \n",
      "Semantic similarity   : 0.3166\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6b5718e41e4096915ac465ac15569a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c94c473a0b94e489b578bccdb652ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 62 (orig #2249) ---\n",
      "Actual cleaned report : degenerative change\n",
      "Generated report      :  FINDINGS: No significant interval change \n",
      "Semantic similarity   : 0.2117\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3412a3eebd4c9885b2603f8839c219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fe32e6168d4af8a89d73cb87d392f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 57 (orig #727) ---\n",
      "Actual cleaned report : No bony abnormality. No bony abnormality.\n",
      "Generated report      :  FINDINGS: Both ankle soft tissue swelling. No bony abnormality. \n",
      "Semantic similarity   : 0.4614\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c13a9ee15a147b8882acbbc681c6e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab546bcc4d7c4b0f9183dedf48633fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 35 (orig #2192) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.9006\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8094cdc619a45d2a7ae5acb39932f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d7bba01d7941eaa502842bd4f6bee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 188 (orig #1000) ---\n",
      "Actual cleaned report : No bony abnormality\n",
      "Generated report      :  FINDINGS: No bony abnormality. No bony abnormality. \n",
      "Semantic similarity   : 0.9100\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60345b028044063a4a54786ed625218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfe406acbe94eb5982f3a933aa0de46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 26 (orig #1286) ---\n",
      "Actual cleaned report : both ankle OA. both knee, minimal OA Rt. 1st MTP joint, R/O gout.\n",
      "Generated report      :  FINDINGS: - No significant bony abnormality \n",
      "Semantic similarity   : 0.2754\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137202e0bf9248148b905310246c2eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285d292cfd994aec8042980908e866e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 173 (orig #923) ---\n",
      "Actual cleaned report : Tiny osteophyte of right 1st MTP joint. Subtle radiolucent lesion of rigth 1st proximal phalangeal bone base. --- suspicious subchondral cyst. Accessory navicular bone, both.\n",
      "Generated report      :  FINDINGS: No bony abnormality. No bony abnormality. \n",
      "Semantic similarity   : 0.4561\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a295966b776d43ceb13ac8d9c3974834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c0e82cbc734fa3948817e80a9a1a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 228 (orig #113) ---\n",
      "Actual cleaned report : degenerative change.\n",
      "Generated report      :  FINDINGS: periarticular osteopenia, both feet. \n",
      "Semantic similarity   : 0.1935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9665bdf735544a5b9aeaf1a7cc40931c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46d0debdcea48b3b47448c6c6417af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 139 (orig #1120) ---\n",
      "Actual cleaned report : degenerative change. osteopenia. degenerative change. osteopenia.\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.6692\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aebc7072a94c2aa409b0eb6d528ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbf0e3f8caf4f87a0fbfcbef584cbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 22 (orig #514) ---\n",
      "Actual cleaned report : mild degenerative change\n",
      "Generated report      :  FINDINGS: No bony abnormality No bony abnormality \n",
      "Semantic similarity   : 0.2358\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10c28bc81b444f89385f232cbc03ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fa4235c4f94ac2800384aabcdb0f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 151 (orig #217) ---\n",
      "Actual cleaned report : ulnar negative variance both Lt. 1st toe, corrective osteotomy.\n",
      "Generated report      :  FINDINGS: No bony abnormality No bony abnormality \n",
      "Semantic similarity   : 0.4477\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda28249b60442b2b104910255636aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75c3470c7db4cf8a20c0d01f9b12713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 108 (orig #732) ---\n",
      "Actual cleaned report : Hallux valgus, right. Possible osteoarthritis in both 1st IP and right 1st, 2nd MTP joint. Osteophyte in talus neck, both.\n",
      "Generated report      :  FINDINGS: No bony abnormality. No bony abnormality. \n",
      "Semantic similarity   : 0.4366\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3961612c6a44735ad5304ad6df353b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975f5622dc2f453380ba98cb5382e2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 8 (orig #814) ---\n",
      "Actual cleaned report : No bony abnormality\n",
      "Generated report      :  FINDINGS: No bony abnormality No bony abnormality \n",
      "Semantic similarity   : 0.9395\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f30927ad28d45f7b9f43474e5f9d889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f237e284b19243d4b3588e3c21ce42b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 7 (orig #2219) ---\n",
      "Actual cleaned report : No significant interval change\n",
      "Generated report      :  FINDINGS: No significant interval change since last study. \n",
      "Semantic similarity   : 0.7503\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2227e29ff96a42fcbd8a8a74b3c91f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cbd6983207450ba40d568b62230e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 23 (orig #1698) ---\n",
      "Actual cleaned report : both feet, R/O gout\n",
      "Generated report      :  FINDINGS: degenerative change. \n",
      "Semantic similarity   : 0.0303\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50adf0d08b1e48e498da077b0cf87ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48a7afe357d4f8daa2a84f98f44baed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 55 (orig #480) ---\n",
      "Actual cleaned report : mild soft tissue swelling, Rt lateral ankle\n",
      "Generated report      :  FINDINGS: No significant interval change since last study. \n",
      "Semantic similarity   : 0.0609\n",
      "\n",
      "Overall average semantic similarity over 20 examples: 0.4533\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm import create_model\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Logging: INFO+ to training.log\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) Your FinalSamplesDataset (verbatim)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform, patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token  = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        input_text = f\"{self.tokenizer.bos_token} FINDINGS: {clean} {self.tokenizer.eos_token}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        input_ids      = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids: {input_ids.shape}, attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0 = max(x-crop_size[0]//2,0); y0 = max(y-crop_size[1]//2,0)\n",
    "                    x1 = min(x+crop_size[0]//2,arr.shape[1]); y1 = min(y+crop_size[1]//2,arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size: lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17: lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left,right = [],[]\n",
    "        if len(paths)==1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth): return pad17([])+pad17([])\n",
    "            arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side,pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        if side=='left': left=lst\n",
    "                        else:            right=lst\n",
    "\n",
    "        if left and not right: right=[cv2.flip(p,1) for p in left]\n",
    "        if right and not left: left =[cv2.flip(p,1) for p in right]\n",
    "        if not left and not right: return pad17([])+pad17([])\n",
    "\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_',' ')\n",
    "        return re.sub(r'\\s+',' ', text).strip()\n",
    "\n",
    "# prepare_data for binary tasks\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in data.values():\n",
    "        lbl = entry.get('class_label','').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl]+=1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    if is_binary:\n",
    "        class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "        min_count = min(class_counts.values())\n",
    "        combined = []\n",
    "        for lbl in target_classes:\n",
    "            combined += random.sample(data_by_class[lbl], min_count)\n",
    "        return combined, class_counts, class_counts\n",
    "    else:\n",
    "        return list(data.values()), {}, {}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) Transforms\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) Collate fn\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    patches = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in patches)\n",
    "    padded = []\n",
    "    for p in patches:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        padded.append(p)\n",
    "    patches = torch.stack(padded, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "\n",
    "    labels = []\n",
    "    for i in ids:\n",
    "        seq = i.tolist()\n",
    "        shifted = seq[1:] + [-100]\n",
    "        labels.append(torch.tensor(shifted, dtype=torch.long))\n",
    "\n",
    "    input_ids = nn.utils.rnn.pad_sequence(ids,    batch_first=True,\n",
    "                                          padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence(masks, batch_first=True,\n",
    "                                               padding_value=0)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True,\n",
    "                                       padding_value=-100)\n",
    "\n",
    "    return imgs, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) VisionGPT2Model + submodules (same as reference)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size*self.n_heads*3)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "        self.c_proj= nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "    def forward(self,x):\n",
    "        b,t,c = x.size()\n",
    "        q,k,v = self.c_attn(x).chunk(3,-1)\n",
    "        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = (q@k.transpose(-2,-1))*self.scale\n",
    "        att = att.masked_fill(self.mask[:,:,:t,:t]==0, float('-inf'))\n",
    "        att = F.softmax(att,-1); att = self.attn_dropout(att)\n",
    "        y = att@v; y = y.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim//self.n_heads\n",
    "        self.q= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.k= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.v= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.scale = self.head_size**-0.5\n",
    "        self.c_proj= nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.normal_(m.weight,0,0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    def forward(self, q,k,v):\n",
    "        b,qt,_=q.size()\n",
    "        q_ = self.q(q).view(b,qt,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        k_ = self.k(k).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        v_ = self.v(v).view(b,-1,self.n_heads,self.head_size).permute(0,2,1,3)\n",
    "        att = F.softmax((q_@k_.transpose(-2,-1))*self.scale, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att@v_; y = y.permute(0,2,1,3).contiguous().view(b,qt,self.embed_dim)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.embed_dim, config.embed_dim*config.mlp_ratio)\n",
    "        self.act=nn.GELU()\n",
    "        self.c_proj=nn.Linear(config.embed_dim*config.mlp_ratio, config.embed_dim)\n",
    "        self.dropout=nn.Dropout(config.mlp_dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln_1=nn.LayerNorm(config.embed_dim)\n",
    "        self.attn=GPT2Attention(config)\n",
    "        self.ln_2=nn.LayerNorm(config.embed_dim)\n",
    "        self.cross=GPT2CrossAttention(config)\n",
    "        self.ln_3=nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp=GPT2MLP(config)\n",
    "    def forward(self,x,enc):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross(self.ln_2(x), enc, enc)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x\n",
    "\n",
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        vit = create_model('vit_base_patch16_224',pretrained=True,num_classes=0)\n",
    "        self.patch_embed=vit.patch_embed\n",
    "        self.cls_token=vit.cls_token\n",
    "        self.pos_embed=vit.pos_embed\n",
    "        self.pos_drop=nn.Dropout(0.)\n",
    "        self.vit_blocks=vit.blocks[:config.depth]\n",
    "\n",
    "        self.config=config\n",
    "        self.wte=nn.Embedding(config.vocab_size,config.embed_dim)\n",
    "        self.wpe=nn.Embedding(config.seq_len,config.embed_dim)\n",
    "        self.drop=nn.Dropout(config.emb_dropout)\n",
    "        self.h=nn.ModuleList([GPT2Block(config) for _ in range(config.depth)])\n",
    "        self.ln_f=nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head=nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.wte.weight\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,config):\n",
    "        model=cls(config)\n",
    "        hf=GPT2LMHeadModel.from_pretrained('gpt2').state_dict()\n",
    "        sd=model.state_dict()\n",
    "        for k,v in hf.items():\n",
    "            if k in sd and sd[k].shape==v.shape:\n",
    "                sd[k].copy_(v)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "\n",
    "    def forward(self,image,input_ids,labels=None):\n",
    "        x=self.patch_embed(image)\n",
    "        x=torch.cat([self.cls_token.expand(x.size(0),-1,-1),x],1)\n",
    "        x=x+self.pos_embed; x=self.pos_drop(x)\n",
    "        for blk in self.vit_blocks: x=blk(x)\n",
    "\n",
    "        b,t=input_ids.size()\n",
    "        tok=self.wte(input_ids)\n",
    "        pos=torch.arange(t,device=input_ids.device).unsqueeze(0)\n",
    "        pos=self.wpe(pos)\n",
    "        h=self.drop(tok+pos)\n",
    "\n",
    "        for blk in self.h: h=blk(h,x)\n",
    "        h=self.ln_f(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits=self.lm_head(h)\n",
    "            loss=F.cross_entropy(lm_logits.view(-1,lm_logits.size(-1)),\n",
    "                                 labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return self.lm_head(h[:,-1,:])\n",
    "\n",
    "    def generate(self,image,seq,max_tokens=50,temperature=1.0,deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            logits=self(image,seq)/temperature\n",
    "            probs=F.softmax(logits,-1)\n",
    "            if deterministic:\n",
    "                nxt=torch.argmax(probs,dim=-1,keepdim=True)\n",
    "            else:\n",
    "                nxt=torch.multinomial(probs,1)\n",
    "            seq=torch.cat([seq,nxt],1)\n",
    "            if nxt.item()==tokenizer.eos_token_id: break\n",
    "        return seq\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) Trainer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class Trainer:\n",
    "    def __init__(self,model_config,train_config,dls):\n",
    "        self.device=train_config.device\n",
    "        self.model=VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        # freeze pretrained for first epochs\n",
    "        for p in self.model.parameters(): p.requires_grad=False\n",
    "        self.model.h.requires_grad_(True)\n",
    "\n",
    "        self.tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token=self.tokenizer.eos_token\n",
    "\n",
    "        self.optim=torch.optim.Adam(self.model.parameters(),lr=train_config.lr)\n",
    "        self.train_dl,self.val_dl=dls\n",
    "        self.train_config=train_config\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.train_dl,desc=\"Train\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            loss=self.model(imgs,ids,labels)\n",
    "            self.optim.zero_grad(); loss.backward(); self.optim.step()\n",
    "            total+=loss.item()\n",
    "        return total/len(self.train_dl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self):\n",
    "        self.model.eval(); total=0\n",
    "        for imgs,patches,ids,mask,labels in tqdm(self.val_dl,desc=\"Val\"):\n",
    "            imgs,ids,labels=imgs.to(self.device),ids.to(self.device),labels.to(self.device)\n",
    "            total+=self.model(imgs,ids,labels).item()\n",
    "        return total/len(self.val_dl)\n",
    "\n",
    "    def fit(self):\n",
    "        best=1e9\n",
    "        os.makedirs(self.train_config.model_path,exist_ok=True)\n",
    "        for ep in range(self.train_config.epochs):\n",
    "            tl=self.train_one_epoch(); vl=self.valid_one_epoch()\n",
    "            print(f\"Epoch {ep+1}/{self.train_config.epochs} — train {tl:.4f}, val {vl:.4f}\")\n",
    "            if vl<best:\n",
    "                best=vl\n",
    "                torch.save(self.model.state_dict(),\n",
    "                           os.path.join(self.train_config.model_path,'best.pt'))\n",
    "\n",
    "    def generate_caption(self,image_path,temp=1.0,det=False):\n",
    "        seq=torch.tensor([[self.tokenizer.bos_token_id]],\n",
    "                         device=self.device)\n",
    "        img=np.array(Image.open(image_path).convert('RGB'))\n",
    "        img=train_transform(Image.fromarray(img)).unsqueeze(0).to(self.device)\n",
    "        out=self.model.generate(img,seq,temperature=temp,deterministic=det)\n",
    "        return self.tokenizer.decode(out[0].cpu(),skip_special_tokens=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) Main\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if __name__==\"__main__\":\n",
    "    # configs\n",
    "    model_config=SimpleNamespace(\n",
    "        vocab_size=50257, embed_dim=768, num_heads=12,\n",
    "        seq_len=512, depth=12,\n",
    "        attention_dropout=0.1, residual_dropout=0.1,\n",
    "        mlp_ratio=4, mlp_dropout=0.1, emb_dropout=0.1\n",
    "    )\n",
    "    train_config=SimpleNamespace(\n",
    "        epochs=20,\n",
    "        lr=1e-4,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model_path='captioner'\n",
    "    )\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "    # dataset config\n",
    "    cfg=SimpleNamespace()\n",
    "    cfg.DATASET=SimpleNamespace()\n",
    "    cfg.DATASET.JSON='final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.TARGET_CLASSES=['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE=False\n",
    "    cfg.DATASET.AUGMENT=False\n",
    "\n",
    "    # dataset + splits\n",
    "    ds=FinalSamplesDataset(cfg,\n",
    "                           image_transform=train_transform,\n",
    "                           patch_transform=patch_transform)\n",
    "    ds.tokenizer=tokenizer\n",
    "    ds.eos_token=tokenizer.eos_token\n",
    "\n",
    "    n=len(ds)\n",
    "    n_train=int(0.8*n)\n",
    "    n_val=int(0.1*n)\n",
    "    n_test=n-n_train-n_val\n",
    "    train_ds,val_ds,test_ds=random_split(\n",
    "        ds,[n_train,n_val,n_test],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_dl=DataLoader(train_ds,batch_size=8,shuffle=True,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "    val_dl  =DataLoader(val_ds,  batch_size=8,shuffle=False,\n",
    "                        collate_fn=collate_fn,pin_memory=True)\n",
    "\n",
    "    # train\n",
    "    trainer=Trainer(model_config,train_config,(train_dl,val_dl))\n",
    "    trainer.fit()\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 7) Sample 20 random examples from test set + compute semantic sim\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    print(\"\\n=== 20 Random Test Examples ===\\n\")\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    random.seed(42)\n",
    "    sample_idxs = random.sample(range(len(test_ds)), min(20, len(test_ds)))\n",
    "    sims = []\n",
    "    for idx in sample_idxs:\n",
    "        orig_idx = test_ds.indices[idx]\n",
    "        item = test_ds[idx]\n",
    "        actual = item['cleaned_report']\n",
    "        img_path = ds.data[orig_idx]['file_path']\n",
    "        generated = trainer.generate_caption(img_path, temp=1.0, det=False)\n",
    "        emb_actual = stm.encode(actual, convert_to_tensor=True)\n",
    "        emb_gen    = stm.encode(generated, convert_to_tensor=True)\n",
    "        sim = F.cosine_similarity(emb_actual, emb_gen, dim=0).item()\n",
    "        sims.append(sim)\n",
    "        print(f\"--- Example {idx} (orig #{orig_idx}) ---\")\n",
    "        print(f\"Actual cleaned report : {actual}\")\n",
    "        print(f\"Generated report      : {generated}\")\n",
    "        print(f\"Semantic similarity   : {sim:.4f}\\n\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 8) Overall semantic similarity\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    overall_sim = sum(sims) / len(sims) if sims else 0.0\n",
    "    print(f\"Overall average semantic similarity over {len(sims)} examples: {overall_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d94196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n",
      "\n",
      "Number of training samples:   1915\n",
      "Number of validation samples: 239\n",
      "Number of test samples:       240\n",
      "Total samples:                2394\n",
      "\n",
      "Custom GPT-2 decoder integrated (no LoRA)\n",
      "\n",
      "-- Phase 1, Epoch 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad norm after 1st epoch backward: 16.8984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be836b3a02d4f0e8bca75b6919e9fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88d7882d51947358f576dcc7d57ca47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 16.5386\n",
      "  Validation Loss     : 5.3970\n",
      "  Semantic Similarity : 0.0466\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Phase 2, Epoch 1/1 --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761f4e2a566b49d680d9c0072130a36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eee2271a804b24b31c6221eb67f1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 5.3146\n",
      "  Validation Loss     : 4.9402\n",
      "  Semantic Similarity : 0.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65a992d950443bea1335136da9f0205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85daaa447edb4c7a89578a5d36fb1789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss               : 4.9816\n",
      "Test Semantic Similarity: 0.0556\n",
      "\n",
      "--- Example 23 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality No bony abnormality\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 97 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 24 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 91 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality. No bony abnormality.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 216 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality. No bony abnormality.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 88 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "판독 의뢰의와 판독 소견에 대해 구두로 discussion함._x000D_\n",
      "_x000D_\n",
      "No additional findings._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "임상적 증상과 correlation 후 추가 판독 및 discussion이 필요한 경우 재의뢰 요망.\n",
      "Cleaned Report   : \n",
      "discussion . No additional findings.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 154 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No significant abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No significant abnormality.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 67 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "both accessory navicular bone, type III_x000D_\n",
      "calcaneocuboidal joint, OA_x000D_\n",
      "Lt. big toe, suspicoius accessory ossicle._x000D_\n",
      "_x000D_\n",
      "both 1st MTP joint, suspicious bony erosion._x000D_\n",
      "_x000D_\n",
      "rec> clinical correlation._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both accessory navicular bone, type III calcaneocuboidal joint, OA Lt. big toe, suspicoius accessory ossicle. both 1st MTP joint, suspicious bony erosion. rec> clinical correlation.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 206 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Os naviulare type II, Rt, and os naviculare type III, Lt_x000D_\n",
      "Ulnar negative variance, both_x000D_\n",
      "both ankle soft tissue swelling._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Os naviulare type II, Rt, and os naviculare type III, Lt_x000D_\n",
      "Ulnar negative variance, both_x000D_\n",
      "both ankle soft tissue swelling._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Os naviulare type II, Rt, and os naviculare type III, Lt Ulnar negative variance, both both ankle soft tissue swelling. Os naviulare type II, Rt, and os naviculare type III, Lt Ulnar negative variance, both both ankle soft tissue swelling.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 11 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_-_x000D__x000D_[CONCLUSION    ]_x000D_progressed erosions at Rt 1st MTP joint \n",
      "diffuse joint space narrowing, multiple Rt MTP joints\n",
      "erosion, Lt 3rd MTP joint \n",
      "periarticular osteopenia, both \n",
      "-> RA involvement, more likely _x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "- progressed erosions at Rt 1st MTP joint diffuse joint space narrowing, multiple Rt MTP joints erosion, Lt 3rd MTP joint periarticular osteopenia, both -> RA involvement, more likely\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 186 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_suspicious erosion at Lt 4th MT head \n",
      "soft tissue swelling around Rt 5th MTP joint \n",
      "-> RA involvement, more likely _x000D__x000D_[CONCLUSION    ]_x000D_suspicious erosion at Lt 4th MT head \n",
      "soft tissue swelling around Rt 5th MTP joint \n",
      "-> RA involvement, more likely _x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "suspicious erosion at Lt 4th MT head soft tissue swelling around Rt 5th MTP joint -> RA involvement, more likely\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 117 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "ulnar negative variance both_x000D_\n",
      "_x000D_\n",
      "Lt. 1st toe, corrective osteotomy._x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "ulnar negative variance both Lt. 1st toe, corrective osteotomy.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 137 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "LT. foot, diffuse vascular calcifications._x000D_\n",
      "_x000D_\n",
      "degenerative change_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "LT. foot, diffuse vascular calcifications._x000D_\n",
      "_x000D_\n",
      "degenerative change_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "LT. foot, diffuse vascular calcifications. degenerative change\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 31 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_-_x000D__x000D_[CONCLUSION    ]_x000D_degenerative change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "- degenerative change\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 96 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "No bony abnormalities._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormalities.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 20 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change, both feet._x000D_\n",
      "both 5th MC bone, brachydactyly._x000D_\n",
      "degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change, both feet._x000D_\n",
      "both 5th MC bone, brachydactyly._x000D_\n",
      "degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change, both feet. both 5th MC bone, brachydactyly. degenerative change.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 141 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "minimal OA, both knee joints._x000D_\n",
      "both 1st toe, MTP joint, OA._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "minimal OA, both knee joints._x000D_\n",
      "both 1st toe, MTP joint, OA._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "minimal OA, both knee joints. both 1st toe, MTP joint, OA.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 75 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Lt. 1st MTP joint, gout_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Lt. 1st MTP joint, gout_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Lt. 1st MTP joint, gout\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 212 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No significant interval change since last study._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No significant interval change since last study.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 160 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no bony lesion.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 158 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Rt. 2dn toe, R/O polydactyly._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Rt. 2dn toe, R/O polydactyly._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Rt. 2dn toe, R/O polydactyly. Rt. 2dn toe, R/O polydactyly.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 92 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change, both feet_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change, both feet_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change, both feet\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 147 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Bipartite medial sesamoid bone, Rt._x000D_\n",
      "Otherwise, unremarkable_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Bipartite medial sesamoid bone, Rt._x000D_\n",
      "Otherwise, unremarkable_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Bipartite medial sesamoid bone, Rt. Otherwise, unremarkable\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 49 ---\n",
      "Raw Report       : \n",
      " 임시판독 결과 입니다 추후에 판독결과가 수정 될수 있으므로 확인 바랍니다._x000D_\n",
      "------------------------------------------------------------------------ _x000D_\n",
      "[ Finding ]_x000D_\n",
      "Hallux valgus, both. _x000D_\n",
      "Possible bipartite medial sesamoid or fracture, 1st metatarsal head level, Rt._x000D_\n",
      "  --- Rec) clinical correlation._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      ". ------------------------------------------------------------------------ Hallux valgus, both. Possible bipartite medial sesamoid or fracture, 1st metatarsal head level, Rt. --- Rec) clinical correlation.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 180 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality. No bony abnormality.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 17 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "os naviculare, both._x000D_\n",
      " - right, type 3_x000D_\n",
      " - left, type 2._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "os naviculare, both. - right, type 3 - left, type 2.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 230 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_hallux valgus with OA, Lt 1st MTP joint \n",
      "nonspecific calcification, Rt 2nd MTP joint _x000D__x000D_[CONCLUSION    ]_x000D_hallux valgus with OA, Lt 1st MTP joint \n",
      "nonspecific calcification, Rt 2nd MTP joint _x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "hallux valgus with OA, Lt 1st MTP joint nonspecific calcification, Rt 2nd MTP joint\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 169 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both calcaneal spur._x000D_\n",
      "both ankle degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both calcaneal spur._x000D_\n",
      "both ankle degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both calcaneal spur. both ankle degenerative change.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 58 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no bony lesion.\n",
      "Generated Report : \n",
      " \n",
      "\n",
      "--- Example 197 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change.\n",
      "Generated Report : \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import random\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Logging configuration: write INFO+ logs only to training.log (no console output)\n",
    "# =============================================================================\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions for data preparation\n",
    "# =============================================================================\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token  = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        input_text = f\"{self.tokenizer.bos_token} FINDINGS: {clean} {self.tokenizer.eos_token}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        input_ids      = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids shape: {input_ids.shape}, attention_mask shape: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side=='left':  left  = lst\n",
    "                            else:             right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p,1) for p in left]\n",
    "        if right and not left:\n",
    "            left  = [cv2.flip(p,1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    logging.info(f\"[Collate] imgs: {imgs.shape}\")\n",
    "\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "    logging.info(f\"[Collate] patches: {patches.shape}\")\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "    logging.info(f\"[Collate] ids: {ids.shape}, masks: {masks.shape}\")\n",
    "\n",
    "    return {\n",
    "        'full_imgs':       imgs,\n",
    "        'patches':         patches,\n",
    "        'input_ids':       ids,\n",
    "        'attention_mask':  masks,\n",
    "        'raw_reports':     [b['raw_report']     for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Custom GPT-2 components\n",
    "# =============================================================================\n",
    "class CustomGPT2Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 50257,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: int = 4,\n",
    "        seq_len: int = 512,\n",
    "        attention_dropout: float = 0.1,\n",
    "        residual_dropout: float = 0.1,\n",
    "        mlp_dropout: float = 0.1,\n",
    "        num_layers: int = 12,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.residual_dropout = residual_dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1, 1, self.seq_len, self.seq_len)))\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        q, k, v = self.c_attn(x).chunk(3, -1)\n",
    "        q = q.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        k = k.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        att = att.masked_fill(self.mask[:, :, :t, :t] == 0, float('-inf'))\n",
    "        att = F.softmax(att, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.permute(0, 2, 1, 3).contiguous().view(b, t, c)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.q = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b, qt, _ = q.size()\n",
    "        q_ = self.q(q).view(b, qt, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        k_ = self.k(k).view(b, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v_ = self.v(v).view(b, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        att = (q_ @ k_.transpose(-2, -1)) * self.scale\n",
    "        att = F.softmax(att, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v_\n",
    "        y = y.permute(0, 2, 1, 3).contiguous().view(b, qt, self.embed_dim)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embed_dim, config.embed_dim * config.mlp_ratio)\n",
    "        self.act = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.embed_dim * config.mlp_ratio, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.mlp_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.cross = GPT2CrossAttention(config)\n",
    "        self.ln_3 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self, x, enc):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross(self.ln_2(x), enc, enc)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x\n",
    "\n",
    "class CustomGPT2Decoder(nn.Module):\n",
    "    def __init__(self, config: CustomGPT2Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.seq_len, config.embed_dim)\n",
    "        self.drop = nn.Dropout(config.residual_dropout)\n",
    "        self.h = nn.ModuleList([GPT2Block(config) for _ in range(config.num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        b, t = input_ids.size()\n",
    "        positions = torch.arange(t, device=input_ids.device).unsqueeze(0).expand(b, -1)\n",
    "        x = self.wte(input_ids) + self.wpe(positions)\n",
    "        x = self.drop(x)\n",
    "        for block in self.h:\n",
    "            x = block(x, encoder_hidden_states)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.vocab_size - 1)\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        return type(\"ModelOutput\", (object,), {\"loss\": loss, \"logits\": logits})\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        max_length: int = 150,\n",
    "        eos_token_id: int = None,\n",
    "        pad_token_id: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        B, cur_len = input_ids.shape\n",
    "        generated = input_ids\n",
    "        for _ in range(max_length - cur_len):\n",
    "            out = self.forward(\n",
    "                generated,\n",
    "                attention_mask=None,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "            )\n",
    "            next_logits = out.logits[:, -1, :]\n",
    "            next_token = next_logits.argmax(dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            if eos_token_id is not None and (next_token == eos_token_id).all():\n",
    "                break\n",
    "        return generated\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.wte\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "# =============================================================================\n",
    "# MultiModalModel with custom decoder (no LoRA)\n",
    "# =============================================================================\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, custom_gpt_config: CustomGPT2Config):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj    = nn.Linear(self.global_encoder.num_features, custom_gpt_config.embed_dim)\n",
    "\n",
    "        self.patch_encoder  = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj     = nn.Linear(self.patch_encoder.num_features, custom_gpt_config.embed_dim)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=custom_gpt_config.embed_dim,\n",
    "                                          num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(custom_gpt_config.embed_dim)\n",
    "\n",
    "        self.decoder = CustomGPT2Decoder(custom_gpt_config)\n",
    "        print(\"Custom GPT-2 decoder integrated (no LoRA)\")\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g_feats = self.global_encoder(imgs)\n",
    "        g       = self.global_proj(g_feats).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B*N, C, H, W)\n",
    "        pf_feats = (self.patch_encoder.forward_features(p)\n",
    "                    if hasattr(self.patch_encoder, 'forward_features')\n",
    "                    else self.patch_encoder(p))\n",
    "        pf_pooled = pf_feats.mean(dim=[2,3])\n",
    "        pf = self.patch_proj(pf_pooled).view(B, N, -1)\n",
    "\n",
    "        cat, _ = self.attn(\n",
    "            torch.cat([g, pf], 1),\n",
    "            torch.cat([g, pf], 1),\n",
    "            torch.cat([g, pf], 1)\n",
    "        )\n",
    "        comb = self.norm(cat)\n",
    "\n",
    "        out = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            encoder_attention_mask=torch.ones(B, comb.size(1), device=comb.device),\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# Training & evaluation loops\n",
    "# =============================================================================\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    all_gen, all_gt = [], []\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            g_feats = model.global_encoder(imgs)\n",
    "            g       = model.global_proj(g_feats).unsqueeze(1)\n",
    "            B,N,C,H,W = pts.shape\n",
    "            p      = pts.view(B*N, C, H, W)\n",
    "            pf_feats = (model.patch_encoder.forward_features(p)\n",
    "                        if hasattr(model.patch_encoder, 'forward_features')\n",
    "                        else model.patch_encoder(p))\n",
    "            pf_pooled= pf_feats.mean(dim=[2,3])\n",
    "            pf        = model.patch_proj(pf_pooled).view(B, N, -1)\n",
    "            cat,_  = model.attn(\n",
    "                torch.cat([g,pf],1),\n",
    "                torch.cat([g,pf],1),\n",
    "                torch.cat([g,pf],1)\n",
    "            )\n",
    "            comb    = model.norm(cat)\n",
    "\n",
    "            prompt_ids = tokenizer(\"FINDINGS:\", return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "            prompt_ids = prompt_ids.expand(B, -1)\n",
    "            prompt_mask= torch.ones_like(prompt_ids, device=device)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                encoder_hidden_states=comb,\n",
    "                encoder_attention_mask=torch.ones(B, comb.size(1), device=device),\n",
    "                max_length=150,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt  = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt .extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt,  convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Similarity\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    class Cfg: pass\n",
    "    cfg = Cfg()\n",
    "    cfg.DATASET = Cfg()\n",
    "    cfg.DATASET.JSON           = 'final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.USE_RAW        = True\n",
    "    cfg.DATASET.USE_PATCH      = True\n",
    "    cfg.DATASET.REPORT         = True\n",
    "    cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE        = False\n",
    "    cfg.DATASET.AUGMENT        = False\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"bos_token\": \"<BOS>\",\n",
    "        \"additional_special_tokens\": [\"FINDINGS:\"]\n",
    "    })\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token     = tokenizer.eos_token\n",
    "    globals()['tokenizer']  = tokenizer\n",
    "\n",
    "    # Show class counts\n",
    "    dataset = FinalSamplesDataset(cfg)\n",
    "    dataset.tokenizer = tokenizer\n",
    "    dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "    dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "    for cls, cnt in dist.items():\n",
    "        logging.info(f\"  {cls}: {cnt}\")\n",
    "        print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "    # Train/Val/Test split sizes\n",
    "    n = len(dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    print(f\"\\nNumber of training samples:   {n_train}\")\n",
    "    print(f\"Number of validation samples: {n_val}\")\n",
    "    print(f\"Number of test samples:       {n_test}\")\n",
    "    print(f\"Total samples:                {n_train + n_val + n_test}\\n\")\n",
    "\n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Model setup\n",
    "    custom_cfg = CustomGPT2Config(\n",
    "        vocab_size=len(tokenizer),\n",
    "        embed_dim=768,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        seq_len=512,\n",
    "        attention_dropout=0.1,\n",
    "        residual_dropout=0.1,\n",
    "        mlp_dropout=0.1,\n",
    "        num_layers=12,\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model  = MultiModalModel(custom_cfg).to(device)\n",
    "\n",
    "    # Phase 1\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    scaler    = torch.amp.GradScaler()\n",
    "    train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(f\"\\n-- Phase 1, Epoch {epoch+1}/1 --\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.data.norm(2).item()**2\n",
    "        print(f\"Grad norm after 1st epoch backward: {total_norm**0.5:.4f}\")\n",
    "\n",
    "        val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device, tokenizer)\n",
    "        sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        sems.append(sem)\n",
    "\n",
    "        print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "        print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "        print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "    # Phase 2\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    "    scaler    = torch.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(f\"\\n-- Phase 2, Epoch {epoch+1}/1 --\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "        val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device, tokenizer)\n",
    "        sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "        print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "        print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "        print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    # Final test\n",
    "    test_loss, test_gen, test_gt = evaluate(model, test_loader, device, tokenizer)\n",
    "    test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "    print(\"\\n========== TEST RESULTS ==========\")\n",
    "    print(f\"Test Loss               : {test_loss:.4f}\")\n",
    "    print(f\"Test Semantic Similarity: {test_sem:.4f}\")\n",
    "\n",
    "    # Random examples\n",
    "    for idx in random.sample(range(len(test_ds)), min(30, len(test_ds))):\n",
    "        ex    = test_ds[idx]\n",
    "        raw   = ex['raw_report']\n",
    "        clean = ex['cleaned_report']\n",
    "        fi    = ex['full_img'].unsqueeze(0).to(device)\n",
    "        pa    = ex['patches'].unsqueeze(0).to(device)\n",
    "\n",
    "        g_feats = model.global_encoder(fi)\n",
    "        g       = model.global_proj(g_feats).unsqueeze(1)\n",
    "        B,N,C,H,W = pa.shape\n",
    "        p      = pa.view(B*N, C, H, W)\n",
    "        pf_feats= (model.patch_encoder.forward_features(p)\n",
    "                   if hasattr(model.patch_encoder, 'forward_features')\n",
    "                   else model.patch_encoder(p))\n",
    "        pf_pooled= pf_feats.mean(dim=[2,3])\n",
    "        pf        = model.patch_proj(pf_pooled).view(B,N,-1)\n",
    "        cat,_    = model.attn(torch.cat([g,pf],1),\n",
    "                              torch.cat([g,pf],1),\n",
    "                              torch.cat([g,pf],1))\n",
    "        comb     = model.norm(cat)\n",
    "\n",
    "        prompt_ids = tokenizer(\"FINDINGS:\", return_tensors=\"pt\", add_special_tokens=False).input_ids.to(device)\n",
    "        prompt_mask= torch.ones_like(prompt_ids, device=device)\n",
    "        gen_ids = model.decoder.generate(\n",
    "            input_ids=prompt_ids,\n",
    "            encoder_hidden_states=comb,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\n--- Example {idx} ---\")\n",
    "        print(f\"Raw Report       : \\n{raw}\")\n",
    "        print(f\"Cleaned Report   : \\n{clean}\")\n",
    "        print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c422b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af0fba04",
   "metadata": {},
   "source": [
    "## New JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de37790c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a698c0304e49449d712adc917a9882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0decf7567146b8b72a6a64e189dc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 — train 4.6618, val 3.6917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f79c443cb19472c863f8270b1eb1ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a895822ba5e428288237eaf809ecf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 — train 3.4451, val 2.9861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7493e514646447d984f89031a3e2d728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64db035112be4be0953e6fe4af6cb8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 — train 2.8612, val 2.5721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89a2f21b43542dd9f1ad91f25c8d17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35f683d40964459b35e1a80e4daaf40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 — train 2.5022, val 2.3614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4880f414616747c08126e9876c914f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acc939d45ea4aa5b5da3f2dcd4682cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 — train 2.2294, val 2.2444\n",
      "\n",
      "=== 10 Random Test Examples ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f907f337c1474e9b3d57df8588a1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce927d1c5130447e91e6a9a7a8fe3044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 163 (orig #399) ---\n",
      "Actual cleaned report : The patient presents with rheumatoid arthritis affecting both hands and feet with erosions and bone changes, suggesting a possible RA diagnosis.\n",
      "Generated report      :  FINDINGS: The report indicates a degenerative change and recommending further investigation and potential treatment. \n",
      "Semantic similarity   : 0.3145\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c82ae3550bc44c89cf6944a7cc80d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304118a88ec54afd88898858c6a74323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 28 (orig #1421) ---\n",
      "Actual cleaned report : The patient has narrowing joint space in the left big toe, suggestive of gout arthritis.\n",
      "Generated report      :  FINDINGS: The patient has a mild osteopenia in small calcoarthritis with lout, and a elite discussion; bone swelling swelling osteo point may resting malohydrate on ankle swelling swelling on soft tissue swelling in with probableus.\n",
      "Semantic similarity   : 0.4632\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f91eb277764e3f943766fde09a53de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd218946004b43858043ef231ef04dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 6 (orig #438) ---\n",
      "Actual cleaned report : No significant bony abnormality, therefore, no significant bony abnormality, and no specific recommendation.\n",
      "Generated report      :  FINDINGS: The suspicious indicates a degenerative change and recommends further investigation and potential treatment. \n",
      "Semantic similarity   : 0.3631\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a90fe2df7f4efeb1d0062484d5821f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b947a8307d641a6b9e62e0633634b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 189 (orig #1485) ---\n",
      "Actual cleaned report : The report indicates a degenerative change with a conclusion recommending further investigation.\n",
      "Generated report      :  FINDINGS: No bony lesion; no findings of concern, \n",
      "Semantic similarity   : 0.3482\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917bb6da0439477db2e410f0a934c91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede0f046575040689f20ce5b138eb2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 70 (orig #1203) ---\n",
      "Actual cleaned report : No significant bony lesion was detected on radiographs, and no diagnosis is required.\n",
      "Generated report      :  FINDINGS: The patient has mild osteopenia in degenerative changes on both feet of suggests. \n",
      "Semantic similarity   : 0.3143\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcb709fa8744a518f38dfb6db8fe635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d12a2cdeae1494e8e5b21249c303855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 62 (orig #2249) ---\n",
      "Actual cleaned report : The report indicates a degenerative change, with a conclusion suggesting further investigation is needed, and a recommendation for monitoring.\n",
      "Generated report      :  FINDINGS: The patient has degenerative changes and, degenerative changes on under cause such rheumatoid arthritis coalition repair, and tPT joint, and degenerative change and calcophyte involving and requiring further investigation. \n",
      "Semantic similarity   : 0.5255\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d49755f5fb74cd2b895f56e90126b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1325923eff43d48c7fee6629095db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 57 (orig #727) ---\n",
      "Actual cleaned report : No bony abnormalities are present, and the patient is diagnosed with no bony abnormalities.\n",
      "Generated report      :  FINDINGS: The patient has [ degenerative change, degenerative exceeding mal vibration bone swelling fracture and supp nav centre and degenerative change, requiring further investigation and lower management. \n",
      "Semantic similarity   : 0.4173\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cb9784268d465f982522ac1879a3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a804c16890964826afd7360766cb99d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 35 (orig #2192) ---\n",
      "Actual cleaned report : The report indicates a degenerative change, recommending further evaluation and potential treatment.\n",
      "Generated report      :  FINDINGS: No bony abnormalities detected, no findings of concern, \n",
      "Semantic similarity   : 0.3415\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fde2d9fb00e431d8ddeec462a475927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa9c7411a574a4bad1acf2dab78feea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 188 (orig #1000) ---\n",
      "Actual cleaned report : No bony abnormality, requiring no further action.\n",
      "Generated report      :  FINDINGS: No bony abnormalitiesality found found no abnormalitiesality, \n",
      "Semantic similarity   : 0.7842\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205b8b3e82c94c8e9f4f26d2766d8449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64677b1e8b0746778a824320dabbfb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 26 (orig #1286) ---\n",
      "Actual cleaned report : The patient has ankle osteoarthritis and knee osteoarthritis, with specific findings including a fractured first metatarsophalangeal joint (R/O gout) and a right first metatarsophalangeal joint (R/O gout).\n",
      "Generated report      :  FINDINGS: No bony abnormality, no findings of no no recommendations further evaluation. \n",
      "Semantic similarity   : 0.3465\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed551b8cf3347d8b53bb3d601a056a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60358b3fd35045c3ad2e38cba6d5b24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 173 (orig #923) ---\n",
      "Actual cleaned report : The patient has subtle osteophyte, a lesion, and a suspicious subchondral cyst, potentially indicating a subchondral fracture.\n",
      "Generated report      :  FINDINGS: No bony abnormality, no findings of concern, no findings is no recommendations. \n",
      "Semantic similarity   : 0.3320\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418e9f689c484cf79b4c3cf0a5ad60cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9950c78c5829438cbfc35e62efa0915a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 228 (orig #113) ---\n",
      "Actual cleaned report : The report indicates a degenerative change with a conclusion recommending further investigation.\n",
      "Generated report      :  FINDINGS: The patient has no diagnosis of osteopenia in both knees chronic and osteopenia and brhesakoid joint and pointing spot and and calcteart on both knees, and pesane arthritisopenia and and with joint at\n",
      "Semantic similarity   : 0.2424\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01391454bb4b4f2391e70912395a4d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274018ada9a849db83a9bf0578e5becd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 139 (orig #1120) ---\n",
      "Actual cleaned report : The patient has a degenerative change and osteopenia, requiring further investigation and management.\n",
      "Generated report      :  FINDINGS: The Tib eff swelling in degenerative change requiring further evaluation and potential further investigation. \n",
      "Semantic similarity   : 0.4850\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969137ba7cec42cab5fae3ff5a95a44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1c5c5fd8f24efcbb99e7d92a50f280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 22 (orig #514) ---\n",
      "Actual cleaned report : Mild degenerative change requires monitoring and ongoing assessment.\n",
      "Generated report      :  FINDINGS: The patient has soft tissue swelling and degenerative changes, requiring further investigation. \n",
      "Semantic similarity   : 0.4407\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae61d4a917d4cd080fe03b67709937f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5816122ef0564ea981c433f7981d40bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 151 (orig #217) ---\n",
      "Actual cleaned report : The patient was diagnosed with a Lt. 1st toe and recommended corrective osteotomy.\n",
      "Generated report      :  FINDINGS: The report indicates a degenerative change, requiring further evaluation and potential treatment. \n",
      "Semantic similarity   : 0.3744\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80178863e1044d8921d3c7838d89b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bbdada1afc4a96bfbf528577bf21fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 108 (orig #732) ---\n",
      "Actual cleaned report : The patient has hallux valgus on the right foot, with possible osteoarthritis in both first and second metatarsophalangeal joints, and osteophyte in the talus neck.\n",
      "Generated report      :  FINDINGS: No bony abnormalities are present; no diagnosis is required and recommended further investigation. \n",
      "Semantic similarity   : 0.4264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b2c8c2344943b998653542281f2e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c6fca048694b48b610c2586699982a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 8 (orig #814) ---\n",
      "Actual cleaned report : No bony abnormalities detected, resulting in a diagnosis of [Diagnosis] and recommending [Recommend].\n",
      "Generated report      :  FINDINGS: No bony abnormalitiesality, no no diagnosis of recommended further investigation. \n",
      "Semantic similarity   : 0.8074\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146bd00b30df435e9362482eee370c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1ac720347a4d43befa3671f90ae19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 7 (orig #2219) ---\n",
      "Actual cleaned report : No significant interval change, therefore, no significant interval change, and no further recommendations are needed.\n",
      "Generated report      :  FINDINGS: Both degenerative change requires ongoing monitoring. \n",
      "Semantic similarity   : 0.2123\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff37f7229f694343860dc914bba7e11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067cc371f6354ae88537da9f7bf20cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 23 (orig #1698) ---\n",
      "Actual cleaned report : The patient has R/O gout on both feet.\n",
      "Generated report      :  FINDINGS: The patient has degenerative changes and degenerative changes, specifically further investigation. \n",
      "Semantic similarity   : 0.2302\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80aba034b7343ea91c347e8f44e801e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b454d4661a284a778b03f4dedda68e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 55 (orig #480) ---\n",
      "Actual cleaned report : Mild soft tissue swelling of the right lateral ankle is indicated.\n",
      "Generated report      :  FINDINGS: The patient has gout disordery on with left first metatarsophophalangealangealangealangeange head both rout in soft tissue swelling. \n",
      "Semantic similarity   : 0.4475\n",
      "\n",
      "Overall average semantic similarity over 20 examples: 0.4108\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm import create_model\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Logging: INFO+ to training.log\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    filemode='w',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# prepare_data for binary tasks\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def count_labels(data, target_classes):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in data.values():\n",
    "        lbl = entry.get('class_label','').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_data(data, target_classes, is_binary=False):\n",
    "    if is_binary:\n",
    "        class_counts, data_by_class = count_labels(data, target_classes)\n",
    "        min_count = min(class_counts.values())\n",
    "        combined = []\n",
    "        for lbl in target_classes:\n",
    "            combined += random.sample(data_by_class[lbl], min_count)\n",
    "        return combined, class_counts, class_counts\n",
    "    else:\n",
    "        return list(data.values()), {}, {}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) FinalSamplesDataset (updated)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform, patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'cleaned_report': item.get('cleaned_report', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token  = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        clean = e.get('cleaned_report', '').strip()\n",
    "        input_text = f\"{self.tokenizer.bos_token} FINDINGS: {clean} {self.tokenizer.eos_token}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        input_ids      = tok['input_ids'].squeeze(0)\n",
    "        attention_mask = tok['attention_mask'].squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids: {input_ids.shape}, attention_mask: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     e['cleaned_report'],\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        if side == 'left':  left = lst\n",
    "                        else:               right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p,1) for p in left]\n",
    "        if right and not left:\n",
    "            left  = [cv2.flip(p,1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) Transforms\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112,112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) Collate fn\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    patches = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in patches)\n",
    "    padded = []\n",
    "    for p in patches:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        padded.append(p)\n",
    "    patches = torch.stack(padded, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "\n",
    "    labels = []\n",
    "    for i in ids:\n",
    "        seq = i.tolist()\n",
    "        shifted = seq[1:] + [-100]\n",
    "        labels.append(torch.tensor(shifted, dtype=torch.long))\n",
    "\n",
    "    input_ids = nn.utils.rnn.pad_sequence(ids, batch_first=True,\n",
    "                                          padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence(masks, batch_first=True,\n",
    "                                               padding_value=0)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True,\n",
    "                                       padding_value=-100)\n",
    "\n",
    "    return imgs, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) VisionGPT2Model + submodules\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size*self.n_heads*3)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "        self.c_proj= nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout= nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout= nn.Dropout(config.residual_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        q, k, v = self.c_attn(x).chunk(3, -1)\n",
    "        q = q.view(b, t, self.n_heads, self.head_size).permute(0,2,1,3)\n",
    "        k = k.view(b, t, self.n_heads, self.head_size).permute(0,2,1,3)\n",
    "        v = v.view(b, t, self.n_heads, self.head_size).permute(0,2,1,3)\n",
    "        att = (q @ k.transpose(-2,-1)) * self.scale\n",
    "        att = att.masked_fill(self.mask[:,:,:t,:t]==0, float('-inf'))\n",
    "        att = F.softmax(att, -1); att = self.attn_dropout(att)\n",
    "        y = att @ v; y = y.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2CrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim, self.n_heads = config.embed_dim, config.num_heads\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.q = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b, qt, _ = q.size()\n",
    "        q_ = self.q(q).view(b, qt, self.n_heads, self.head_size).permute(0,2,1,3)\n",
    "        k_ = self.k(k).view(b, -1, self.n_heads, self.head_size).permute(0,2,1,3)\n",
    "        v_ = self.v(v).view(b, -1, self.n_heads, self.head_size).permute(0,2,1,3)\n",
    "        att = F.softmax((q_ @ k_.transpose(-2,-1)) * self.scale, -1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v_; y = y.permute(0,2,1,3).contiguous().view(b, qt, self.embed_dim)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embed_dim, config.embed_dim * config.mlp_ratio)\n",
    "        self.act  = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.embed_dim * config.mlp_ratio, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.mlp_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.cross = GPT2CrossAttention(config)\n",
    "        self.ln_3 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def forward(self, x, enc):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.cross(self.ln_2(x), enc, enc)\n",
    "        x = x + self.mlp(self.ln_3(x))\n",
    "        return x\n",
    "\n",
    "class VisionGPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        vit = create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.patch_embed = vit.patch_embed\n",
    "        self.cls_token   = vit.cls_token\n",
    "        self.pos_embed   = vit.pos_embed\n",
    "        self.pos_drop    = nn.Dropout(0.)\n",
    "        self.vit_blocks  = vit.blocks[:config.depth]\n",
    "\n",
    "        self.config   = config\n",
    "        self.wte      = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.wpe      = nn.Embedding(config.seq_len,   config.embed_dim)\n",
    "        self.drop     = nn.Dropout(config.emb_dropout)\n",
    "        self.h        = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)])\n",
    "        self.ln_f     = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head  = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, config):\n",
    "        model = cls(config)\n",
    "        hf = GPT2LMHeadModel.from_pretrained('gpt2').state_dict()\n",
    "        sd = model.state_dict()\n",
    "        for k, v in hf.items():\n",
    "            if k in sd and sd[k].shape == v.shape:\n",
    "                sd[k].copy_(v)\n",
    "        model.load_state_dict(sd)\n",
    "        return model\n",
    "\n",
    "    def forward(self, image, input_ids, labels=None):\n",
    "        x = self.patch_embed(image)\n",
    "        x = torch.cat([self.cls_token.expand(x.size(0), -1, -1), x], 1)\n",
    "        x = x + self.pos_embed; x = self.pos_drop(x)\n",
    "        for blk in self.vit_blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        b, t = input_ids.size()\n",
    "        tok = self.wte(input_ids)\n",
    "        pos = torch.arange(t, device=input_ids.device).unsqueeze(0)\n",
    "        pos = self.wpe(pos)\n",
    "        h = self.drop(tok + pos)\n",
    "\n",
    "        for blk in self.h:\n",
    "            h = blk(h, x)\n",
    "        h = self.ln_f(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            lm_logits = self.lm_head(h)\n",
    "            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.size(-1)),\n",
    "                                   labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return self.lm_head(h[:, -1, :])\n",
    "\n",
    "    def generate(self, image, seq, max_tokens=50, temperature=1.0, deterministic=False):\n",
    "        for _ in range(max_tokens):\n",
    "            logits = self(image, seq) / temperature\n",
    "            probs = F.softmax(logits, -1)\n",
    "            if deterministic:\n",
    "                nxt = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                nxt = torch.multinomial(probs, 1)\n",
    "            seq = torch.cat([seq, nxt], 1)\n",
    "            if nxt.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "        return seq\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) Trainer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class Trainer:\n",
    "    def __init__(self, model_config, train_config, dls):\n",
    "        self.device = train_config.device\n",
    "        self.model  = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.model.h.requires_grad_(True)\n",
    "\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=train_config.lr)\n",
    "        self.train_dl, self.val_dl = dls\n",
    "        self.train_config = train_config\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total = 0\n",
    "        for imgs, patches, ids, mask, labels in tqdm(self.train_dl, desc=\"Train\"):\n",
    "            imgs, ids, labels = imgs.to(self.device), ids.to(self.device), labels.to(self.device)\n",
    "            loss = self.model(imgs, ids, labels)\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            total += loss.item()\n",
    "        return total / len(self.train_dl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_one_epoch(self):\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        for imgs, patches, ids, mask, labels in tqdm(self.val_dl, desc=\"Val\"):\n",
    "            imgs, ids, labels = imgs.to(self.device), ids.to(self.device), labels.to(self.device)\n",
    "            total += self.model(imgs, ids, labels).item()\n",
    "        return total / len(self.val_dl)\n",
    "\n",
    "    def fit(self):\n",
    "        best = float('inf')\n",
    "        os.makedirs(self.train_config.model_path, exist_ok=True)\n",
    "        for ep in range(self.train_config.epochs):\n",
    "            tl = self.train_one_epoch()\n",
    "            vl = self.valid_one_epoch()\n",
    "            print(f\"Epoch {ep+1}/{self.train_config.epochs} — train {tl:.4f}, val {vl:.4f}\")\n",
    "            if vl < best:\n",
    "                best = vl\n",
    "                torch.save(self.model.state_dict(),\n",
    "                           os.path.join(self.train_config.model_path, 'best.pt'))\n",
    "\n",
    "    def generate_caption(self, image_path, temp=1.0, det=False):\n",
    "        seq = torch.tensor([[self.tokenizer.bos_token_id]],\n",
    "                           device=self.device)\n",
    "        img = np.array(Image.open(image_path).convert('RGB'))\n",
    "        img = train_transform(Image.fromarray(img)).unsqueeze(0).to(self.device)\n",
    "        out = self.model.generate(img, seq, temperature=temp, deterministic=det)\n",
    "        return self.tokenizer.decode(out[0].cpu(), skip_special_tokens=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) Main\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    model_config = SimpleNamespace(\n",
    "        vocab_size=50257, embed_dim=768, num_heads=12,\n",
    "        seq_len=512, depth=12,\n",
    "        attention_dropout=0.1, residual_dropout=0.1,\n",
    "        mlp_ratio=4, mlp_dropout=0.1, emb_dropout=0.1\n",
    "    )\n",
    "    train_config = SimpleNamespace(\n",
    "        epochs=5,\n",
    "        lr=1e-4,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        model_path='captioner'\n",
    "    )\n",
    "\n",
    "    # Tokenizer for collate_fn\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    cfg = SimpleNamespace()\n",
    "    cfg.DATASET = SimpleNamespace()\n",
    "    cfg.DATASET.JSON = 'final_samples_both_only_v2_cleaned.json'\n",
    "    cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE = False\n",
    "    cfg.DATASET.AUGMENT = False\n",
    "\n",
    "    ds = FinalSamplesDataset(cfg,\n",
    "                             image_transform=train_transform,\n",
    "                             patch_transform=patch_transform)\n",
    "    ds.tokenizer = tokenizer\n",
    "    ds.eos_token = tokenizer.eos_token\n",
    "\n",
    "    n = len(ds)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        ds, [n_train, n_val, n_test],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=8, shuffle=True,\n",
    "                          collate_fn=collate_fn, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False,\n",
    "                          collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    trainer = Trainer(model_config, train_config, (train_dl, val_dl))\n",
    "    trainer.fit()\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 7) Sample 20 random examples from test set + compute semantic sim\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    print(\"\\n=== 10 Random Test Examples ===\\n\")\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    random.seed(42)\n",
    "    sample_idxs = random.sample(range(len(test_ds)), min(20, len(test_ds)))\n",
    "    sims = []\n",
    "    for idx in sample_idxs:\n",
    "        orig_idx = test_ds.indices[idx]\n",
    "        item = test_ds[idx]\n",
    "        actual = item['cleaned_report']\n",
    "        img_path = ds.data[orig_idx]['file_path']\n",
    "        generated = trainer.generate_caption(img_path, temp=1.0, det=False)\n",
    "        emb_actual = stm.encode(actual, convert_to_tensor=True)\n",
    "        emb_gen    = stm.encode(generated, convert_to_tensor=True)\n",
    "        sim = F.cosine_similarity(emb_actual, emb_gen, dim=0).item()\n",
    "        sims.append(sim)\n",
    "        print(f\"--- Example {idx} (orig #{orig_idx}) ---\")\n",
    "        print(f\"Actual cleaned report : {actual}\")\n",
    "        print(f\"Generated report      : {generated}\")\n",
    "        print(f\"Semantic similarity   : {sim:.4f}\\n\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # 8) Overall semantic similarity\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    overall_sim = sum(sims) / len(sims) if sims else 0.0\n",
    "    print(f\"Overall average semantic similarity over {len(sims)} examples: {overall_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb89a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
