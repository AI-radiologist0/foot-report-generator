{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c04b5cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class dist: Counter({'normal': 748, 'uncertain': 620, 'oa': 573, 'gout': 267, 'ra': 115, 'ref.prev': 60, 'oa, ra': 8, 'combination of oa, ra': 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_18839/1250774560.py:459: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_18839/1250774560.py:377: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                 \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 464\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/10\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m     tl \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     vl, vs \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n\u001b[1;32m    466\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(tl); val_losses\u001b[38;5;241m.\u001b[39mappend(vl); sems\u001b[38;5;241m.\u001b[39mappend(vs)\n",
      "Cell \u001b[0;32mIn[5], line 378\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, scaler, device)\u001b[0m\n\u001b[1;32m    376\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m--> 378\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    380\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 344\u001b[0m, in \u001b[0;36mMultiModalModel.forward\u001b[0;34m(self, imgs, patches, input_ids, attention_mask, decoder_labels)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minject_layers:\n\u001b[1;32m    343\u001b[0m     ln_cross \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mln_cross_attn(hidden)\n\u001b[0;32m--> 344\u001b[0m     cross_out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrossattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mln_cross\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden \u001b[38;5;241m+\u001b[39m cross_out\n\u001b[1;32m    348\u001b[0m ln2 \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mln_2(hidden)\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:335\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(\n\u001b[1;32m    332\u001b[0m         query_states, key_states, value_states, attention_mask, head_mask\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_dropout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mattn_output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    348\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:35\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# SDPA with memory-efficient backend is bugged with non-contiguous inputs and custom attn_mask for some torch versions\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw   = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok   = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts  = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids,   batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs':      imgs,\n",
    "        'patches':        patches,\n",
    "        'input_ids':      ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports':    [b['raw_report']    for b in batch],\n",
    "        'cleaned_reports':[b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# MultiModalModel with multi-layer fusion\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2', inject_layers=(3,6,9)):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj    = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj    = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        config = GPT2Config.from_pretrained(gpt2_model_name)\n",
    "        config.add_cross_attention = True\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, config=config)\n",
    "\n",
    "        self.inject_layers = set(inject_layers)\n",
    "        self.multihead_vis = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2,3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        B = imgs.size(0)\n",
    "        # global image\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "        # patches\n",
    "        B,N,C,H,W = patches.shape\n",
    "        p = patches.view(B*N, C,H,W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf).view(B, N, -1)\n",
    "        # fuse\n",
    "        vis = torch.cat([g, pf], dim=1)\n",
    "        fused, _ = self.multihead_vis(vis, vis, vis)\n",
    "        fused = fused.contiguous()\n",
    "        # token embeddings\n",
    "        inputs_embeds = self.decoder.transformer.wte(input_ids)\n",
    "        pos_ids = torch.arange(input_ids.size(1), device=imgs.device)\n",
    "        pos_embeds = self.decoder.transformer.wpe(pos_ids).unsqueeze(0)\n",
    "        hidden = (inputs_embeds + pos_embeds).contiguous()\n",
    "        hidden = self.decoder.transformer.drop(hidden)\n",
    "        # masks\n",
    "        ext_attn_mask = self.decoder.get_extended_attention_mask(attention_mask, input_ids.shape, imgs.device)\n",
    "        enc_attn_mask = torch.ones(B, fused.size(1), device=imgs.device)\n",
    "        # blocks\n",
    "        for i, block in enumerate(self.decoder.transformer.h):\n",
    "            ln1 = block.ln_1(hidden)\n",
    "            attn_out, _ = block.attn(ln1, attention_mask=ext_attn_mask)\n",
    "            hidden = hidden + attn_out\n",
    "            if i in self.inject_layers:\n",
    "                ln_cross = block.ln_cross_attn(hidden)\n",
    "                cross_out, _ = block.crossattention(ln_cross,\n",
    "                                                    encoder_hidden_states=fused,\n",
    "                                                    encoder_attention_mask=enc_attn_mask)\n",
    "                hidden = hidden + cross_out\n",
    "            ln2 = block.ln_2(hidden)\n",
    "            hidden = hidden + block.mlp(ln2)\n",
    "        hidden = self.decoder.transformer.ln_f(hidden)\n",
    "        logits = self.decoder.lm_head(hidden)\n",
    "        loss = None\n",
    "        if decoder_labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = decoder_labels[..., 1:].contiguous()\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=self.decoder.config.pad_token_id)(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss, logits=logits, past_key_values=None,\n",
    "            decoder_hidden_states=None, decoder_attentions=None,\n",
    "            cross_attentions=None, encoder_last_hidden_state=fused\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Training / Eval / Plot\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs, pts = b['full_imgs'].to(device), b['patches'].to(device)\n",
    "        ids, msk = b['input_ids'].to(device), b['attention_mask'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total += loss.item()\n",
    "    return total / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    gens, gts = [], []\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs, pts = b['full_imgs'].to(device), b['patches'].to(device)\n",
    "        ids, msk = b['input_ids'].to(device), b['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total += out.loss.item()\n",
    "            prompt = ids[:, :1]\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=out.encoder_last_hidden_state,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=100, do_sample=True,\n",
    "                top_k=50, top_p=0.95,\n",
    "                temperature=0.7, repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            gens += [tokenizer.decode(g, skip_special_tokens=True) for g in gen_ids]\n",
    "            gts  += [tokenizer.decode(i, skip_special_tokens=True) for i in ids]\n",
    "    loss = total / len(loader)\n",
    "    e1 = stm.encode(gens, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gts,  convert_to_tensor=True)\n",
    "    sim = nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "    return loss, sim\n",
    "\n",
    "def plot_metrics(tr, va, sm):\n",
    "    epochs = range(1, len(tr)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(epochs, tr, label=\"Train Loss\")\n",
    "    plt.plot(epochs, va, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "    plt.twinx()\n",
    "    plt.plot(epochs, sm, label=\"Sim\"); plt.ylabel(\"Similarity\"); plt.legend(loc='lower right')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Cfg: pass\n",
    "    cfg = Cfg()\n",
    "    cfg.DATASET = Cfg()\n",
    "    cfg.DATASET.JSON = 'final_samples_both_only_fixed.json'\n",
    "    cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE = False\n",
    "    cfg.DATASET.AUGMENT = False\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    dataset = FinalSamplesDataset(cfg)\n",
    "    dataset.eos_token = tokenizer.eos_token\n",
    "    dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "    print(\"Class dist:\", dist)\n",
    "\n",
    "    n = len(dataset)\n",
    "    n_train = int(0.8*n); n_val = int(0.1*n); n_test = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [n_train,n_val,n_test])\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = MultiModalModel(inject_layers=(3,6,9)).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_losses, val_losses, sems = [], [], []\n",
    "    for epoch in range(1, 2):\n",
    "        print(f\"\\nEpoch {epoch}/10\")\n",
    "        tl = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "        vl, vs = evaluate(model, val_loader, device)\n",
    "        train_losses.append(tl); val_losses.append(vl); sems.append(vs)\n",
    "        print(f\" Train Loss: {tl:.4f} | Val Loss: {vl:.4f} | Val Sim: {vs:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "    plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "    test_loss, test_sim = evaluate(model, test_loader, device)\n",
    "    print(f\"\\nTEST Loss: {test_loss:.4f} | TEST Sim: {test_sim:.4f}\")\n",
    "\n",
    "    for idx in random.sample(range(len(test_ds)), min(5, len(test_ds))):\n",
    "        ex = test_ds[idx]\n",
    "        raw, clean = ex['raw_report'], ex['cleaned_report']\n",
    "        fi, pa = ex['full_img'].unsqueeze(0).to(device), ex['patches'].unsqueeze(0).to(device)\n",
    "        prompt = ex['input_ids'][:1].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(fi, pa, prompt, torch.ones_like(prompt))\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=out.encoder_last_hidden_state,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=120, do_sample=True,\n",
    "                top_k=40, top_p=0.95,\n",
    "                temperature=0.5, repetition_penalty=1.3,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        print(f\"\\n--- Example {idx} ---\")\n",
    "        print(\"Raw:    \", raw)\n",
    "        print(\"Clean:  \", clean)\n",
    "        print(\"Gen:    \", gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdfb5925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Saved raw image to: Image_and_Patches_Samples/1.2.392.200046.100.2.1.200325.81405.20120807153110.1.1.1.1_raw.jpg\n",
      "Saved 34 patches to: Image_and_Patches_Samples\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_20518/245397032.py:509: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_20518/245397032.py:345: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 516\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 516\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     val_loss, gen_txt, gt_txt \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n\u001b[1;32m    518\u001b[0m     sem \u001b[38;5;241m=\u001b[39m compute_semantic_similarity(gen_txt, gt_txt)\n",
      "Cell \u001b[0;32mIn[3], line 338\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, scaler, device)\u001b[0m\n\u001b[1;32m    336\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    337\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 338\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_imgs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[3], line 162\u001b[0m, in \u001b[0;36mFinalSamplesDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    161\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m--> 162\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transform(img)\n\u001b[1;32m    165\u001b[0m     patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_patches(e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_right_file_path\u001b[39m\u001b[38;5;124m'\u001b[39m], e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/PIL/Image.py:984\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    982\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B, N, 768)\n",
    "\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            prompt = ids[:, :1]\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf)\n",
    "            pf = pf.view(B, N, 768)\n",
    "\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=comb,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\", color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_fixed.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Create output directory\n",
    "out_dir = \"Image_and_Patches_Samples\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 2. Pick one random sample from the dataset’s internal data dict\n",
    "sample_idx = random.choice(list(dataset.data.keys()))\n",
    "entry = dataset.data[sample_idx]\n",
    "\n",
    "# 3. Load and save the raw (merged) image\n",
    "raw_path = entry['file_path']\n",
    "raw_img = Image.open(raw_path).convert('RGB')\n",
    "raw_basename = os.path.splitext(os.path.basename(raw_path))[0]\n",
    "raw_save_path = os.path.join(out_dir, f\"{raw_basename}_raw.jpg\")\n",
    "raw_img.save(raw_save_path)\n",
    "print(f\"Saved raw image to: {raw_save_path}\")\n",
    "\n",
    "# 4. Generate patches using the same logic as in FinalSamplesDataset._gen_patches\n",
    "#    Note: _gen_patches expects (paths_list, keypoints_dict)\n",
    "patch_arrays = dataset._gen_patches(\n",
    "    entry['left_right_file_path'],\n",
    "    entry['keypoints']\n",
    ")\n",
    "\n",
    "# 5. Save each patch\n",
    "for i, patch_arr in enumerate(patch_arrays):\n",
    "    # patch_arr is a NumPy array in RGB format\n",
    "    patch_img = Image.fromarray(patch_arr)\n",
    "    patch_save_path = os.path.join(out_dir, f\"{raw_basename}_patch_{i:02d}.jpg\")\n",
    "    patch_img.save(patch_save_path)\n",
    "print(f\"Saved {len(patch_arrays)} patches to: {out_dir}\")\n",
    "\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss              : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim     : {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    prompt = ex['input_ids'][:1].unsqueeze(0).to(device)\n",
    "\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf)\n",
    "    pf = pf.view(B, N, 768)\n",
    "\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=120,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f887a",
   "metadata": {},
   "source": [
    "## With Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Saved raw image to: Image_and_Patches_Samples3/1.2.276.0.7230010.3.1.4.67515890.9420.1652762406.165468_raw.jpg\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x1 must be greater than or equal to x0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 514\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# Draw & save annotated\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# After saving raw image:\u001b[39;00m\n\u001b[1;32m    513\u001b[0m annotated_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_basename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_annotated.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 514\u001b[0m \u001b[43mdraw_and_save_bboxes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# the merged image you just saved\u001b[39;49;00m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeypoints_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeypoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# from your dataset entry\u001b[39;49;00m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotated_path\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Generate & save patches\u001b[39;00m\n\u001b[1;32m    522\u001b[0m patch_arrays \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39m_gen_patches(entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_right_file_path\u001b[39m\u001b[38;5;124m'\u001b[39m], entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 375\u001b[0m, in \u001b[0;36mdraw_and_save_bboxes\u001b[0;34m(img_path, keypoints_dict, out_path, crop_size, color, thickness)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Draw and save\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x0, y0, x1, y1 \u001b[38;5;129;01min\u001b[39;00m all_boxes:\n\u001b[0;32m--> 375\u001b[0m     \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrectangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthickness\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m pil\u001b[38;5;241m.\u001b[39msave(out_path)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotated image saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/PIL/ImageDraw.py:413\u001b[0m, in \u001b[0;36mImageDraw.rectangle\u001b[0;34m(self, xy, fill, outline, width)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mdraw_rectangle(xy, fill_ink, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ink \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ink \u001b[38;5;241m!=\u001b[39m fill_ink \u001b[38;5;129;01mand\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_rectangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: x1 must be greater than or equal to x0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg            = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes   = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "        self.is_binary        = len(self.target_classes) == 2\n",
    "        self.abnormal_classify= self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra':'abnormal','oa':'abnormal','gout':'abnormal','normal':'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path','')\n",
    "            fp     = item.get('file_paths',[])\n",
    "            if isinstance(fp, str): fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class','unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis',''),\n",
    "                'keypoints': item.get('keypoints',{})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i:e for i,e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt,0) if pt else torch.zeros(34,3,112,112)\n",
    "\n",
    "        raw   = e.get('diagnosis','')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok   = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        # one merged image\n",
    "        if len(paths)==1:\n",
    "            pth = paths[0]\n",
    "            if pth and os.path.exists(pth):\n",
    "                arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "                if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            # separate left/right files\n",
    "            for side,pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        (left  if side=='left' else right).extend(lst)\n",
    "\n",
    "        # mirror if only one side present\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p,1) for p in left]\n",
    "        if right and not left:\n",
    "            left  = [cv2.flip(p,1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([])+pad17([])\n",
    "\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$','', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        # normalize section tags & strip recommendations\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]','', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_',' ')\n",
    "        text = re.sub(r'\\s+',' ', text).strip()\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts  = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p-p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads,0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids, batch_first=True,\n",
    "                                       padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs'       : imgs,\n",
    "        'patches'         : patches,\n",
    "        'input_ids'       : ids,\n",
    "        'attention_mask'  : masks,\n",
    "        'raw_reports'     : [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports' : [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model Definition (unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj    = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder  = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj     = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2,3]) if feats.ndim>2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B,N,C,H,W = patches.shape\n",
    "        p = patches.view(B*N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B,N,768)\n",
    "\n",
    "        cat, _ = self.attn(torch.cat([g,pf],1),\n",
    "                           torch.cat([g,pf],1),\n",
    "                           torch.cat([g,pf],1))\n",
    "        cat = self.norm(cat)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Bounding‐Box Drawing Helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def draw_and_save_bboxes(img_path, keypoints_dict, out_path,\n",
    "                         crop_size=(200,300), color=(255,0,0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw the left‐side keypoint boxes on the left half of the merged image,\n",
    "    and the right‐side on the right half.\n",
    "    \"\"\"\n",
    "    # Load merged image\n",
    "    merged_bgr = cv2.imread(img_path)\n",
    "    if merged_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not load {img_path}\")\n",
    "    merged_rgb = cv2.cvtColor(merged_bgr, cv2.COLOR_BGR2RGB)\n",
    "    H, W, _ = merged_rgb.shape\n",
    "    half_w = W // 2\n",
    "\n",
    "    pil = Image.fromarray(merged_rgb)\n",
    "    draw = ImageDraw.Draw(pil)\n",
    "\n",
    "    def extract_boxes(kps, x_offset, width_limit):\n",
    "        boxes = []\n",
    "        pts = kps[0]['keypoints']\n",
    "        for i in range(17):\n",
    "            x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "            if s > 0:\n",
    "                x0 = max(x - crop_size[0]//2, 0) + x_offset\n",
    "                y0 = max(y - crop_size[1]//2, 0)\n",
    "                x1 = min(x + crop_size[0]//2, width_limit) + x_offset\n",
    "                y1 = min(y + crop_size[1]//2, H)\n",
    "                boxes.append((x0, y0, x1, y1))\n",
    "        return boxes\n",
    "\n",
    "    all_boxes = []\n",
    "    # Left foot boxes on left half\n",
    "    if keypoints_dict.get('left'):\n",
    "        all_boxes += extract_boxes(keypoints_dict['left'], x_offset=0, width_limit=half_w)\n",
    "    # Right foot boxes on right half\n",
    "    if keypoints_dict.get('right'):\n",
    "        all_boxes += extract_boxes(keypoints_dict['right'], x_offset=half_w, width_limit=half_w)\n",
    "\n",
    "    # Draw and save\n",
    "    for x0, y0, x1, y1 in all_boxes:\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=color, width=thickness)\n",
    "\n",
    "    pil.save(out_path)\n",
    "    print(f\"Annotated image saved to: {out_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval Helpers (unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # generation\n",
    "            prompt = ids[:,:1]\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "            B,N,C,H,W = pts.shape\n",
    "            p = pts.view(B*N,C,H,W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf).view(B,N,768)\n",
    "            cat, _ = model.attn(torch.cat([g,pf],1),\n",
    "                                torch.cat([g,pf],1),\n",
    "                                torch.cat([g,pf],1))\n",
    "            cat = model.norm(cat)\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=cat,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt  = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1  = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2  = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Similarity\"); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON           = 'final_samples_both_only_fixed.json'\n",
    "cfg.DATASET.USE_RAW        = True\n",
    "cfg.DATASET.USE_PATCH      = True\n",
    "cfg.DATASET.REPORT         = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "cfg.DATASET.BALANCE        = False\n",
    "cfg.DATASET.AUGMENT        = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "# Create output dir\n",
    "out_dir = \"Image_and_Patches_Samples1\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Pick random sample\n",
    "sample_idx = random.choice(list(dataset.data.keys()))\n",
    "entry      = dataset.data[sample_idx]\n",
    "raw_path   = entry['file_path']\n",
    "raw_img    = Image.open(raw_path).convert('RGB')\n",
    "raw_basename = os.path.splitext(os.path.basename(raw_path))[0]\n",
    "\n",
    "# Save raw image\n",
    "raw_save_path = os.path.join(out_dir, f\"{raw_basename}_raw.jpg\")\n",
    "raw_img.save(raw_save_path)\n",
    "print(f\"Saved raw image to: {raw_save_path}\")\n",
    "\n",
    "# Draw & save annotated\n",
    "# After saving raw image:\n",
    "annotated_path = os.path.join(out_dir, f\"{raw_basename}_annotated.jpg\")\n",
    "draw_and_save_bboxes(\n",
    "    img_path=raw_save_path,            # the merged image you just saved\n",
    "    keypoints_dict=entry['keypoints'], # from your dataset entry\n",
    "    out_path=annotated_path\n",
    ")\n",
    "\n",
    "\n",
    "# Generate & save patches\n",
    "patch_arrays = dataset._gen_patches(entry['left_right_file_path'], entry['keypoints'])\n",
    "for i, patch_arr in enumerate(patch_arrays):\n",
    "    patch_img = Image.fromarray(patch_arr)\n",
    "    patch_save_path = os.path.join(out_dir, f\"{raw_basename}_patch_{i:02d}.jpg\")\n",
    "    patch_img.save(patch_save_path)\n",
    "print(f\"Saved {len(patch_arrays)} patches to: {out_dir}\")\n",
    "\n",
    "# Print distribution\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "# Split & loaders\n",
    "n = len(dataset)\n",
    "n_train = int(0.8*n)\n",
    "n_val   = int(0.1*n)\n",
    "n_test  = n - n_train - n_val\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train,n_val,n_test])\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model, optimizer, etc.\n",
    "model     = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler    = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Train one epoch (example)\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    tl = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    vl, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "    train_losses.append(tl); val_losses.append(vl); sems.append(sem)\n",
    "    print(f\"  Train Loss          : {tl:.4f}\")\n",
    "    print(f\"  Validation Loss     : {vl:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss          : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim  : {test_sem:.4f}\")\n",
    "\n",
    "# Show some random test examples\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw   = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi    = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa    = ex['patches'].unsqueeze(0).to(device)\n",
    "    prompt= ex['input_ids'][:1].unsqueeze(0).to(device)\n",
    "\n",
    "    # re‐encode & generate\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B,N,C,H,W = pa.shape\n",
    "    p = pa.view(B*N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf).view(B,N,768)\n",
    "    cat,_ = model.attn(torch.cat([g,pf],1),\n",
    "                       torch.cat([g,pf],1),\n",
    "                       torch.cat([g,pf],1))\n",
    "    cat = model.norm(cat)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=cat,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=120,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(\"Raw Report       :\\n\", raw)\n",
    "    print(\"Cleaned Report   :\\n\", clean)\n",
    "    print(\"Generated Report :\\n\", gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e3e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Patient ID: CAUOLD_CAUHGOUT0151_20120801195902_CR\n",
      "Saved raw image to: Image_and_Patches_Samples2/1.2.392.200046.100.2.1.200325.81405.20120807153110.1.1.1.1_raw.jpg\n",
      "Annotated image saved to: Image_and_Patches_Samples2/1.2.392.200046.100.2.1.200325.81405.20120807153110.1.1.1.1_annotated.jpg\n",
      "Saved 34 patches to: Image_and_Patches_Samples2\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.53 GiB of which 14.62 MiB is free. Process 2980 has 390.70 MiB memory in use. Process 44858 has 13.63 GiB memory in use. Including non-PyTorch memory, this process has 9.36 GiB memory in use. Of the allocated memory 8.59 GiB is allocated by PyTorch, and 307.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 571\u001b[0m\n\u001b[1;32m    568\u001b[0m test_loader  \u001b[38;5;241m=\u001b[39m DataLoader(test_ds,  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Model, optimizer, etc.\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m model     \u001b[38;5;241m=\u001b[39m \u001b[43mMultiModalModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m    573\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingWarmRestarts(optimizer, T_0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, T_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.53 GiB of which 14.62 MiB is free. Process 2980 has 390.70 MiB memory in use. Process 44858 has 13.63 GiB memory in use. Including non-PyTorch memory, this process has 9.36 GiB memory in use. Of the allocated memory 8.59 GiB is allocated by PyTorch, and 307.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg            = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes   = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "        self.is_binary        = len(self.target_classes) == 2\n",
    "        self.abnormal_classify= self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra':'abnormal','oa':'abnormal','gout':'abnormal','normal':'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path','')\n",
    "            fp     = item.get('file_paths',[])\n",
    "            if isinstance(fp, str): fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class','unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis',''),\n",
    "                'keypoints': item.get('keypoints',{}),\n",
    "                'patient_id': item.get('patient_id', 'unknown')\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i:e for i,e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt,0) if pt else torch.zeros(34,3,112,112)\n",
    "\n",
    "        raw   = e.get('diagnosis','')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok   = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        # one merged image\n",
    "        if len(paths)==1:\n",
    "            pth = paths[0]\n",
    "            if pth and os.path.exists(pth):\n",
    "                arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "                if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            # separate left/right files\n",
    "            for side,pth in zip(['left','right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    arr = cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst = extract(arr, kps_dict[side])\n",
    "                        (left  if side=='left' else right).extend(lst)\n",
    "\n",
    "        # mirror if only one side present\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p,1) for p in left]\n",
    "        if right and not left:\n",
    "            left  = [cv2.flip(p,1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([])+pad17([])\n",
    "\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$','', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        # normalize section tags & strip recommendations\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]','', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_',' ')\n",
    "        text = re.sub(r'\\s+',' ', text).strip()\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts  = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p-p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads,0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids, batch_first=True,\n",
    "                                       padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs'       : imgs,\n",
    "        'patches'         : patches,\n",
    "        'input_ids'       : ids,\n",
    "        'attention_mask'  : masks,\n",
    "        'raw_reports'     : [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports' : [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model Definition (unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj    = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder  = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj     = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2,3]) if feats.ndim>2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B,N,C,H,W = patches.shape\n",
    "        p = patches.view(B*N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B,N,768)\n",
    "\n",
    "        cat, _ = self.attn(torch.cat([g,pf],1),\n",
    "                           torch.cat([g,pf],1),\n",
    "                           torch.cat([g,pf],1))\n",
    "        cat = self.norm(cat)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Bounding‐Box Drawing Helper\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def draw_and_save_bboxes(img_path, keypoints_dict, out_path,\n",
    "                         crop_size=(200,300), color=(255,0,0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw every keypoint‐centered box on the merged image,\n",
    "    using the absolute x,y coords in keypoints_dict.\n",
    "    \"\"\"\n",
    "    # Load merged image\n",
    "    merged = cv2.imread(img_path)\n",
    "    if merged is None:\n",
    "        raise FileNotFoundError(f\"Could not load {img_path}\")\n",
    "    merged = cv2.cvtColor(merged, cv2.COLOR_BGR2RGB)\n",
    "    H, W, _ = merged.shape\n",
    "\n",
    "    pil  = Image.fromarray(merged)\n",
    "    draw = ImageDraw.Draw(pil)\n",
    "\n",
    "    boxes = []\n",
    "    # loop both sides\n",
    "    for side in ['left','right']:\n",
    "        entries = keypoints_dict.get(side, [])\n",
    "        for entry in entries:\n",
    "            pts = entry['keypoints']\n",
    "            # there are 17 keypoints, each (x,y,s)\n",
    "            for i in range(17):\n",
    "                x = int(pts[3*i])\n",
    "                y = int(pts[3*i+1])\n",
    "                s = pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, W)\n",
    "                    y1 = min(y + crop_size[1]//2, H)\n",
    "                    boxes.append((x0, y0, x1, y1))\n",
    "\n",
    "    # sanitize: drop degenerate\n",
    "    clean = []\n",
    "    for x0,y0,x1,y1 in boxes:\n",
    "        xa, xb = sorted((x0,x1))\n",
    "        ya, yb = sorted((y0,y1))\n",
    "        if xb>xa and yb>ya:\n",
    "            clean.append((xa,ya,xb,yb))\n",
    "\n",
    "    # draw them\n",
    "    for x0,y0,x1,y1 in clean:\n",
    "        draw.rectangle([x0,y0,x1,y1], outline=color, width=thickness)\n",
    "\n",
    "    pil.save(out_path)\n",
    "    print(f\"Annotated image saved to: {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval Helpers (unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # generation\n",
    "            prompt = ids[:,:1]\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "            B,N,C,H,W = pts.shape\n",
    "            p = pts.view(B*N,C,H,W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf).view(B,N,768)\n",
    "            cat, _ = model.attn(torch.cat([g,pf],1),\n",
    "                                torch.cat([g,pf],1),\n",
    "                                torch.cat([g,pf],1))\n",
    "            cat = model.norm(cat)\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=cat,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt  = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1  = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2  = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Similarity\"); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON           = 'final_samples_both_only_fixed.json'\n",
    "cfg.DATASET.USE_RAW        = True\n",
    "cfg.DATASET.USE_PATCH      = True\n",
    "cfg.DATASET.REPORT         = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "cfg.DATASET.BALANCE        = False\n",
    "cfg.DATASET.AUGMENT        = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "# Create output dir\n",
    "out_dir = \"Image_and_Patches_Samples2\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Pick the fixed sample by filename\n",
    "target_basename = \"1.2.392.200046.100.2.1.200325.81405.20120807153110.1.1.1.1.jpg\"\n",
    "sample_idx = next(\n",
    "    idx for idx, e in dataset.data.items()\n",
    "    if os.path.basename(e['file_path']) == target_basename\n",
    ")\n",
    "entry = dataset.data[sample_idx]\n",
    "\n",
    "# Print patient ID\n",
    "print(\"Patient ID:\", entry.get('patient_id', 'unknown'))\n",
    "\n",
    "\n",
    "\n",
    "raw_path   = entry['file_path']\n",
    "raw_img    = Image.open(raw_path).convert('RGB')\n",
    "raw_basename = os.path.splitext(os.path.basename(raw_path))[0]\n",
    "\n",
    "# Save raw image\n",
    "raw_save_path = os.path.join(out_dir, f\"{raw_basename}_raw.jpg\")\n",
    "raw_img.save(raw_save_path)\n",
    "print(f\"Saved raw image to: {raw_save_path}\")\n",
    "\n",
    "# Draw & save annotated\n",
    "# After saving raw image:\n",
    "annotated_path = os.path.join(out_dir, f\"{raw_basename}_annotated.jpg\")\n",
    "''' \n",
    "draw_and_save_bboxes(\n",
    "    img_path=raw_save_path,            # the merged image you just saved\n",
    "    keypoints_dict=entry['keypoints'], # from your dataset entry\n",
    "    out_path=annotated_path\n",
    ")\n",
    "'''\n",
    "draw_and_save_bboxes(\n",
    "    img_path=raw_path,                 # use the original merged image\n",
    "    keypoints_dict=entry['keypoints'],\n",
    "    out_path=annotated_path\n",
    ")\n",
    "\n",
    "\n",
    "# Generate & save patches\n",
    "patch_arrays = dataset._gen_patches(entry['left_right_file_path'], entry['keypoints'])\n",
    "for i, patch_arr in enumerate(patch_arrays):\n",
    "    patch_img = Image.fromarray(patch_arr)\n",
    "    patch_save_path = os.path.join(out_dir, f\"{raw_basename}_patch_{i:02d}.jpg\")\n",
    "    patch_img.save(patch_save_path)\n",
    "print(f\"Saved {len(patch_arrays)} patches to: {out_dir}\")\n",
    "\n",
    "# Print distribution\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "# Split & loaders\n",
    "n = len(dataset)\n",
    "n_train = int(0.8*n)\n",
    "n_val   = int(0.1*n)\n",
    "n_test  = n - n_train - n_val\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train,n_val,n_test])\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model, optimizer, etc.\n",
    "model     = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler    = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Train one epoch (example)\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    tl = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    vl, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "    train_losses.append(tl); val_losses.append(vl); sems.append(sem)\n",
    "    print(f\"  Train Loss          : {tl:.4f}\")\n",
    "    print(f\"  Validation Loss     : {vl:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss          : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim  : {test_sem:.4f}\")\n",
    "\n",
    "# Show some random test examples\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw   = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi    = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa    = ex['patches'].unsqueeze(0).to(device)\n",
    "    prompt= ex['input_ids'][:1].unsqueeze(0).to(device)\n",
    "\n",
    "    # re‐encode & generate\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B,N,C,H,W = pa.shape\n",
    "    p = pa.view(B*N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf).view(B,N,768)\n",
    "    cat,_ = model.attn(torch.cat([g,pf],1),\n",
    "                       torch.cat([g,pf],1),\n",
    "                       torch.cat([g,pf],1))\n",
    "    cat = model.norm(cat)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=cat,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=120,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(\"Raw Report       :\\n\", raw)\n",
    "    print(\"Cleaned Report   :\\n\", clean)\n",
    "    print(\"Generated Report :\\n\", gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033a7dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n",
      "\n",
      "Number of training samples:   1915\n",
      "Number of validation samples: 239\n",
      "Number of test samples:       240\n",
      "Total samples:                2394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_28588/2015509001.py:480: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_28588/2015509001.py:347: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f00cbfc8fc442e82e28a63a96cd23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a8edbe63be4fc7a17febc9866e626c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.3916\n",
      "  Validation Loss     : 0.9149\n",
      "  Semantic Similarity : 0.5727\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb96e7dc16c24277b8dc705fa94e430b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41047179a3bd462ea0620b5d815004d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.8510\n",
      "  Validation Loss     : 0.7991\n",
      "  Semantic Similarity : 0.5726\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af47eb141fb64bb5b5f83ab9a5dfe6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ee48081a214f95a6bb2fb828e3e3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7393\n",
      "  Validation Loss     : 0.7376\n",
      "  Semantic Similarity : 0.6567\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e937427d7748f8a44c4be44e9c107f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0273d9940a0842119fae89e1e16ac4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.6744\n",
      "  Validation Loss     : 0.7056\n",
      "  Semantic Similarity : 0.6747\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deda8e9c9eb44cff9bb1537a29902580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cb683f4deb4f0d8ed31ee75926aa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.6216\n",
      "  Validation Loss     : 0.6834\n",
      "  Semantic Similarity : 0.6158\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25b2fd9a18c4807b3b53ec58e7ec36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6245c59c346a42349c103ce337cb4656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss          : 0.7833\n",
      "Test Semantic Sim  : 0.5709\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 63 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both accessory navicular bone, type II_x000D_\n",
      "degenerative change, both feet._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both accessory navicular bone, type II_x000D_\n",
      "degenerative change, both feet._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both accessory navicular bone, type II degenerative change, both feet. <|endoftext|>\n",
      "Generated Report : \n",
      "both hallux valgus. \n",
      "\n",
      "--- Example 41 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_diffuse osteopenia\n",
      "degenerative change_x000D__x000D_[CONCLUSION    ]_x000D_diffuse osteopenia\n",
      "degenerative change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "diffuse osteopenia degenerative change <|endoftext|>\n",
      "Generated Report : \n",
      "diffuse osteopenia degenerative change \n",
      "\n",
      "--- Example 51 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "osteopenia._x000D_\n",
      "degenerative change._x000D_\n",
      "old fracture, left lateral malleolus._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "osteopenia._x000D_\n",
      "degenerative change._x000D_\n",
      "old fracture, left lateral malleolus._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "osteopenia. degenerative change. old fracture, left lateral malleolus. <|endoftext|>\n",
      "Generated Report : \n",
      "osteopenia. degenerative change \n",
      "\n",
      "--- Example 44 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality. No bony abnormality. <|endoftext|>\n",
      "Generated Report : \n",
      "No significant interval change since last study. \n",
      "\n",
      "--- Example 39 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "no significant bony lesion on radiographs._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no significant bony lesion on radiographs. <|endoftext|>\n",
      "Generated Report : \n",
      "no significant bony lesion on radiographs. \n",
      "\n",
      "--- Example 87 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_No significant interval change_x000D__x000D_[CONCLUSION    ]_x000D_No significant interval change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "No significant interval change <|endoftext|>\n",
      "Generated Report : \n",
      "No significant interval change \n",
      "\n",
      "--- Example 220 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "1. A separate ossicle, lateral aspect of cuboid bone, left._x000D_\n",
      " --- Os peroneum rather than fracture._x000D_\n",
      "2. Otherwise, no bony abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "1. A separate ossicle, lateral aspect of cuboid bone, left. --- Os peroneum rather than fracture. 2. Otherwise, no bony abnormality. <|endoftext|>\n",
      "Generated Report : \n",
      "1. Soft tissue swelling in medial portion of right 1st MTP joint. 2. Enthesophyte in plantar aspect of both calcaneus. \n",
      "\n",
      "--- Example 110 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "R/O old avulsion, Rt. fibulat distal tip_x000D_\n",
      "Degenerative change_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "R/O old avulsion, Rt. fibulat distal tip_x000D_\n",
      "Degenerative change_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "R/O old avulsion, Rt. fibulat distal tip Degenerative change <|endoftext|>\n",
      "Generated Report : \n",
      "Rt. 1st MTP joint, R/O gout. \n",
      "\n",
      "--- Example 219 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no bony lesion. <|endoftext|>\n",
      "Generated Report : \n",
      "no significant bony lesion on radiographs. \n",
      "\n",
      "--- Example 81 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change._x000D_\n",
      "osteopenia._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change._x000D_\n",
      "osteopenia._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change. osteopenia. <|endoftext|>\n",
      "Generated Report : \n",
      "degenerative change. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B, N, 768)\n",
    "\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            prompt = ids[:, :1]\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf)\n",
    "            pf = pf.view(B, N, 768)\n",
    "\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=comb,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=100,\n",
    "                num_beams=5,              # use beam search\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,   # avoid trivial repeats\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "           )\n",
    "\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "# split into train / val / test\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "# print sample counts\n",
    "print(f\"\\nNumber of training samples:   {len(train_ds)}\")\n",
    "print(f\"Number of validation samples: {len(val_ds)}\")\n",
    "print(f\"Number of test samples:       {len(test_ds)}\")\n",
    "print(f\"Total samples:                {len(train_ds) + len(val_ds) + len(test_ds)}\\n\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss          : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim  : {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    prompt = ex['input_ids'][:1].unsqueeze(0).to(device)\n",
    "\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf)\n",
    "    pf = pf.view(B, N, 768)\n",
    "\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=120,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd483280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 12:27:59.393971: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 12:27:59.400715: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746559679.408429   29687 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746559679.410798   29687 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746559679.416927   29687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746559679.416934   29687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746559679.416935   29687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746559679.416936   29687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-06 12:27:59.419008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n",
      "\n",
      "Number of training samples:   1915\n",
      "Number of validation samples: 239\n",
      "Number of test samples:       240\n",
      "Total samples:                2394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29687/2799820574.py:481: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_29687/2799820574.py:347: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:17,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:15,  3.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:15,  3.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:14,  3.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:13,  4.03it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:13,  3.97it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:01<00:14,  3.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:13,  3.88it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:12,  3.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:02<00:12,  4.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:02<00:12,  3.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:12,  3.92it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:03<00:11,  3.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:03<00:11,  3.90it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:03<00:11,  3.92it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:04<00:11,  3.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:04<00:10,  4.02it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:04<00:10,  4.00it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:04<00:10,  4.07it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:09,  4.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:05<00:10,  3.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:05<00:10,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:05<00:10,  3.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:06<00:09,  3.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:06<00:09,  3.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:06<00:09,  3.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:07<00:09,  3.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:07<00:09,  3.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:07<00:09,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:07<00:08,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:08<00:08,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:08<00:08,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:08<00:07,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:09<00:07,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:09<00:07,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:09<00:06,  3.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:09<00:06,  3.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:10<00:05,  3.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:10<00:05,  3.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:10<00:05,  3.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:10<00:04,  3.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:11<00:04,  3.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:11<00:04,  3.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:11<00:04,  3.85it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:11<00:03,  3.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:12<00:03,  3.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:12<00:03,  3.87it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:12<00:03,  3.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:12<00:02,  4.00it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:13<00:02,  4.08it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:13<00:02,  4.00it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:13<00:02,  3.93it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:13<00:01,  3.88it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:14<00:01,  3.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:14<00:01,  3.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:14<00:01,  3.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:15<00:00,  3.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:15<00:00,  3.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:15<00:00,  3.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707d26291a7f40eeb18146e47d398a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e237f442a04773ae1f9c503936875d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.4324\n",
      "  Validation Loss     : 0.9134\n",
      "  Semantic Similarity : 0.4142\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:18,  3.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:17,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:16,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:15,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:15,  3.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:15,  3.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:15,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:14,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:14,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:02<00:13,  3.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:03<00:13,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:13,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:03<00:13,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:03<00:13,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:12,  3.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:04<00:12,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:04<00:11,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:05<00:11,  3.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:05<00:11,  3.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:11,  3.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:05<00:11,  3.45it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:06<00:11,  3.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:06<00:11,  3.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:06<00:10,  3.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:07<00:10,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:07<00:09,  3.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:07<00:09,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:08<00:09,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:08<00:09,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:08<00:08,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:08<00:08,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:09<00:08,  3.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:09<00:07,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:09<00:07,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:10<00:07,  3.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:10<00:06,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:10<00:06,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:10<00:06,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:11<00:06,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:11<00:05,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:11<00:05,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:12<00:05,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:12<00:04,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:12<00:04,  3.45it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:13<00:04,  3.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:13<00:04,  3.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:13<00:03,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:13<00:03,  3.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:14<00:03,  3.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:14<00:02,  3.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:14<00:02,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:14<00:02,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:15<00:01,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:15<00:01,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:15<00:01,  3.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:16<00:01,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:16<00:00,  3.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:16<00:00,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:17<00:00,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070c363552e345d5ae5603027f42d4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a8782da81546888b29a4c2e3d83516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.8570\n",
      "  Validation Loss     : 0.7999\n",
      "  Semantic Similarity : 0.4006\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:17,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:16,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:16,  3.45it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:15,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:15,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:15,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:15,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:14,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:14,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:02<00:13,  3.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:03<00:13,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:13,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:03<00:13,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:03<00:13,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:12,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:04<00:12,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:04<00:12,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:05<00:11,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:05<00:11,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:11,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:06<00:11,  3.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:06<00:11,  3.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:06<00:11,  3.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:06<00:10,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:07<00:10,  3.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:07<00:10,  3.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:07<00:09,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:08<00:09,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:08<00:09,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:08<00:08,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:08<00:08,  3.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:09<00:08,  3.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:09<00:08,  3.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:09<00:07,  3.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:10<00:07,  3.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:10<00:07,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:10<00:06,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:11<00:06,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:11<00:06,  3.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:11<00:05,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:11<00:05,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:12<00:05,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:12<00:04,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:12<00:04,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:13<00:04,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:13<00:04,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:13<00:03,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:13<00:03,  3.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:14<00:03,  3.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:14<00:02,  3.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:14<00:02,  3.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:14<00:02,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:15<00:01,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:15<00:01,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:15<00:01,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:16<00:01,  3.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:16<00:00,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:16<00:00,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:17<00:00,  3.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ad75be52d1402a9390301a11c28856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89a18d287594eda893f11e6696a34c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7441\n",
      "  Validation Loss     : 0.7501\n",
      "  Semantic Similarity : 0.3891\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:18,  3.15it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:18,  3.15it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:18,  3.07it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:17,  3.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:16,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:16,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:16,  3.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:15,  3.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:15,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:14,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:03<00:14,  3.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:13,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:03<00:13,  3.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:04<00:13,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:13,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:04<00:13,  3.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:05<00:12,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:05<00:12,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:05<00:11,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:11,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:06<00:11,  3.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:06<00:11,  3.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:06<00:11,  3.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:07<00:11,  3.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:07<00:10,  3.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:07<00:10,  3.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:08<00:09,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:08<00:09,  3.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:08<00:09,  3.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:09<00:08,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:09<00:08,  3.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:09<00:08,  3.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:09<00:08,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:10<00:07,  3.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:10<00:07,  3.19it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:10<00:07,  3.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:11<00:06,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:11<00:06,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:11<00:06,  3.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:12<00:05,  3.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:12<00:05,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:12<00:05,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:12<00:05,  3.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:13<00:04,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:13<00:04,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:13<00:04,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:14<00:03,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:14<00:03,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:14<00:03,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:14<00:02,  3.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:15<00:02,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:15<00:02,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:15<00:02,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:16<00:01,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:16<00:01,  3.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:16<00:01,  3.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:17<00:00,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:17<00:00,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:17<00:00,  3.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5636682bd7f44719b75972e20434db3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f5e51033834b98aa24646e281a1146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.6683\n",
      "  Validation Loss     : 0.7221\n",
      "  Semantic Similarity : 0.3977\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:17,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:16,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:16,  3.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:15,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:15,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:15,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:15,  3.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:14,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:14,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:02<00:13,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:03<00:13,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:13,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:03<00:12,  3.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:03<00:13,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:12,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:04<00:12,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:04<00:12,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:05<00:11,  3.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:05<00:11,  3.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:11,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:05<00:11,  3.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:06<00:11,  3.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:06<00:11,  3.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:06<00:10,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:07<00:10,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:07<00:09,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:07<00:09,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:08<00:09,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:08<00:08,  3.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:08<00:08,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:08<00:08,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:09<00:08,  3.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:09<00:07,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:09<00:07,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:10<00:07,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:10<00:06,  3.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:10<00:06,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:10<00:06,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:11<00:05,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:11<00:05,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:11<00:05,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:12<00:05,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:12<00:04,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:12<00:04,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:12<00:04,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:13<00:03,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:13<00:03,  3.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:13<00:03,  3.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:13<00:02,  3.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:14<00:02,  3.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:14<00:02,  3.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:14<00:02,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:15<00:01,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:15<00:01,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:15<00:01,  3.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:15<00:01,  3.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:16<00:00,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:16<00:00,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:16<00:00,  3.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c651ea06dba846dfbcd1a1debabcd907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af5aceed7e6439890f379927ed57367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.6232\n",
      "  Validation Loss     : 0.6983\n",
      "  Semantic Similarity : 0.3572\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAwk1JREFUeJzs3Xd0VOXaxuHfzKR3ICShpNA7IfReBCkKUlSK0qLgAQVFjo0jonJUjg2VoigqAVQQFREbiBQhJHRC76QBSegJSUid+f5A8p0cASlJdsp9rbXXInv23nPPEMjOM+/7vCabzWZDRERERERERESkCJmNDiAiIiIiIiIiImWPilIiIiIiIiIiIlLkVJQSEREREREREZEip6KUiIiIiIiIiIgUORWlRERERERERESkyKkoJSIiIiIiIiIiRU5FKRERERERERERKXIqSomIiIiIiIiISJGzMzpAUbNarZw6dQp3d3dMJpPRcURERKQYsdlsXLp0icqVK2M267O7G9E9lYiIiFzPzd5Tlbmi1KlTp/D39zc6hoiIiBRj8fHxVK1a1egYxZruqUREROTv/N09VZkrSrm7uwNX3hgPDw+D04iIiEhxkpKSgr+/f979glyf7qlERETkem72nqrMFaWuDi/38PDQDZSIiIhck6aj/T3dU4mIiMjf+bt7KjVLEBERERERERGRIqeilIiIiIiIiIiIFDkVpUREREREREREpMiVuZ5SIiJScuTm5pKdnW10DClF7O3tsVgsRscQEZEyxGq1kpWVZXQMkQJVUPdUKkqJiEixY7PZSExM5OLFi0ZHkVLIy8sLPz8/NTMXEZFCl5WVRXR0NFar1egoIgWuIO6pVJQSEZFi52pBysfHBxcXFxUPpEDYbDbS09M5ffo0AJUqVTI4kYiIlGY2m42EhAQsFgv+/v6YzeqeI6VDQd5TqSglIiLFSm5ubl5BqkKFCkbHkVLG2dkZgNOnT+Pj46OpfCIiUmhycnJIT0+ncuXKuLi4GB1HpEAV1D2VSrUiIlKsXO0hpZs3KSxXv7fUr0xERApTbm4uAA4ODgYnESkcBXFPpaKUiIgUS5qyJ4VF31siIlKU9HNHSquC+N5WUUpERERERERERIqcilIiIiLFVFBQEO+//77RMURERERKtJiYGEwmE1FRUYX2HK+88gpNmjS5o2v8b85169ZhMpkKZEVqk8nEsmXL7vg6BU1FKRERkTtkMpluuL3yyiu3dd2tW7fy2GOP3VG2zp07M2HChDu6hoiIiJQdZ86cYezYsQQEBODo6Iifnx89evRg48aNRke7KSNHjqRfv3759vn7+5OQkEDDhg1v+7rff/89rVu3xtPTE3d3dxo0aJDvHuuZZ55h9erVt339gsp5PQkJCfTq1QsomiLdzdLqeyIiIncoISEh789ff/01U6ZM4dChQ3n73Nzc8v5ss9nIzc3Fzu7vfwRXrFixYIOKiIiI/I3777+frKws5s+fT/Xq1UlKSmL16tWcO3fO6Gi3zWKx4Ofnd9vnr169mkGDBvH6669z3333YTKZ2L9/P6tWrco7xs3NLd89nxE5ryUrKwsHB4cCv25B0UgpERGRO+Tn55e3eXp6YjKZ8r4+ePAg7u7u/PrrrzRr1gxHR0fCw8M5duwYffv2xdfXFzc3N1q0aMHvv/+e77r/O33PZDLx6aef0r9/f1xcXKhVqxbLly+/o+zfffcdDRo0wNHRkaCgIN599918j3/44YfUqlULJycnfH19eeCBB/Ie+/bbb2nUqBHOzs5UqFCBbt26kZaWdkd5RERExDgXL15kw4YNvPnmm3Tp0oXAwEBatmzJpEmTuO+++/IdN2rUKCpWrIiHhwd33XUXu3btynv86lS2zz//nICAANzc3Hj88cfJzc3lrbfews/PDx8fH15//fV8zz99+nQaNWqEq6sr/v7+PP7446SmpuY9HhYWhpeXFytXrqRevXq4ubnRs2fPvA8IX3nlFebPn88PP/yQN2J93bp11xwZtG/fPnr37o2Hhwfu7u506NCBY8eOXfN9+fHHH2nXrh3PPvssderUoXbt2vTr14/Zs2f/5TVfdXXE1htvvIGvry9eXl5MnTqVnJwcnn32WcqXL0/VqlWZN29e3jl/N4Lp3LlzDBkyhCpVquDi4kKjRo1YtGhRvmM6d+7MuHHjmDBhAt7e3vTo0QPIP32vWrVqAISEhGAymejcuTPr16/H3t6exMTEfNebMGECHTp0uGaegqCilIiIFGs2m430rBxDNpvNVmCv44UXXuA///kPBw4coHHjxqSmpnLPPfewevVqdu7cSc+ePenTpw9xcXE3vM6rr77KwIED2b17N/fccw8PP/ww58+fv61M27dvZ+DAgQwePJg9e/bwyiuv8NJLLxEWFgbAtm3bePLJJ5k6dSqHDh1ixYoVdOzYEbgyOmzIkCE88sgjHDhwgHXr1jFgwIACfc+kbLNa9b0kIqVLSbinuTraZ9myZWRmZl73uAcffJDTp0/z66+/sn37dpo2bUrXrl3z3ZMcO3aMX3/9lRUrVrBo0SI+++wz7r33Xk6cOMEff/zBm2++yeTJk9m8eXPeOWazmRkzZrBv3z7mz5/PmjVreO655/I9d3p6Ou+88w4LFy5k/fr1xMXF8cwzzwBXptANHDgwr1CVkJBA27Zt/5L/5MmTdOzYEUdHR9asWcP27dt55JFHyMnJuebr9fPzY9++fezdu/em3ser1qxZw6lTp1i/fj3Tp0/n5Zdfpnfv3pQrV47NmzczZswY/vGPf3DixImbul5GRgbNmjXj559/Zu/evTz22GMMGzaMLVu25Dtu/vz5ODg4sHHjRubMmfOX61w9/vfffychIYGlS5fSsWNHqlevzsKFC/OOy87O5ssvv+SRRx65pdd9KzR9rwClZeYQFhHD+sNn+Gp0ayxmLf0pInKnLmfnUn/KSkOee//UHrg4FMyPyqlTp3L33XfnfV2+fHmCg4Pzvv73v//N999/z/Llyxk3btx1rzNy5EiGDBkCwBtvvMGMGTPYsmULPXv2vOVM06dPp2vXrrz00ksA1K5dm/379/P2228zcuRI4uLicHV1pXfv3ri7uxMYGEhISAhwpSiVk5PDgAEDCAwMBKBRo0a3nEHkWqxWG2O/3E7Dyp480aUmZt1TiUgpUBLuaezs7AgLC2P06NHMmTOHpk2b0qlTJwYPHkzjxo0BCA8PZ8uWLZw+fRpHR0cA3nnnHZYtW8a3336b1w/TarXy+eef4+7uTv369enSpQuHDh3il19+wWw2U6dOHd58803Wrl1Lq1atAPL1aAoKCuK1115jzJgxfPjhh3n7s7OzmTNnDjVq1ABg3LhxTJ06FbhSVHN2diYzM/OG09Vmz56Np6cnixcvxt7eHrhyH3Q948ePZ8OGDTRq1IjAwEBat25N9+7defjhh/Peg2spX748M2bMyHu9b731Funp6fzrX/8CYNKkSfznP/8hPDycwYMHX/c6V1WpUiWvAHc118qVK1myZAktW7bM21+rVi3eeuut617naouIChUq5HufHn30UebNm8ezzz4LXBkhlpGRwcCBA/822+3SSKkCZDLBx38cY3P0edYdOm10HBERKUaaN2+e7+vU1FSeeeYZ6tWrh5eXF25ubhw4cOBvR0pdvSEEcHV1xcPDg9Onb+9nzoEDB2jXrl2+fe3atePIkSPk5uZy9913ExgYSPXq1Rk2bBhffvkl6enpAAQHB9O1a1caNWrEgw8+yNy5c7lw4cJt5RD5X38cPsPKfUm8u+owoxdsI/lyttGRRETKjPvvv59Tp06xfPlyevbsybp162jatGneSOpdu3aRmppKhQoV8kZWubm5ER0dnW/6W1BQEO7u7nlf+/r6Ur9+fcxmc759/30f8/vvv9O1a1eqVKmCu7s7w4YN49y5c3n3HwAuLi55BSmASpUq3fK9UFRUFB06dMgrSP0dV1dXfv75Z44ePcrkyZNxc3Pjn//8Jy1btsyX7X81aNDgL6/3vz/Es1gsVKhQ4abz5+bm8u9//5tGjRpRvnx53NzcWLly5V/uH5s1a3ZT1/tfI0eO5OjRo2zatAm4Ml1y4MCBuLq63tb1boZGShUgFwc7BrcM4JP1xwmLiKFrPV+jI4mIlHjO9hb2T+1h2HMXlP/9Yf7MM8+watUq3nnnHWrWrImzszMPPPAAWVlZN7zO/948mUwmrFZrgeX8b+7u7uzYsYN169bx22+/MWXKFF555RW2bt2Kl5cXq1atIiIigt9++42ZM2fy4osvsnnz5rw+BSK3q0tdH966vzGTf9jL6oOnuW9WOHOGNqNeJQ+jo4mI3LaSdE/j5OTE3Xffzd13381LL73EqFGjePnllxk5ciSpqalUqlSJdevW/eU8Ly+vvD9f657lRvcxMTEx9O7dm7Fjx/L6669Tvnx5wsPDefTRR8nKysLFxeW6173V9gHOzs63dPxVNWrUoEaNGowaNYoXX3yR2rVr8/XXXxMaGnrN42/1Pfg7b7/9Nh988AHvv/9+Xu+tCRMm/OX+8XaLSD4+PvTp04d58+ZRrVo1fv3112v+PRckFaUK2LDWgXy64TgbjpzlSNIlavm6//1JIiJyXSaTqcCm0BUnGzduZOTIkfTv3x+4MnIqJiamSDPUq1fvL8s7b9y4kdq1a2OxXLl5tbOzo1u3bnTr1o2XX34ZLy8v1qxZw4ABAzCZTLRr14527doxZcoUAgMD+f7775k4cWKRvg4pnQa28Kd+ZQ/+sXA7sefS6f/hRqYNaET/kKpGRxMRuS0l+Z6mfv36eU2ymzZtSmJiInZ2dgQFBRXYc2zfvh2r1cq7776bN7poyZIlt3wdBwcHcnNzb3hM48aNmT9/PtnZ2Tc9Wup/BQUF4eLiUqSLvGzcuJG+ffsydOhQ4MoUycOHD1O/fv1buo6DgwPANd+nUaNGMWTIEKpWrUqNGjX+Mqq+oGn6XgHzL+9Ctz9HSM2PjDE2jIiIFFu1atVi6dKlREVFsWvXLh566KFCG/F05swZoqKi8m1JSUn885//ZPXq1fz73//m8OHDzJ8/n1mzZuX1Kvjpp5+YMWMGUVFRxMbGsmDBAqxWK3Xq1GHz5s288cYbbNu2jbi4OJYuXcqZM2eoV69eobwGKZsaVvHkp/Ht6Vi7IhnZVp7+ehcv/7CXrJzC+bciIlLWnTt3jrvuuosvvviC3bt3Ex0dzTfffMNbb71F3759AejWrRtt2rShX79+/Pbbb8TExBAREcGLL77Itm3bbvu5a9asSXZ2NjNnzuT48eMsXLjwmk26/05QUBC7d+/m0KFDnD17luzsv04BHzduHCkpKQwePJht27Zx5MgRFi5cyKFDh655zVdeeYXnnnuOdevWER0dzc6dO3nkkUfIzs7O1zO0sNWqVStvpPqBAwf4xz/+QVJS0i1fx8fHB2dnZ1asWEFSUhLJycl5j/Xo0QMPDw9ee+21644AK0gqShWCke2CAPhu+0n1QBARkWuaPn065cqVo23btvTp04cePXrQtGnTQnmur776ipCQkHzb3Llzadq0KUuWLGHx4sU0bNiQKVOmMHXqVEaOHAlcGYK/dOlS7rrrLurVq8ecOXNYtGgRDRo0wMPDg/Xr13PPPfdQu3ZtJk+ezLvvvkuvXr0K5TVI2VXO1YF5I1vw5F01AZgfGcvgTyJJTM4wOJmISOnj5uZGq1ateO+99+jYsSMNGzbkpZdeYvTo0cyaNQu4MuLrl19+oWPHjoSGhlK7dm0GDx5MbGwsvr6338ImODiY6dOn8+abb9KwYUO+/PJLpk2bdsvXGT16NHXq1KF58+ZUrFjxL6PC4UqD7zVr1pCamkqnTp1o1qwZc+fOve6oqU6dOnH8+HGGDx9O3bp16dWrF4mJifz222/UqVPnljPersmTJ9O0aVN69OhB586d8fPzo1+/frd8HTs7O2bMmMHHH39M5cqV8wqOcGUFxJEjR5Kbm8vw4cMLMP21mWxlbO3mlJQUPD09SU5OxsOjcPoS2Gw2er6/gUNJl5h8bz1GdaheKM8jIlIaZWRkEB0dTbVq1XBycjI6jpRCN/oeK4r7hNLCiPdq9YEkJnwdxaWMHLzdHJg5pCltalQokucWEblVuqeRkurRRx/lzJkzLF++/IbHFcQ9lUZKFQKTyZQ3Wmp+ZAy51jJV9xMREREpFF3r+fLT+PbU9XPnbGoWQz/bzNz1x2+5wa2IiIj8VXJyMuHh4Xz11VeMHz++SJ5TRalC0q9JFTyd7Yk/f5k1B29vqW4RERERyS+wgivfP96O/iFVyLXaeP2XAzzx1Q5SM3OMjiYiIlKi9e3bl+7duzNmzJgi65WlolQhcXawMLilPwBhEdEGpxEREREpPZwdLEwfGMy/+zbA3mLilz2J9J0VztHTl4yOJiIiUmKtW7eO9PR03nvvvSJ7ThWlCtGw1oGYTbDx6DkOJ+kmSURERKSgmEwmhrUJYvFjbfD1cOTYmTT6ztrIL3sSjI4mIiIiN0lFqUJUtZwL3ev7ARAWEWNsGBEREZFSqFlgOX4a34HW1cuTlpXL41/u4I1fDpCTazU6mogIgPreSalVEN/bhhal1q9fT58+fahcuTImk4lly5bd9LkbN27Ezs6OJk2aFFq+gnC14fnSHSdITs82NoyIiIhIKVTR3ZEvHm3FYx2vrHj8yfrjDP1sM2cuZRqcTETKMovFAkBWVpbBSUQKR3p6OgD29va3fQ27ggpzO9LS0ggODuaRRx5hwIABN33exYsXGT58OF27diUpKakQE965VtXKU6+SBwcSUvh6WxyPdaxhdCQRERGRUsfOYuZf99Sjib8Xz36zi03Hz9NnZjizH25Ks8ByRscTkTLIzs4OFxcXzpw5g729PWazJipJ6WCz2UhPT+f06dN4eXnlFWBvh6FFqV69etGrV69bPm/MmDE89NBDWCyWWxpdZQSTyURo2yCe+2438yNiebR9dSxmk9GxREREREqlexpVoravO/9YuI1jZ9IY/EkkU3rXZ2jrQEwm3YOJSNExmUxUqlSJ6OhoYmNjjY4jUuC8vLzw8/O7o2sYWpS6HfPmzeP48eN88cUXvPbaa0bHuSn3NanMtF8PcPLiZX4/kESPBnf2lyYiIqVT586dadKkCe+//z4AQUFBTJgwgQkTJlz3HJPJxPfff0+/fv3u6LkL6joixUFNHzd+GNee577dxS97Ennph33sjLvI6/0b4exw+5/miojcKgcHB2rVqqUpfFLq2Nvb39EIqatKVFHqyJEjvPDCC2zYsAE7u5uLnpmZSWbm//cTSElJKax41+Vkb2FIywA+XHeMsI0xKkqJiJQyffr0ITs7mxUrVvzlsQ0bNtCxY0d27dpF48aNb+m6W7duxdXVtaBiAvDKK6+wbNkyoqKi8u1PSEigXLnCneIUFhbGhAkTuHjxYqE+jwiAm6Mdsx9qyqcbovnPioMs3XmS/QkpfDysGYEVCvbflYjIjZjNZpycnIyOIVIslZhJrbm5uTz00EO8+uqr1K5d+6bPmzZtGp6ennmbv79/Iaa8vqGtA7GYTUQeP8eBhKIvjImISOF59NFHWbVqFSdOnPjLY/PmzaN58+a3XJACqFixIi4uLgUR8W/5+fnh6OhYJM8lUlRMJhOjO1bni0db4e3mwMHES/SeGc7qA8W7J6mIiEhZUWKKUpcuXWLbtm2MGzcOOzs77OzsmDp1Krt27cLOzo41a9Zc87xJkyaRnJyct8XHxxdx8isqeznT888RUvMjYgzJICIihaN3795UrFiRsLCwfPtTU1P55ptvePTRRzl37hxDhgyhSpUquLi40KhRIxYtWnTD6wYFBeVN5YMrI4Y7duyIk5MT9evXZ9WqVX855/nnn6d27dq4uLhQvXp1XnrpJbKzr6z+GhYWxquvvsquXbswmUyYTKa8zP+7Cu6ePXu46667cHZ2pkKFCjz22GOkpqbmPT5y5Ej69evHO++8Q6VKlahQoQJPPPFE3nPdjri4OPr27YubmxseHh4MHDgw34Imu3btokuXLri7u+Ph4UGzZs3Ytm0bALGxsfTp04dy5crh6upKgwYN+OWXX247i5QubWpU4KfxHWga4MWljBwenb+N6b8dIteqZdpFRESMVGKm73l4eLBnz558+z788EPWrFnDt99+S7Vq1a55nqOjY7H55HdkuyB+3pPA9ztP8nzPupRzdTA6koiIFAA7OzuGDx9OWFgYL774Yl4z5W+++Ybc3FyGDBlCamoqzZo14/nnn8fDw4Off/6ZYcOGUaNGDVq2bPm3z2G1WhkwYAC+vr5s3ryZ5OTka/aacnd3JywsjMqVK7Nnzx5Gjx6Nu7s7zz33HIMGDWLv3r2sWLGC33//HQBPT8+/XCMtLY0ePXrQpk0btm7dyunTpxk1ahTjxo3LV3hbu3YtlSpVYu3atRw9epRBgwbRpEkTRo8efcvvodVqzStI/fHHH+Tk5PDEE08waNAg1q1bB8DDDz9MSEgIH330ERaLhaioqLwliJ944gmysrJYv349rq6u7N+/Hzc3t1vOIaWXn6cTix9rw+s/72d+ZCwz1hwl6kQyHwxqonsyERERgxhalEpNTeXo0aN5X0dHRxMVFUX58uUJCAhg0qRJnDx5kgULFmA2m2nYsGG+8318fHBycvrL/uKqeWA5GlT2YN+pFBZvjWds5xpGRxIRKf5sNshON+a57V3gJlfreuSRR3j77bf5448/6Ny5M3Bl6t7999+fN4X8mWeeyTt+/PjxrFy5kiVLltxUUer333/n4MGDrFy5ksqVKwPwxhtv/GUV28mTJ+f9OSgoiGeeeYbFixfz3HPP4ezsjJubG3Z2djdcKeWrr74iIyODBQsW5PW0mjVrFn369OHNN9/E19cXgHLlyjFr1iwsFgt169bl3nvvZfXq1bdVlFq9ejV79uwhOjo6b6r9ggULaNCgAVu3bqVFixbExcXx7LPPUrduXQBq1aqVd35cXBz3338/jRo1AqB69eq3nEFKPwc7M6/2bUiTAC8mLd3D+sNn6D0znDlDm9Go6l8LtCIiIlK4DC1Kbdu2jS5duuR9PXHiRABGjBhBWFgYCQkJxMXFGRWvwJlMJka2DeLZb3ezMDKG0R2qYWcpMTMoRUSMkZ0Ob1Q25rn/dQocbq4hct26dWnbti2ff/45nTt35ujRo2zYsIGpU6cCV3ojvvHGGyxZsoSTJ0+SlZVFZmbmTfeMOnDgAP7+/nkFKYA2bdr85bivv/6aGTNmcOzYMVJTU8nJycHDw+OmnuO/nys4ODhfk/V27dphtVo5dOhQXlGqQYMG+VZdqVSp0l9GNd/Kc/r7++fr/Vi/fn28vLw4cOAALVq0YOLEiYwaNYqFCxfSrVs3HnzwQWrUuPIBz5NPPsnYsWP57bff6NatG/fff/9t9fGSsqF/SFXq+nkw5ovtxJ5L5/45EbzWtyEDWxjTe1RERKSsMrQi0rlzZ2w221+2q1MDwsLC8obsX8srr7zyl9WDirs+wZUp7+rAqeQMVu1Xk00RkdLk0Ucf5bvvvuPSpUvMmzePGjVq0KlTJwDefvttPvjgA55//nnWrl1LVFQUPXr0KNAloiMjI3n44Ye55557+Omnn9i5cycvvvhioS1DfXXq3FUmkwmr1VoozwVXfu7v27ePe++9lzVr1lC/fn2+//57AEaNGsXx48cZNmwYe/bsoXnz5sycObPQskjJV6+SB8vHtadrXR+ycqw8991uJi3dTUZ2rtHRREREyowS01OqtHCyt/BQywBmrT3KvIgYejWqZHQkEZHizd7lyoglo577FgwcOJCnnnqKr776igULFjB27Ni8/lIbN26kb9++DB06FLjSQ+nw4cPUr1//pq5dr1494uPjSUhIoFKlKz87Nm3alO+YiIgIAgMDefHFF/P2xcbG5jvGwcGB3Nwb/9Jdr149wsLCSEtLyxsttXHjRsxmM3Xq1LmpvLfq6uuLj4/PGy21f/9+Ll68mO89ql27NrVr1+bpp59myJAhzJs3j/79+wPg7+/PmDFjGDNmDJMmTWLu3LmMHz++UPJK6eDpbM/c4c2ZvfYo038/zKIt8ew7lcJHQ5tRxcvZ6HgiIiKlnuaOGWBo60AsZhNbos+z71Sy0XFERIo3k+nKFDojtpvsJ3WVm5sbgwYNYtKkSSQkJDBy5Mi8x2rVqsWqVauIiIjgwIED/OMf/8i3stzf6datG7Vr12bEiBHs2rWLDRs25Cs+XX2OuLg4Fi9ezLFjx5gxY0beSKKrgoKC8no4nj17lszMzL8818MPP4yTkxMjRoxg7969rF27lvHjxzNs2LC8qXu3Kzc3l6ioqHzbgQMH6NatG40aNeLhhx9mx44dbNmyheHDh9OpUyeaN2/O5cuXGTduHOvWrSM2NpaNGzeydetW6tWrB8CECRNYuXIl0dHR7Nixg7Vr1+Y9JnIjZrOJ8V1rERbaEi8Xe3afSKb3jA2EHzlrdDQREZFST0UpA/h5OtGr4ZUGs/MjYowNIyIiBerRRx/lwoUL9OjRI1//p8mTJ9O0aVN69OhB586d8fPzo1+/fjd9XbPZzPfff8/ly5dp2bIlo0aN4vXXX893zH333cfTTz/NuHHjaNKkCREREbz00kv5jrn//vvp2bMnXbp0oWLFiixatOgvz+Xi4sLKlSs5f/48LVq04IEHHqBr167MmjXr1t6Ma0hNTSUkJCTf1qdPH0wmEz/88APlypWjY8eOdOvWjerVq/P1118DYLFYOHfuHMOHD6d27doMHDiQXr168eqrrwJXil1PPPEE9erVo2fPntSuXZsPP/zwjvNK2dGpdkV+HNeeRlU8uZCezfDPNzN77VGsVpvR0UREREotk81mK1M/aVNSUvD09CQ5OfmWG78WpO2x57n/o0gc7MxsmtSV8lqKWEQEgIyMDKKjo6lWrRpOTk5Gx5FS6EbfY8XlPuFaZs+ezdtvv01iYiLBwcHMnDnzplZuXLx4MUOGDKFv374sW7Ysb//SpUuZM2cO27dv5/z58+zcuZMmTZrcdJ7i/F7diYzsXF7+YR9fb4sH4O76vrw7MBgPJ/u/OVNERESuutn7BI2UMkjTgHI0quJJVo6VRVtKzwqDIiIiUvC+/vprJk6cyMsvv8yOHTsIDg6mR48enD59+obnxcTE8Mwzz9ChQ4e/PJaWlkb79u158803Cyt2ieRkb+HNBxrznwGNcLCYWbU/iftmhnMo8ZLR0UREREodFaUMYjKZGNk2CIAvNsWSnVt4qxWJiIhIyTZ9+nRGjx5NaGgo9evXZ86cObi4uPD5559f95zc3FwefvhhXn31VapXr/6Xx4cNG8aUKVPo1q1bYUYvsQa3DOCbMW2o4uVMzLl0+s3eyA9RJ42OJSIiUqqoKGWg3sGV8HZzICE5g9/23XyzWxERESk7srKy2L59e77ikdlsplu3bkRGRl73vKlTp+Lj48Ojjz5aIDkyMzNJSUnJt5V2wf5e/Di+PR1qeXM5O5enFkfxyvJ9ZOXow0QREZGCoKKUgRztLDzUMgCAsIhog9OIiIhIcXT27Flyc3P/svKhr68viYmJ1zwnPDyczz77jLlz5xZYjmnTpuHp6Zm3+fv7F9i1i7Pyrg6EhbZkXJeaAIRFxPDQ3E0kpWQYnExERKTkU1HKYENbB2JnNrE15gJ7TyYbHUdERERKuEuXLjFs2DDmzp2Lt7d3gV130qRJJCcn523x8fEFdu3izmI28UyPOswd3hx3Rzu2xV7g3hnhbD5+zuhoIiIiJZqKUgbz8XDi3saVgCufvImIyBVlbHFYKUIl7XvL29sbi8VCUlL+qf5JSUn4+fn95fhjx44RExNDnz59sLOzw87OjgULFrB8+XLs7Ow4duzYbeVwdHTEw8Mj31bW3F3fl+Xj21PH152zqZk89OlmPt1wvMR9T4mIiBQXKkoVA1cbni+POsXZ1Exjw4iIGMze/sqy6+np6QYnkdLq6vfW1e+14s7BwYFmzZqxevXqvH1Wq5XVq1fTpk2bvxxft25d9uzZQ1RUVN5233330aVLF6KiosrMtLvCUs3ble+faEvfJpXJtdp47ecDjF+0k7TMHKOjiYiIlDh2RgcQCAkoR7C/F7viL7Jocxzju9YyOpKIiGEsFgteXl55S927uLhgMpkMTiWlgc1mIz09ndOnT+Pl5YXFYjE60k2bOHEiI0aMoHnz5rRs2ZL333+ftLQ0QkNDARg+fDhVqlRh2rRpODk50bBhw3zne3l5AeTbf/78eeLi4jh16hQAhw4dAsDPz++aI7Dk/7k42PH+oCaE+Hvx2s8H+Gl3AocSLzFnWDNqVHQzOp6IiEiJoaJUMRHaNogJX0excFMsYzrXwN6iQWwiUnZd/YX4amFKpCB5eXmVuKLLoEGDOHPmDFOmTCExMZEmTZqwYsWKvObncXFxmM23du+wfPnyvKIWwODBgwF4+eWXeeWVVwose2llMpkY2a4aDat48viXOzhyOpW+szbyzoPB9GxYsr6/REREjGKylbFJ8CkpKXh6epKcnFyseiFk5Vhp9+YazlzKZMaQEO4Lrmx0JBERw+Xm5pKdnW10DClF7O3tbzhCqrjeJxRHeq/+3+lLGYz7aidbos8DMKZTDZ7pXhs7fcgoIiJl1M3eJ2ikVDHhYGfm4VYBvP/7EcI2RqsoJSLClal8JWmKlYiUTT7uTnw5qhVv/nqQT8OjmfPHMXafuMiMISF4uzkaHU9ERKTY0sc3xchDrQKwt5jYEXeRXfEXjY4jIiIiIjfJ3mJmcu/6zHooBBcHCxHHztFnZjg74y4YHU1ERKTYUlGqGPFxd6J34ysjpOZHxBgbRkRERERuWe/GlfnhiXZUr+hKQnIGgz7exBebYiljHTNERERuiopSxczItkEA/Lj7FKcvZRgbRkRERERuWS1fd354oh09G/iRlWtl8rK9PPPNbjKyc42OJiIiUqyoKFXMBPt7ERLgRXaujUWb442OIyIiIiK3wd3Jno+GNmVSr7qYTfDdjhMM+DCCuHPpRkcTEREpNlSUKoaujpb6YnMsWTlWY8OIiIiIyG0xmUz8o1MNvni0FRVcHdifkEKfWeGsPXja6GgiIiLFgopSxVCvhpXwcXfkzKVMft2bYHQcEREREbkDbWt68+P49jTx9yL5cjaPzN/Ke6sOY7Wqz5SIiJRtKkoVQw52Zoa2DgRg3sYYY8OIiIiIyB2r7OXM1/9ozdDWAdhs8MHqIzw6fysX07OMjiYiImIYFaWKqSEtA3CwmImKv6ilhEVERERKAUc7C6/1a8Q7DwbjaGdm7aEz9JkVzt6TyUZHExERMYSKUsVURXdHegdXAmB+RIyxYURERESkwDzQrCpLH2+Lf3ln4s9f5v6PIvh2+wmjY4mIiBQ5FaWKsdC21QD4eU8Cp1MyDE4jIiIiIgWlQWVPfhrXgS51KpKZY+WZb3bx4vd7yMzJNTqaiIhIkVFRqhhrVNWTZoHlyM618eXmOKPjiIiIiEgB8nSx57MRLXi6W21MJvhycxwDP97EqYuXjY4mIiJSJFSUKuZC2wUBV25S9MmZiIiISOliNpt4qlstPh/ZAk9ne3bFX6T3zHAijp41OpqIiEihU1GqmOvRwA8/DyfOpmbyy54Eo+OIiIiISCHoUseHn8a3p0FlD86nZTH0s83M+eMYNpvN6GgiIiKFRkWpYs7eYmZYm0AA5m2M0Y2JiIiISCnlX96F78a25YFmVbHa4D+/HmTMF9u5lJFtdDQREZFCoaJUCTC4hT8OdmZ2n0hmR9xFo+OIiIiISCFxsrfw9gONeb1/QxwsZlbuS6LvrI0cTrpkdDQREZECp6JUCVDBzZG+wZUBCIuIMTaMiIiIiBQqk8nEw60CWTKmDZU8nTh+No1+szfy465TRkcTEREpUCpKlRAj2gYB8OueBBKTM4wNIyIiIiKFrom/Fz+Nb0/bGhVIz8pl/KKd/Pun/WTnWo2OJiIiUiBUlCohGlbxpGVQeXKsNr7cHGt0HBEREREpAhXcHFnwSEvGdq4BwGfh0Tw8dzOnL+lDShERKflUlCpBRrYLAuCrzXFkZOcaG0ZEREREioSdxczzPesyZ2gz3Bzt2BJznt4zwtkWc97oaCIiIndERakSpHt9Xyp5OnEuLYufdicYHUdEREREilDPhn4sH9eO2r5unL6UyeBPNjFvY7RWZxYRkRJLRakSxM5iZlibQADdgIiIiIiUQdUruvH94+3o3bgSOVYbr/64nwlfR5GelWN0NBERkVumolQJM7hFAI52ZvadSmF77AWj44iIiIhIEXN1tGPmkBCm9K6PndnED1Gn6D87guizaUZHExERuSWGFqXWr19Pnz59qFy5MiaTiWXLlt3w+PDwcNq1a0eFChVwdnambt26vPfee0UTtpgo7+pAvyZVAJgXEWNsGBERERExhMlk4pH21fhqdGsqujtyKOkS980M57d9iUZHExERuWmGFqXS0tIIDg5m9uzZN3W8q6sr48aNY/369Rw4cIDJkyczefJkPvnkk0JOWryMaBsEwIq9iSQkXzY2jIiIiIgYpmW18vw8vj0tgspxKTOHxxZu5+2VB8m1qs2DiIgUfyZbMWlMZDKZ+P777+nXr98tnTdgwABcXV1ZuHDhTR2fkpKCp6cnycnJeHh43EbS4mHQx5Fsjj7PE11q8GyPukbHERERKRVKy31CUdB7Vbxk51p545cDzNsYA0D7mt7MGBJCeVcHY4OJiEiZdLP3CSW6p9TOnTuJiIigU6dORkcpcqHtggD4anMcGdm5xoYREREREUPZW8y83KcBHwxugrO9hfCjZ+kzM5xd8ReNjiYiInJdJbIoVbVqVRwdHWnevDlPPPEEo0aNuu6xmZmZpKSk5NtKg271fKni5cyF9GyW7zpldBwRERERKQb6NqnCsifaUc3blZMXL/PgnEgWbYnTqs0iIlIslcii1IYNG9i2bRtz5szh/fffZ9GiRdc9dtq0aXh6euZt/v7+RZi08NhZzAxrEwhA2MYY3WiIiIiICAB1/Nz5YVw77q7vS1aulUlL9/D8d7s1ul5ERIqdElmUqlatGo0aNWL06NE8/fTTvPLKK9c9dtKkSSQnJ+dt8fHxRRe0kA1u4Y+TvZn9CSlsjblgdBwRERERKSY8nOz5eGgznutZB7MJlmw7wQNzIog/n250NBERkTwlsij136xWK5mZmdd93NHREQ8Pj3xbaeHl4kD/kKoAhEVEG5xGRERERIoTs9nE451rsuCRVpR3dWDvyRT6zArnj8NnjI4mIiICGFyUSk1NJSoqiqioKACio6OJiooiLi4OuDLKafjw4XnHz549mx9//JEjR45w5MgRPvvsM9555x2GDh1qRPxiYWTbIABW7kvi5MXLxoYRERERkWKnfS1vfhzfnuCqnlxMz2bkvC3MWH0Eq1XtH0RExFiGFqW2bdtGSEgIISEhAEycOJGQkBCmTJkCQEJCQl6BCq6Mipo0aRJNmjShefPmzJ49mzfffJOpU6cakr84qOPnTtsaFci12lgYGWt0HBEREREphqp4ObNkTBseahWAzQbTVx1m9IJtJF/ONjqaiIiUYSZbGeuQnZKSgqenJ8nJyaVmKt9v+xJ5bOF2vFzsiXyhK84OFqMjiYiIlEil8T6hsOi9KrmWbItn8rK9ZOVYCazgwkcPN6N+Zf0diohIwbnZ+4QS31NKoGs9X6qWc+ZiejY/RJ00Oo6IiIiIFGMDm/uzdGxbqpZzJvZcOgM+2sj3O08YHUtERMogFaVKAYvZxIg2QQCERcRQxga/iYiIiMgtaljFkx/HtadT7YpkZFt5+utdTPnhyugpERGRoqKiVCkxsLk/zvYWDiZeYtPx80bHEREREZFirpyrA5+PbMGTXWsBsCAylkGfRJKYnGFwMhERKStUlColPF3sGdC0CgBhEdEGpxERERGRksBiNjHx7tp8NqI5Hk527Iy7SO+ZG4g8ds7oaCIiUgaoKFWKjGwbBMCq/UnEn083NoyIiIiIlBhd6/ny4/j21KvkwdnULIZ+tplP1h9TWwgRESlUKkqVIrV83Wlf0xurDb7YFGt0HBEREREpQQIruLJ0bFsGhFQh12rjjV8O8viXO0jNzDE6moiIlFIqSpUyV0dLLdoSR3qWbiBERERE5OY5O1h4d2Aw/+7XEHuLiV/3JtJ3VjhHT18yOpqIiJRCKkqVMl3q+hBQ3oWUjByW7TxldBwRERERKWFMJhPDWgfy9T/a4OfhxLEzafSdtZGfdycYHU1EREoZFaVKGYvZxPA2gcCVhufqAyAiIiIit6NpQDl+erI9bapXIC0rlye+2sHrP+8nJ9dqdDQRESklVJQqhR5s7o+Lg4XDSalaOUVEREREbpu3myMLH23JPzpVB2Duhmge/nQzZy5lGpxMRERKAxWlSiFPZ3vub1oVgHkRMcaGEREREZESzc5iZlKvenz0cFNcHSxsjj5P75kb2B57wehoIiJSwqkoVUqNaHtlCt/vB5KIP59ucBoRERERKel6NarED+PaU9PHjaSUTAZ/EsmCyBi1ixARkdumolQpVdPHnQ61vLHZYEFkjNFxRERERKQUqOnjxrIn2nFvo0pk59qY8sM+Ji7ZxeWsXKOjiYhICaSiVCkW2i4IgMVb40nLzDE2jIiIiIiUCm6Odsx6KITJ99bDYjbx/c6T9P9wIzFn04yOJiIiJYyKUqVY59o+BFVw4VJGDt/vPGl0HBEREREpJUwmE6M6VOfLUa3wdnPgYOIl+swK5/f9SUZHExGREkRFqVLMbDYxom0QAGERmu8vIiIiIgWrdfUK/DS+A00DvLiUkcOoBdt497dD5Fp13ykiIn9PRalS7oFmVXF1sHD0dCrhR88aHUdEREREShk/TycWP9aGkX9+GDpzzVFCw7ZyIS3L2GAiIlLsqShVyrk72fNgc38AwjbGGBtGREREREolBzszr9zXgPcHNcHJ3sz6w2foPTOcPSeSjY4mIiLFmIpSZcDwNoEArDl0Wg0oRURERKTQ9AupwvePtyOwggsnL17m/jkRfL01zuhYIiJSTKkoVQZUr+hG5zoVsdlgQWSs0XFEREREpBSrV8mD5ePa062eD1k5Vp7/bg+Tlu4mIzvX6GgiIlLMqChVRlyd4//NtnhSM3OMDSMiIiIipZqnsz2fDGvOM91rYzLBoi3xDPw4khMX0o2OJiIixYiKUmVEx1oVqe7tyqXMHJbuOGF0HBEREREp5cxmE+PuqsX80JZ4udiz+0QyfWaGs+HIGaOjiYhIMaGiVBlhNpsY8edoqbCIGKxapldEREREikDH2hX5aXx7GlXx5EJ6NsM/38JbKw5qOp+IiKgoVZbc36wqbo52HD+TxoajZ42OIyIiIrdg9uzZBAUF4eTkRKtWrdiyZctNnbd48WJMJhP9+vXLt99mszFlyhQqVaqEs7Mz3bp148iRI4WQXASqlnPhmzFtGNzCH5sNPlx3jN4zw9kZd8HoaCIiYiAVpcoQN0c7HmxeFYCwjdEGpxEREZGb9fXXXzNx4kRefvllduzYQXBwMD169OD06dM3PC8mJoZnnnmGDh06/OWxt956ixkzZjBnzhw2b96Mq6srPXr0ICMjo7BehpRxTvYW/nN/Y+YMbYa3myNHT6dy/0cRvP7zfo2aEhEpo1SUKmNGtAnCZIK1h84QfTbN6DgiIiJyE6ZPn87o0aMJDQ2lfv36zJkzBxcXFz7//PPrnpObm8vDDz/Mq6++SvXq1fM9ZrPZeP/995k8eTJ9+/alcePGLFiwgFOnTrFs2bJCfjVS1vVs6MeqpzsyIKQKVhvM3RBNrw82sDXmvNHRRESkiKkoVcYEebvSpY4PAPMjYowNIyIiIn8rKyuL7du3061bt7x9ZrOZbt26ERkZed3zpk6dio+PD48++uhfHouOjiYxMTHfNT09PWnVqtV1r5mZmUlKSkq+TeR2lXN1YPqgJnw2ojm+Ho5En01j4MeRvLJ8H+lZWilaRKSsUFGqDBr5Z8Pzb7ef4FJGtrFhRERE5IbOnj1Lbm4uvr6++fb7+vqSmJh4zXPCw8P57LPPmDt37jUfv3rerVxz2rRpeHp65m3+/v63+lJE/qJrPV9+e7oTA5tXxWa7siBPz/c3EHnsnNHRRESkCKgoVQZ1qOVNjYqupGbm8N32E0bHERERkQJ06dIlhg0bxty5c/H29i6w606aNInk5OS8LT4+vsCuLWWbp7M9bz0QzPxHWlLZ04m48+kMmbuJycv2kJqpUVMiIqWZilJlkMlkyhstNT8yFqvVZmwgERERuS5vb28sFgtJSUn59iclJeHn5/eX448dO0ZMTAx9+vTBzs4OOzs7FixYwPLly7Gzs+PYsWN5593sNQEcHR3x8PDIt4kUpE61K7Ly6Y481CoAgC82xdHjvfVsOHLG4GQiIlJYVJQqowY0rYq7ox3RZ9P4Qz/oRUREii0HBweaNWvG6tWr8/ZZrVZWr15NmzZt/nJ83bp12bNnD1FRUXnbfffdR5cuXYiKisLf359q1arh5+eX75opKSls3rz5mtcUKSruTva80b8RX41qRdVyzpy8eJlhn23hhe92k6K2EyIipY6KUmWUq6MdA1tc6QURtjHG2DAiIiJyQxMnTmTu3LnMnz+fAwcOMHbsWNLS0ggNDQVg+PDhTJo0CQAnJycaNmyYb/Py8sLd3Z2GDRvi4OCAyWRiwoQJvPbaayxfvpw9e/YwfPhwKleuTL9+/Qx8pSJXtK3pzcoJHfNG9y/eGk/36etZe/C0scFERKRA2RkdQIwzok0Qn2+M5o/DZzh2JpUaFd2MjiQiIiLXMGjQIM6cOcOUKVNITEykSZMmrFixIq9ReVxcHGbzrX3W+Nxzz5GWlsZjjz3GxYsXad++PStWrMDJyakwXoLILXN1tOOV+xpwT6NKPPftLmLOpRMatpUBTaswpXd9vFwcjI4oIiJ3yGSz2cpUQ6GUlBQ8PT1JTk5WLwRg1Pxt/H4gieFtApnat6HRcURERAyl+4Sbp/dKitLlrFze/e0Qn22MxmaDiu6OvN6vId0bXLsHmoiIGOtm7xM0fa+MC20XBMC3209onr6IiIiIFEvODhYm967Pt2PaUqOiK2cuZfLYwu2MX7ST82lZRscTEZHbpKJUGde2RgVq+biRnpXLN9tOGB1HREREROS6mgWW4+cnOzC2cw3MJvhx1ynunv4HP+9OMDqaiIjcBhWlyjiTycTIP0dLzY+IIddapmZzioiIiEgJ42Rv4fmedVn2RDvq+LpzLi2LJ77awdgvtnPmUqbR8URE5BYYWpRav349ffr0oXLlyphMJpYtW3bD45cuXcrdd99NxYoV8fDwoE2bNqxcubJowpZi/UOq4OFkR9z5dNYd0oomIiIiIlL8Na7qxfLx7XjyrprYmU38ujeR7u/9wQ9RJyljbXNFREosQ4tSaWlpBAcHM3v27Js6fv369dx999388ssvbN++nS5dutCnTx927txZyElLNxcHOwa3DAAgLCLG2DAiIiIiIjfJ0c7CxO51+GFcO+pX8uBCejZPLY5i9ILtnE7JMDqeiIj8jWKz+p7JZOL777+nX79+t3RegwYNGDRoEFOmTLmp47VSzLXFn0+n09trsdpg1dMdqeXrbnQkERGRIqf7hJun90qKm+xcKx+tO8bMNUfIzrXh4WTHlD4NuL9pFUwmk9HxRETKlDKx+p7VauXSpUuUL1/e6Cglnn95F7rV8wVgfmSMsWFERERERG6RvcXMk11r8eP49jSu6klKRg7PfLOL0LCtJCRfNjqeiIhcQ4kuSr3zzjukpqYycODA6x6TmZlJSkpKvk2u7WrD8++2nyT5craxYUREREREbkNdPw+Wjm3L8z3r4mBnZt2hM3Sfvp5FW+LUa0pEpJgpsUWpr776ildffZUlS5bg4+Nz3eOmTZuGp6dn3ubv71+EKUuWNtUrUMfXncvZuXyzLd7oOCIiIiIit8XOYmZs5xr88mR7QgK8uJSZw6Slexj22Rbiz6cbHU+kREjLzOHjP47R8/31LNwUa3QcKaVKZFFq8eLFjBo1iiVLltCtW7cbHjtp0iSSk5Pztvh4FVuux2Qy5Y2Wmh8ZQ65VnySJiIiISMlV08edb8e0ZfK99XC0MxN+9Cw93l/PgsgYrLrXFbmmSxnZzF57lPZvrmHarwc5mHiJN389SGpmjtHRpBQqcUWpRYsWERoayqJFi7j33nv/9nhHR0c8PDzybXJ9/ZpUwdPZnvjzl1lz8LTRcURERERE7ojFbGJUh+qsmNCRlkHlSc/KZcoP+xgydxOx59KMjidSbCSnZ/P+74dp9581vL3yEBfSs6nm7UplTydSM3P4bvsJoyNKKWRoUSo1NZWoqCiioqIAiI6OJioqiri4OODKKKfhw4fnHf/VV18xfPhw3n33XVq1akViYiKJiYkkJycbEb9UcnawMLjllSmOYRHRBqcRERERESkY1bxdWfxYa169rwEuDhY2R5+nx/vr+Sw8WjMEpEy7kJbFOysP0f7NNbz/+xFSMnKo6ePGB4ObsOrpjoztXAOAsAiNMJSCZ2hRatu2bYSEhBASEgLAxIkTCQkJYcqUKQAkJCTkFagAPvnkE3JycnjiiSeoVKlS3vbUU08Zkr+0GtY6ELMJNh49x+GkS0bHEREREREpEGaziRFtg1g5oSNta1QgI9vKv3/az8CPIzl2JtXoeCJF6mxqJtN+PUC7N9cwa+1RLmXmUNfPnVkPhbByQkf6NqmCncXMgKZVcXe0I/psGn8cOWN0bCllTLYytgRFSkoKnp6eJCcnayrfDYxZuJ0V+xJ5qFUAb/RvZHQcERGRIqH7hJun90pKOpvNxldb4pj2y5VeOY52ZibeXZtRHapjMZuMjidSaE6nZPDJ+uN8sTmWjGwrAA0qezD+rlp0r++L+Rrf///+aT+fhUfTqXZF5j/SsqgjSwl0s/cJJa6nlBSNqw3Pl+44QXJ6trFhREREREQKmMlk4uFWgax8uiMdanmTmWNl2q8HGfBRBEc0W0BKoYTky7yyfB/t31rLp+HRZGRbCa7qyWcjmvPT+Pb0bOh3zYIUwIg2QZhM8MfhMxw9rVGFUnBUlJJralWtPHX93MnItvL1tri/P0FEREREpASq4uXMgkda8tb9jXF3smNX/EXunRHO7LVHyc61Gh1P5I6duJDOi9/vodNb6wiLiCErx0qzwHLMf6Qly55oR9d6vphMNx4dGFDBha51fQFYEBlTBKmlrFBRSq7JZDLxSLtqAMyPiFXzRxEREREptUwmEwNb+LPq6U7cVdeHrFwrb688RP8PN7L/VIrR8URuS+y5NJ7/djed317Hl5vjyMq10qpaeb4a1Ypvx7ShU+2Kf1uM+m+hf86m+Xb7CVIyNJtGCoaKUnJd9zWpTDkXe05evMyq/UlGxxERERERKVR+nk58NqI57w0KxtPZnr0nU7hvVjjvrTpMVo5GTUnJcPxMKhOXRHHXu3/w9bZ4cqw22tf05uvHWvP1P9rQtqb3LRWjrmpbowK1fd1Iz8plydb4QkguZZGKUnJdTvYWhrQMACAsItrgNCIiIiIihc9kMtE/pCqrJnakRwNfcqw2Plh9hPtmhbPnRLLR8USu60jSJZ5ctJNu0/9g6Y6T5FptdKpdke/GtuGLUa1oVb3CHV3fZDIxsu2V2TQLIjWbRgqGilJyQ0NbB2Ixm9h0/DwHEjR0WURERETKBh93J+YMbcbMISGUd3XgYOIl+n24kbdWHCQzJ9foeCJ5DiSk8MSXO+j+/nqW7zqF1Qbd6vnwwxPtmP9IS5oFli+w5+ofUgVPZ3vizqez9uDpAruulF0qSskNVfZypmcDPwDmR8QYG0ZEREREpAiZTCb6BFdm1dMd6d24ErlWGx+uO8a9M8LZGXfB6HhSxu09mcxjC7bR64MN/LwnAZsNejbw46fx7fl0RAuC/b0K/DmdHSwMbukPwDzNppECoKKU/K2Rfza0+37nSS6kZRkbRkRERESkiFVwc2TWQ02ZM7QZ3m6OHD2dyv0fRfDGLwfIyNaoKSlaO+Mu8EjYVnrPDOe3/UmYTNC7cSVWTOjAnGHNaFjFs1Cff1jrQMwm2Hj0HIeTLhXqc0npp6KU/K3mgeVoUNmDzBwri9XQTkRERETKqJ4N/Vj1dEf6h1TBaoNP1h/nng82sC3mvNHRpAzYFnOeYZ9tpv+HEaw5eBqzCfo1uTKSb9ZDTanr51EkOaqWc6F7/SuzacI0m0bukIpS8reuNLQLAmBhZAw5uVp5RERERETKpnKuDrw3qAmfjWiOr4cjx8+m8eDHkbz64z7Ss3KMjieljM1mI/LYOYZ8sokH5kSy4chZLGYTDzSryup/dub9wSHU9HEv8lyhf86mWbrjBMnp2UX+/FJ6qCglN6VPcGXKuzpwKjmDVfuTjI4jIiIiImKorvV8+e3pTjzYrCo2G8zbGEPP9zcQeeyc0dGkFLDZbGw4coZBH29iyNxNRB4/h73FxJCW/qz9Z2feeTCYat6uhuVrWa089Sp5kJFtZfHWOMNySMmnopTcFCd7Cw+1DABgnoZoioiIiIjg6WzP2w8GExbagsqeTsSdT2fI3E28tGwvqZkaNSW3zmazsfbQaQZ8FMGwz7awJeY8DhYzw1oHsu7ZLkwb0JiACi5Gx8RkMhH652yaBZGxmk0jt01FKblpQ1sHYjGb2BJ9nn2nko2OIyIiIiJSLHSu48PKpzvyUKsrH+Iu3BRLj/fWs+HIGYOTSUlhs9lYtT+JvrM3EjpvKzvjLuJoZya0XRDrn+vCv/s1pIqXs9Ex87mvSWXKudhz8uJlfj9w2ug4UkKpKCU3zc/TiV4NrzS0m6/RUiIiIiIiedyd7HmjfyO+HNWKquWcOXnxMsM+28IL3+0mJUM9d+TarFYbv+5J4J4Z4YxesI3dJ5JxtrfwWMfqbHi+Cy/3aYCfp5PRMa/Jyd6SV4idtzHa4DRSUqkoJbfkakO7ZVGnOJ+WZWwYEREREZFipl1Nb1ZO6Ji3UNDirfF0n76etQc1kkT+X67VxvJdp+j5wXrGfrmDAwkpuDpYGNu5BuHPd+Ff99TDx714FqP+29XZNJujz7P/VIrRcaQEUlFKbknTgHI0quJJVo6VRVvU0E5ERERE5H+5Otrxyn0NWPKPNgRVcCExJYPQsK1MXBKllcrKuJxcK9/vPMHd7/3Bk4t2cjgpFXdHO568qyYbX7iL53vWpYKbo9Exb1olT2d6ajaN3AEVpeSWmEymvE99vtgUS7Ya2omIiIiIXFPLauX59amOjGpfDZMJlu44Sbf3/uC3fYlGR5Milp1rZcm2eLpO/4Onv97F8TNpeDrbM/Hu2oS/cBcTu9fBy8XB6Ji35ZG82TQnNZtGbpmKUnLLegdXwtvNgYTkDH7bl2R0HBERERGRYsvZwcLk3vX5dkxbalR05cylTB5buJ0nF+3UL/BlQGZOLl9tjqPLO+t47tvdxJ5Lp7yrA8/1rEP48114smstPJ3tjY55R67OpsnUbBq5DSpKyS1ztLPwUMsrDe3CItTQTkRERETk7zQLLMfPT3ZgTKcamE2wfNcpur/3B7/sSTA6mhSCjOxcFkTG0Pntdfzr+z2cuHAZbzdHXrynHuHPd+HxzjVxdyrZxairNJtG7oSKUnJbhrYOxM5sYmvMBfaeTDY6joiIiIhIsedkb+GFXnX5/vF21PZ142xqFo9/uYPHv9zO2dRMo+NJAbiclctn4dF0fGstU37YR0JyBr4ejkzpXZ8Nz3VhdMfquDjYGR2zwP33bJqVmp4qt0BFKbktPh5O3Nu4EgDzNsYYG0ZEREREpAQJ9vfix/HtGX9XTSxmE7/sSeTu6X/wQ9RJbDab0fHkNqRl5vDJ+mN0eGsN//5pP6cvZVLZ04l/923AH8924ZH21XB2sBgds9A42ll4qFUgAGH6/VBugYpSctuuDtH8cdcpfbIjIiIiInILHO0s/LN7HX54oh31KnlwIT2bpxZH8djC7ZxOyTA6ntykSxnZzF57lPZvruGNXw5yNjWLquWcmTagEeue7cKwNkE42ZfeYtR/G9oqAHuLiW2xF9hzQrNp5OaoKCW3LSSgHMH+XmTlWlm0WQ3tRERERERuVcMqniwf146Jd9fG3mJi1f4kuk3/g2+3n9CoqWIs+XI2H/x+hPZvruXtlYe4kJ5NUAUX3n6gMWuf6cyQlgE42JWtX7d9PJy4t9Gfs2nUe1huUtn6VyIFLvTP0VIL1dBOREREROS22FvMPNm1Fj+Ob0/jqp6kZOTwzDe7CA3bSkLyZaPjyX+5kJbFu78dov1/1vDe74dJvpxNjYquvD+oCb9P7MSDzf2xt5TdX7NHtqsGwE+7EjhzSbNp5O+V3X8tUiDuaVSJiu6OnL6Uya971dBOREREROR21fXzYOnYtjzfsy4OdmbWHTpD9+nrWbQlTqOmDHYuNZP//HqQ9m+uYeaao1zKzKG2rxszh4Tw29Od6BdSBbsyXIy6qom/F02uzqbZotk08vf0r0buiIOdmYdbBQAQtlFDNEVERERE7oSdxczYzjX45cn2hAR4cSkzh0lL9zDssy3En083Ol6Zc/pSBq//vJ/2b65lzh/HSMvKpV4lD+YMbcqKpzrSJ7gyFrPJ6JjFSmi7IAC+2BRLVo5m08iNqSgld+yhPxva7Yi7yK74i0bHEREREREp8Wr6uPPtmLZMvrcejnZmwo+epef761kYGYPVqlFThS0xOYNXlu+jw5trmbshmsvZuTSu6smnw5vzy5Pt6dmwEmYVo66pV8NK+OTNpkkwOo4UcypKyR3zcXeid+PKAMyPiDE2jIiIiIhIKWExmxjVoTorJnSkZVB50rJyeemHfTz06SZiz6UZHa9UOnnxMpOX7aHjW2sJi4ghM8dKSIAXYaEt+OGJdnSr74vJpGLUjTjYmRnaOhCAeRtjjA0jxZ6KUlIgRv7Z8PzH3ac4fUlL2IqIiIiIFJRq3q4sfqw1r/Spj7O9hU3Hz9Pz/Q18Hh6tUVMFJO5cOi98t5vOb6/li01xZOVaaRlUni8ebcXSsW3pXMdHxahbMKRlAA4WM1HxF9kZd8HoOFKMqSglBSLY34uQAC+yc20s2hxvdBwRERERkVLFbDYxsl01Vk7oSJvqFbicncvUn/Yz8ONIjp9JNTpeiXX8TCr/XLKLLu+uY/HWeLJzbbStUYHFj7VmyZg2tK/lrWLUbajo7kif4CuzacI0m0ZuQEUpKTBXR0t9sVkN7URERERECkNABRe+HNWK1/o1xNXBwrbYC/T6YAOfrD9GrkZN3bQjSZd4avFOuk3/g+92nCDXaqNj7Yp8O6YNX41uTevqFYyOWOJd/f3w590JJKVoNo1cm4pSUmCuNrQ7o4Z2IiIiIiKFxmw2MbR1IL9N7ESHWt5k5lh545eDDPgogiNJl4yOV6wdTEzhia920P399fwQdQqrDbrW9WHZE+1Y8EhLmgeVNzpiqdGoqifNA8uRY7Xx5eY4o+NIMaWilBQYNbQTERERESk6VbycWfBIS966vzHuTnbsir/IvTPCmb32KNm5mrnw3/aeTOYfC7fR8/0N/Lw7AZsNejTw5afx7flsZAua+HsZHbFUCm1XDYCvNseSmZNrcBopjlSUkgKlhnYiIiIiIkXHZDIxsIU/q57uxF11fcjKtfL2ykP0/3AjBxJSjI5nuKj4izwatpXeM8NZuS8JkwnubVyJX5/qwMfDmtOwiqfREUu17g18qeTpxNnULH7apdk08lcqSkmBqujuSO/gSgDMV0M7EREREZEi4efpxGcjmvPeoGA8ne3ZezKFPjPDeW/V4TLZ73V77HmGf76FfrM3svrgacwm6NukMr9N6Mjsh5pSr5KH0RHLBHvL/8+mCYuIwWZT3zPJT0UpKXChba8M0fx5TwKn1dBORERERKRImEwm+odUZdXEjvRo4EuO1cYHq49w36xw9p5MNjpekdh0/BwPf7qJ+z+KZP3hM1jMJu5vWpXfJ3big8Eh1PJ1NzpimTOkZQCOdmb2nExme6xm00h+KkpJgWtU1ZNmgeXIzlVDOxERERGRoubj7sScoc2YOSSE8q4OHEy8RN/ZG3l75cFS2dfHZrOx8ehZBn4cyeBPNrHx6DnszCYGt/Bn7T878+7AYKpXdDM6ZplV3tWBfk2qADBPs2nkfxhalFq/fj19+vShcuXKmEwmli1bdsPjExISeOihh6hduzZms5kJEyYUSU65daHtggD4Ug3tRERERESKnMlkok9wZVY93ZF7G1ci12pj9tpj9J4RTlT8RaPjFQibzca6Q6e5/6MIHv50M1uiz+NgMTO0dQDrnu3Mf+5vTEAFF6NjCjCibRAAK/YmkpB82dgwUqwYWpRKS0sjODiY2bNn39TxmZmZVKxYkcmTJxMcHFzI6eRO9Gjgh5/HlYZ2P+9WQzsRERERESNUcHNk9kNNmTO0Kd5uDhw5ncqADzcy7ZcDZGSXzA+PbTYbv+9Pou/sjYyct5UdcRdxtDMzsm0QfzzXmdf6NaJqORWjipP6lT1oVa08uVYbCyNjjY4jxYidkU/eq1cvevXqddPHBwUF8cEHHwDw+eefF1YsKQD2FjPD2gTy9spDzNsYQ/+QKphMJqNjiYiIiIiUST0bVqJVtQq8+uM+lkWd4uP1x1m1P4m3H2xMs8DyRse7KVarjd/2JzJj9VH2/7myoLO9haGtAxjdsTo+7k4GJ5QbCW1Xjc3R51m0JY4nu9bCyd5idCQpBtRTSgrN4Bb+OPzZ0G5H3EWj44iIiIiIlGnlXB14f3AInw5vjq+HI8fPpvHAnEim/rif9Kwco+NdV67Vxo+7TtHrgw2M+WIH+xNScHGwMKZTDTY834UX762vglQJ0K2eD1W8nLmQns3yqFNGx5FiotQXpTIzM0lJScm3SdGo4OZI3+DKwJXlP0VERERExHjd6vvy29OdeLBZVWw2+HxjNL0+2EDksXNGR8snJ9fKsp0n6f7eH4xftJNDSZdwd7Rj/F012fj8XbzQqy7ebo5Gx5SbZGcxM7xNIHCl4bnNZjM4kRQHpb4oNW3aNDw9PfM2f39/oyOVKVcb2v26J4HE5Axjw4iIiIiICACezva8/WAwYaEtqOzpROy5dIbM3cRLy/aSmmnsqKnsXCvfbIun2/Q/mPB1FMfOpOHhZMfT3WoT/sJd/LN7Hcq5OhiaUW7P4BYBONtbOJCQwubo80bHkWKg1BelJk2aRHJyct4WHx9vdKQypWEVT1oGlSfHauPLzWpoJyIicrtmz55NUFAQTk5OtGrVii1btlz32KVLl9K8eXO8vLxwdXWlSZMmLFy4MN8xSUlJjBw5ksqVK+Pi4kLPnj05cuRIYb8MESlmOtfxYeXTHXmoVQAACzfF0uO99YQfOVvkWbJyrCzaEkeXd9bx7Le7iTmXTjkXe57tUYeNL9zFU91q4elsX+S5pOB4utjTv2kVAMI2xhgbRoqFUl+UcnR0xMPDI98mRWtkuyAAvtocV2JX+BARETHS119/zcSJE3n55ZfZsWMHwcHB9OjRg9OnT1/z+PLly/Piiy8SGRnJ7t27CQ0NJTQ0lJUrVwJXVq7q168fx48f54cffmDnzp0EBgbSrVs30tLSivKliUgx4O5kzxv9G/HlqFZULefMyYuXGfrZZl74bjcpGdmF/vwZ2bksjIyh89trmbR0DycuXMbbzYFJveoS/vxdPNGlJu5OKkaVFiP/nE3z2/5ETlxINzaMGM7QolRqaipRUVFERUUBEB0dTVRUFHFxccCVUU7Dhw/Pd87V41NTUzlz5gxRUVHs37+/qKPLLehe35dKnk6cS8vip90JRscREREpEmvXri2wa02fPp3Ro0cTGhpK/fr1mTNnDi4uLtddjbhz587079+fevXqUaNGDZ566ikaN25MeHg4AEeOHGHTpk189NFHtGjRgjp16vDRRx9x+fJlFi1aVGC5RaRkaVfTm5UTOjLiz74/i7fG0+O99aw9dO0C+J3KyM7l8/BoOr29lpd+2Mep5Ax83B15qXd9Njx3F//oVANXR0MXjJdCUNvXnfY1vbHaYGGkZtOUdYYWpbZt20ZISAghISEATJw4kZCQEKZMmQJAQkJCXoHqqqvHb9++na+++oqQkBDuueeeIs8uN8/OYmbY1YZ2G6PV0E5ERMqEnj17UqNGDV577bU7ah+QlZXF9u3b6datW94+s9lMt27diIyM/NvzbTYbq1ev5tChQ3Ts2BG4shAMgJPT/69WZTabcXR0zCtc/S8tHiNSNrg62vFq34Z8/VhrAiu4kJCcQei8rfxzyS6S0wtm1FR6Vg5z1x+n/ZtrmfrTfpJSMqnk6cTUvg1Y/1wXHm1fDWcHS4E8lxRPV0dLLdoSV6xXfpTCZ2hRqnPnzthstr9sYWFhAISFhbFu3bp851zr+JiYmCLPLrdmcIsAHO3M7DuVwvbYC0bHERERKXQnT55k3LhxfPvtt1SvXp0ePXqwZMkSsrKybuk6Z8+eJTc3F19f33z7fX19SUxMvO55ycnJuLm54eDgwL333svMmTO5++67Aahbty4BAQFMmjSJCxcukJWVxZtvvsmJEydISLj2qGYtHiNStrSqXoEVT3Xk0fbVMJngux0nuPu9P1i1P+m2r5mamcOH647S/s21vP7LAc6mZlLFy5k3+jdi3bOdGd4mCCd7FaPKgi51fQgo70JKRg7Ldp4yOo4YqNT3lJLiobyrA/2aXGloNy8ixtgwIiIiRcDb25unn36aqKgoNm/eTO3atXn88cepXLkyTz75JLt27SrU53d3dycqKoqtW7fy+uuvM3HixLwP++zt7Vm6dCmHDx+mfPnyuLi4sHbtWnr16oXZfO3bQy0eI1L2ODtYeKl3fb4d04bqFV05fSmT0Qu28dTinVxIu/kCe/LlbGasPkL7N9fw1opDnE/LIrCCC2890Jh1z3bmoVYBONqpGFWWWMymvJXawyI0m6YsU1FKiszV/3RW7E0kIfmysWFERESKUNOmTZk0aRLjxo0jNTWVzz//nGbNmtGhQwf27dt3w3O9vb2xWCwkJeUfnZCUlISfn991zzObzdSsWZMmTZrwz3/+kwceeIBp06blPd6sWTOioqK4ePEiCQkJrFixgnPnzlG9evVrXk+Lx4iUXc0Cy/PLkx34R6fqmE3wQ9Qp7n7vD37dc+N+sRfTs5j+2yHav7mG6asOczE9m+oVXZk+MJjVEzsxsLk/9hb9SlpWPdi8Ki4OFg4npRJx7JzRccQg+h9Aikz9yh60qlaeXKuNLzapoZ2IiJR+2dnZfPvtt9xzzz0EBgaycuVKZs2aRVJSEkePHiUwMJAHH3zwhtdwcHCgWbNmrF69Om+f1Wpl9erVtGnT5qazWK3WvF5S/83T05OKFSty5MgRtm3bRt++fW/+BYpImeFkb2FSr3p8/3g7avu6cTY1i7Ff7uDxL7dzNjX//y3nUjN5c8VB2v1nDTPWHOVSRg61fNyYMSSEVU93YkDTqtipGFXmeTjZ80CzqgDM2xhjbBgxjJYykCIV2i6IzdHn+WpzHOPvqqU54yIiUmqNHz+eRYsWYbPZGDZsGG+99RYNGzbMe9zV1ZV33nmHypUr/+21Jk6cyIgRI2jevDktW7bk/fffJy0tjdDQUACGDx9OlSpV8kZCTZs2jebNm1OjRg0yMzP55ZdfWLhwIR999FHeNb/55hsqVqxIQEAAe/bs4amnnqJfv3507969gN8JESlNgv29+HF8e2atOcqH647xy55EIo+d45X7GtC2hjdzNxxnYWQsl7NzAahXyYMn76pJjwZ+mM0mg9NLcTOibRALImNZfTCJ2HNpBFZwNTqSFDEVpaRIdavnSxUvZ05evMzyXacY2FxNUkVEpHTav38/M2fOZMCAATg6Ol7zGG9vb9auXfu31xo0aBBnzpxhypQpJCYm0qRJE1asWJHX/DwuLi5fL6i0tDQef/xxTpw4gbOzM3Xr1uWLL75g0KBBecckJCQwceJEkpKSqFSpEsOHD+ell166w1ctImWBo52Ff3avQ48Gfjz77W4OJKTw1OIo7MwmcqxXegM1quLJk11r0a2eDyaTilFybTUqutGpdkX+OHyGBZGxvNS7vtGRpIiZbGWso1hKSgqenp4kJyerF4JB5vxxjP/8epD6lTz4+cn2+iElIiLFRkHeJ6xfv562bdtiZ5f/M8CcnBwiIiLo2LHjHV3faLqnEhGA7FwrH607xsw1R8jOtdHE34unutaic52Kus+Xm7L20GlC523F3dGOTf/qiqujxs6UBjd7n6CJvFLkBrfwx8nezP6EFLbGXDA6joiISKHo0qUL58+f/8v+5ORkunTpYkAiEZGCZ28x82TXWqx6uhPfjW3D94+3pUtdjY6Sm9epVkWqe7tyKTOHpTtOGB1HipiKUlLkvFwc6B9ytaFdtMFpRERECofNZrvmL2Xnzp3D1VU9M0SkdAnydqVZYHkVo+SWmc2mvJXa50XEYLWWqclcZZ7GxYkhRrYNYtGWOFbuS+TkxctU8XI2OpKIiEiBGDBgAAAmk4mRI0fm6yeVm5vL7t27adu2rVHxREREip37m1Xl7ZWHOH4mjQ1Hz9KpdkWjI0kR0UgpMUQdP3fa1qiA1QYLI2ONjiMiIlJgPD098fT0xGaz4e7unve1p6cnfn5+PPbYY3zxxRdGxxQRESk23BzteLD5ldk0YZpNU6ZopJQYZmTbICKOnWPx1jie6loLZweL0ZFERETu2Lx58wAICgrimWee0VQ9ERGRmzCiTRBhETGsPXSG42dSqV7RzehIUgQ0UkoM07WeL1XLOXMxPZsfok4aHUdERKRAvfzyyypIiYiI3KQgb1fuquMDwALNpikzVJQSw1jMJka0CQIgLCIGm00N7UREpGRr2rQpFy5cWVk2JCSEpk2bXncTERGR/Ea2CwLgm23xXMrINjaMFAlN3xNDDWzuz/RVhzmYeIlNx8/TpkYFoyOJiIjctr59++Y1Nu/Xr5+xYUREREqY9jW9qenjxtHTqXyz7QSPtK9mdCQpZCpKiaE8XewZ0LQKX26OIywiWkUpEREp0V5++WXgyip7Xbp0oXHjxnh5eRkbSkREpIQwmUyMbBvE5GV7mR8Zw8i2QZjNJqNjSSHS9D0x3Mi2QQCs2p9E/Pl0Y8OIiIgUAIvFQvfu3fOm8omIiMjNGdC0Ch5OdsSeS2fd4dNGx5FCdltFqfj4eE6cOJH39ZYtW5gwYQKffPJJgQWTsqOWrzvta3pjtcEXm9TQTkRESoeGDRty/Phxo2OIiIiUKC4OdgxuGQDAvI0xxoaRQndbRamHHnqItWvXApCYmMjdd9/Nli1bePHFF5k6dWqBBpSy4epoqUVb4kjPyjE2jIiISAF47bXXeOaZZ/jpp59ISEggJSUl3yYiIiLXNqx1IGYTbDhyliNJl4yOI4XotopSe/fupWXLlgAsWbKEhg0bEhERwZdffklYWFhB5pMyoktdHwLKu5CSkcOynaeMjiMiInLH7rnnHnbt2sV9991H1apVKVeuHOXKlcPLy4ty5coZHU9ERKTY8i/vQrd6vgDMj4wxNowUqttqdJ6dnZ23sszvv//OfffdB0DdunVJSEgouHRSZljMJoa3CeS1nw8QFhHNkJb+mExqaCciIiXX1VHlIiIicutGtgvit/1JfLf9JM92r4uni73RkaQQ3FZRqkGDBsyZM4d7772XVatW8e9//xuAU6dOUaGCVk+T2/Ngc3+mrzrM4aRUIo+do21Nb6MjiYiI3LZOnToZHUFERKTEalO9AnX93DmYeIkl2+IZ3bG60ZGkENzW9L0333yTjz/+mM6dOzNkyBCCg4MBWL58ed60PpFb5elsz/1NqwIwLyLG2DAiIiIFJD09nYMHD7J79+58m4iIiFyfyWTK6z08PzKGXKvN2EBSKG5rpFTnzp05e/YsKSkp+XoiPPbYY7i4uBRYOCl7RrQNZOGmWH4/kET8+XT8y+v7SURESqYzZ84QGhrKr7/+es3Hc3NziziRiIhIydK3SRX+s+IgJy5cZvWBJLo38DM6khSw2xopdfnyZTIzM/MKUrGxsbz//vscOnQIHx+fAg0oZUtNH3c61PLGZoMFamgnIiIl2IQJE7h48SKbN2/G2dmZFStWMH/+fGrVqsXy5cuNjiciIlLsOTtYGNIyAIB5G2OMDSOF4raKUn379mXBggUAXLx4kVatWvHuu+/Sr18/PvroowINKGVPaLsgABZvjSctM8fYMCIiIrdpzZo1TJ8+nebNm2M2mwkMDGTo0KG89dZbTJs2zeh4IiIiJcLQ1oFYzCYij5/jYGKK0XGkgN1WUWrHjh106NABgG+//RZfX19iY2NZsGABM2bMKNCAUvZ0ru1DUAUXLmXksHTnSaPjiIiI3Ja0tLS8EeTlypXjzJkzADRq1IgdO3YYGU1ERKTEqOLlTI8GvgDMV+/hUue2ilLp6em4u7sD8NtvvzFgwADMZjOtW7cmNja2QANK2WM2mxjxZ0O7sI3R2GxqaCciIiVPnTp1OHToEADBwcF8/PHHnDx5kjlz5lCpUiWD04mIiJQcoe2qAbB0x0kupGUZnEYK0m0VpWrWrMmyZcuIj49n5cqVdO/eHYDTp0/j4eFRoAGlbHqgWVVcHSwcO5NG+NGzRscRERG5ZU899RQJCQkAvPzyy/z6668EBAQwY8YM3njjDYPTiYiIlBzNA8vRoLIHmTlWFm+NNzqOFKDbKkpNmTKFZ555hqCgIFq2bEmbNm2AK6OmQkJCCjSglE3uTvY82NwfgDA1tBMRkRJo6NChjBw5EoBmzZoRGxvL1q1biY+PZ9CgQcaGExERKUFMJhMj/5xNszAyhpxcq7GBpMDcVlHqgQceIC4ujm3btrFy5cq8/V27duW9994rsHBStg1vEwjAmkOniTmbZnAaERGRO+Pi4kLTpk3x9vY2OoqIiEiJ0ye4MhVcHTiVnMFv+5OMjiMFxO52T/Tz88PPz48TJ04AULVqVVq2bFlgwUSqV3Sjc52KrDt0hgWRsUzpU9/oSCIiIjc0ceLEmz52+vTphZhERESkdHGyt/BQqwBmrjlK2MYY7mmk/oylwW0VpaxWK6+99hrvvvsuqampALi7u/PPf/6TF198EbP5tgZgifzFyLZBrDt0hm+2xTOxe23cHG+7jioiIlLodu7ceVPHmUymQk4iIiJS+gxtHchH646xJeY8e08m07CKp9GR5A7d1m/4L774Ip999hn/+c9/aNeuHQDh4eG88sorZGRk8PrrrxdoSCm7OtaqSHVvV46fTWPpjhMMbxNkdCQREZHrWrt2rdERRERESi1fDyfuaVSJ5btOERYRwzsPBhsdSe7QbQ1pmj9/Pp9++iljx46lcePGNG7cmMcff5y5c+cSFhZWwBGlLDObTYz4s6FdWEQMVqvN2EAiIiIiIiJimJHtggBYHnWKs6mZxoaRO3ZbI6XOnz9P3bp1/7K/bt26nD9//o5Dify3+5tV5e2Vhzh+Jo0NR8/SqXZFoyOJiIhc04ABAwgLC8PDw4MBAwbc8NilS5cWUSoREZHSI8Tfi+Cqnuw6kcziLXGMu6uW0ZHkDtzWSKng4GBmzZr1l/2zZs2icePGdxxK5L+5OdrxYPOqAIRtjDY4jYiIyPV5enrm9Yvy9PS84SYiIiK3zmQy5Y2WWrgpluxcq7GB5I6YbDbbLc+H+uOPP7j33nsJCAigTZs2AERGRhIfH88vv/xChw4dCjxoQUlJScHT05Pk5GQ8PDyMjiM3KeZsGl3eXYfNBmuf6Uw1b1ejI4mISCmk+4Sbp/dKRESMkpVjpd2bazhzKZMZQ0K4L7iy0ZHkf9zsfcJtjZTq1KkThw8fpn///ly8eJGLFy8yYMAA9u3bx8KFC2/6OuvXr6dPnz5UrlwZk8nEsmXL/vacdevW0bRpUxwdHalZs6Z6WJURQd6udKnjA8D8iBhjw4iIiIiIiIhhHOzMPNwqANBsmpLutopSAJUrV+b111/nu+++47vvvuO1117jwoULfPbZZzd9jbS0NIKDg5k9e/ZNHR8dHc29995Lly5diIqKYsKECYwaNYqVK1fe7suQEmTknw3Pv91+gksZ2caGERER+Rvnzp3jiSeeoH79+nh7e1O+fPl8m4iIiNy+h1oFYG8xsSPuIrviLxodR27TbTU6Lyi9evWiV69eN338nDlzqFatGu+++y4A9erVIzw8nPfee48ePXoUVkwpJjrU8qZGRVeOnUnju+0nGNmumtGRRERErmvYsGEcPXqURx99FF9f37xeUyIiInLnfNyd6NO4Mkt3niQsIob3BjUxOpLcBkOLUrcqMjKSbt265dvXo0cPJkyYYEwgKVImk4mRbYN46Yd9zI+MZXibIMxm3eCLiEjxtGHDBsLDwwkODjY6ioiISKk0om0QS3ee5Kfdp5h0T1183J2MjiS36Lan7xkhMTERX1/ffPt8fX1JSUnh8uXL1zwnMzOTlJSUfJuUXAOaVsXd0Y7os2n8ceSM0XFERESuq27dute9PxEREZE7F+zvRdMAL7JzbXy1Oc7oOHIbbmmk1IABA274+MWLF+8kS6GYNm0ar776qtExpIC4OtoxsIU/n4VHM29jTF7zcxERkeLmww8/5IUXXmDKlCk0bNgQe3v7fI9rxToREZE7F9quGjvidvLFpjjGdq6Bo53F6EhyC25ppJSnp+cNt8DAQIYPH15YWfHz8yMpKSnfvqSkJDw8PHB2dr7mOZMmTSI5OTlvi4+PL7R8UjRGtAnCZIL1h89w9HSq0XFERESuycvLi5SUFO666y58fHwoV64c5cqVw8vLi3LlyhkdT0REpFTo2dAPXw9HzqZm8sueBKPjyC26pZFS8+bNK6wcN6VNmzb88ssv+fatWrWKNm3aXPccR0dHHB0dCzuaFKGACi50revL7weSWBAZw9S+DY2OJCIi8hcPP/ww9vb2fPXVV2p0LiIiUkjsLWaGtQ7knd8OM29jDP2aVNHP3BLE0EbnqampHD16NO/r6OhooqKiKF++PAEBAUyaNImTJ0+yYMECAMaMGcOsWbN47rnneOSRR1izZg1Llizh559/NuoliEFC2wXx+4Ekvt1+gmd61MHDyf7vTxIRESlCe/fuZefOndSpU8foKCIiIqXakJYBzFhzlN0nktkRd5FmgRqRXFIY2uh827ZthISEEBISAsDEiRMJCQlhypQpACQkJBAX9//NyqpVq8bPP//MqlWrCA4O5t133+XTTz+lR48ehuQX47StUYFaPm6kZ+XyzbYTRscRERH5i+bNm6ttgIiISBGo4OZI3+DKAIRFxBgbRm6JyWaz2YwOUZRSUlLw9PQkOTlZDUZLuC83x/Li93sJKO/C2mc6YzFriKaIiNyZgrxP+Oabb3jllVd49tlnadSo0V8anTdu3PiOrm803VOJiEhxsvdkMr1nhmNnNhH+/F34eToZHalMu9n7BEOn74ncif4hVXjz14PEnU9n3aHTdK3na3QkERGRPIMGDQLgkUceydtnMpmw2WyYTCZyc3ONiiYiIlLqNKziScug8myJOc8Xm2J5poemz5cEKkpJieXiYMfglgF8sv44YRExKkqJiEixEh0dbXQEERGRMiW0XRBbYs7z1ZY4xt1VEyd7i9GR5G+oKCUl2rDWgXy64TgbjpzlSNIlavm6Gx1JREQEgMDAQKMjiIiIlCl31/elsqcTp5Iz+HHXKR5s7m90JPkbKkpJieZf3oVu9Xz5bX8S8yNjeK1fI6MjiYhIGbZ8+XJ69eqFvb09y5cvv+Gx9913XxGlEhERKRvsLGaGtQnizRUHCYuI4YFmVTGZ1Hu4OFNRSkq8ke2C+G1/Et9tP8mzPeri6Wz/9yeJiIgUgn79+pGYmIiPjw/9+vW77nHqKSUiIlI4hrT054PVh9l3KoWtMRdoWa280ZHkBsxGBxC5U22qV6COrzuXs3P5ZpuW3hYREeNYrVZ8fHzy/ny9TQUpERGRwuHl4kD/kCoAhEWov2Nxp6KUlHgmk4mR7YIAmB8ZQ67VZmwgEREp0yIjI/npp5/y7VuwYAHVqlXDx8eHxx57jMzMTIPSiYiIlH4j2gYBsHJfEicvXjY2jNyQilJSKvRrUgVPZ3viz19mzcHTRscREZEybOrUqezbty/v6z179vDoo4/SrVs3XnjhBX788UemTZtmYEIREZHSra6fB21rVCDXamNhZKzRceQGVJSSUsHZwcLglldWVtAQTRERMVJUVBRdu3bN+3rx4sW0atWKuXPnMnHiRGbMmMGSJUsMTCgiIlL6jfxztNTirXFcztK0+eJKRSkpNYa1DsRsgo1Hz3E46ZLRcUREpIy6cOECvr6+eV//8ccf9OrVK+/rFi1aEB+vHogiIiKFqWs9X/zLO3MxPZsfok4aHUeuQ0UpKTWqlnOhe30/AMIiYowNIyIiZZavry/R0VdG7WZlZbFjxw5at26d9/ilS5ewt9dKsSIiIoXJYjYxok0QAPM2xmCzqfdwcaSilJQqVxueL91xgovpWcaGERGRMumee+7hhRdeYMOGDUyaNAkXFxc6dOiQ9/ju3bupUaOGgQlFRETKhgeb++Nsb+FQ0iUij58zOo5cg4pSUqq0qlaeun7uZGRb+XqrpkaIiEjR+/e//42dnR2dOnVi7ty5zJ07FwcHh7zHP//8c7p3725gQhERkbLB09me+5tVASBsY4yxYeSaVJSSUsVkMvFIu2oALIiMJSfXanAiEREpa7y9vVm/fj0XLlzgwoUL9O/fP9/j33zzDS+//LJB6URERMqWqw3PVx1IIv58urFh5C9UlJJS574mlSnnYs/Ji5f5/cBpo+OIiEgZ5enpicVi+cv+8uXL5xs5JSIiIoWnpo87HWp5Y7PBgsgYo+PI/1BRSkodJ3sLQ1oGABAWEW1wGhERkYIxe/ZsgoKCcHJyolWrVmzZsuW6xy5dupTmzZvj5eWFq6srTZo0YeHChfmOSU1NZdy4cVStWhVnZ2fq16/PnDlzCvtliIiIFLnQP3sPL94aT1pmjrFhJB8VpaRUGto6EIvZxKbj5zmQkGJ0HBERkTvy9ddfM3HiRF5++WV27NhBcHAwPXr04PTpa48ILl++PC+++CKRkZHs3r2b0NBQQkNDWblyZd4xEydOZMWKFXzxxRccOHCACRMmMG7cOJYvX15UL0tERKRIdK7tQ1AFFy5l5LB050mj48h/UVFKSqXKXs70bOAHwPyIGGPDiIiI3KHp06czevRoQkND80Y0ubi48Pnnn1/z+M6dO9O/f3/q1atHjRo1eOqpp2jcuDHh4eF5x0RERDBixAg6d+5MUFAQjz32GMHBwTccgSUiIlISmc0mRvzZWypsYzQ2m83YQJJHRSkptUb+OUTz+50nuZCWZWwYERGR25SVlcX27dvp1q1b3j6z2Uy3bt2IjIz82/NtNhurV6/m0KFDdOzYMW9/27ZtWb58OSdPnsRms7F27VoOHz583ZUBMzMzSUlJybeJiIiUFA80q4qrg4VjZ9IIP3rW6DjyJxWlpNRqHliOBpU9yMyxsnhrvNFxREREbsvZs2fJzc3F19c3335fX18SExOve15ycjJubm44ODhw7733MnPmTO6+++68x2fOnEn9+vWpWrUqDg4O9OzZk9mzZ+crXP23adOm4enpmbf5+/sXzAsUEREpAu5O9jzY/MrPrrCNMcaGkTwqSkmpZTKZ8pb/XBgZQ06u1dhAIiIiRcjd3Z2oqCi2bt3K66+/zsSJE1m3bl3e4zNnzmTTpk0sX76c7du38+677/LEE0/w+++/X/N6kyZNIjk5OW+Lj9cHPiIiUrJcncK35tBpYs6mGRtGALAzOoBIYeoTXJlpvx7kVHIGq/Yn0atRJaMjiYiI3BJvb28sFgtJSUn59iclJeHn53fd88xmMzVr1gSgSZMmHDhwgGnTptG5c2cuX77Mv/71L77//nvuvfdeABo3bkxUVBTvvPNOvqmCVzk6OuLo6FiAr0xERKRoVfN2pUudiqw9dIb5kTG83KeB0ZHKPI2UklLNyd7CQy0DAJinhuciIlICOTg40KxZM1avXp23z2q1snr1atq0aXPT17FarWRmZgKQnZ1NdnY2ZnP+W0GLxYLVqpHFIiJSeo1sVw2Ab7adIDUzx+A0oqKUlHpDWwdiMZvYEn2efaeSjY4jIiJyyyZOnMjcuXOZP38+Bw4cYOzYsaSlpREaGgrA8OHDmTRpUt7x06ZNY9WqVRw/fpwDBw7w7rvvsnDhQoYOHQqAh4cHnTp14tlnn2XdunVER0cTFhbGggUL6N+/vyGvUUREpCh0rOVNjYqupGbm8O02TUU3mqbvSann5+lEr4Z+/LQ7gfkRMbz1QLDRkURERG7JoEGDOHPmDFOmTCExMZEmTZqwYsWKvObncXFx+UY9paWl8fjjj3PixAmcnZ2pW7cuX3zxBYMGDco7ZvHixUyaNImHH36Y8+fPExgYyOuvv86YMWOK/PWJiIgUlau9h1/6YR/zI2MZ3iYIs9lkdKwyy2Sz2WxGhyhKKSkpeHp6kpycjIeHh9FxpIhsjz3P/R9F4mBnZtOkrpR3dTA6koiIFEO6T7h5eq9ERKSkSsvMofW01VzKyGFeaAu61PExOlKpc7P3CZq+J2VC04ByNKriSVaOlUVb4oyOIyIiIiIiIgZxdbRjUHN/AOZtjDE2TBmnopSUCVeHaAJ8sSmW7Fw1cRURERERESmrhrcJwmSC9YfPcPR0qtFxyiwVpaTM6B1cCW83BxKSM1i5L9HoOCIiIiIiImKQgAoudK17pTfjgsgYY8OUYSpKSZnhaGfhoZYBAIRpiKaIiIiIiEiZFtouCIBvt58g+XK2sWHKKBWlpEx5uHUgdmYT22IvsOdEstFxRERERERExCBta1Sgtq8b6Vm5fLMt3ug4ZZKKUlKm+Ho4cW/jSgCERcQYG0ZEREREREQMc6X3cDUAFkTGkmu1GZyo7FFRSsqcqw3Pf9x1irOpmcaGEREREREREcP0D6mCp7M9cefTWXPwtNFxyhwVpQpaRgrYVF0tzkICyhHs70VWrpVFm+OMjiMiIiIiIiIGcXawMLilPwBhEdEGpyl7VJQqaN+GwtwusHcp5OYYnUauI/TP0VILN8WSnWs1NoyIiIiIiIgYZljrQMwm2Hj0HIeTLhkdp0xRUaogXUqCmI1waueV4tSsZrBlLmSlG51M/sc9jSpR0d2R05cy+XVvotFxRERERERExCBVy7nQvb4foN7DRU1FqYLk7gsT9kCn58G5HFyIgV+egfcbwrr/QNo5oxPKnxzszDzcKgCAsI0aoikiIiIiIlKWhbYLAmDpjhNcTM8yNkwZUiyKUrNnzyYoKAgnJydatWrFli1brntsdnY2U6dOpUaNGjg5OREcHMyKFSuKMO3fcKsIXf4FT++DXm+DVyCkn4N10+C9BvDzP+H8caNTCvBQqwDsLSZ2xF1kV/xFo+OIiIiIiIiIQVpWK0+9Sh5kZFv5emu80XHKDMOLUl9//TUTJ07k5ZdfZseOHQQHB9OjRw9On7521/vJkyfz8ccfM3PmTPbv38+YMWPo378/O3fuLOLkf8PBFVo9BuN3wAOfQ6UmkHMZtn4KM5vBkhFwcrvRKcs0H3cnejeuDMB8DdEUEREREREps0wmU17v4QWRseSo93CRMLwoNX36dEaPHk1oaCj169dnzpw5uLi48Pnnn1/z+IULF/Kvf/2Le+65h+rVqzN27Fjuuece3n333SJOfpMsdtDwfnhsHQxfDjW7gc0K+5fB3LsgrDccWaUV+wwy8s//dH7cfYrTlzKMDSMiIiIiIiKGua9JZcq7OnDy4mV+P5BkdJwywdCiVFZWFtu3b6dbt255+8xmM926dSMyMvKa52RmZuLk5JRvn7OzM+Hh4YWa9Y6ZTFC9Ewz9DsZshMaDwWwHMRvgywfgo7YQtQhyNHe1KAX7exES4EV2ro1FmzVEU0REREREpKxysrcwpKU/APM2xhgbpowwtCh19uxZcnNz8fX1zbff19eXxMRrr4jWo0cPpk+fzpEjR7BaraxatYqlS5eSkJBwzeMzMzNJSUnJtxnOryEM+Bie2gVtxoGDG5zeD8vGwAfBEDETMopBzjLi6mipLzbHkpWjIZoiIiIiIiJl1dDWgVjMJjZHn2f/Kf1eXtgMn753qz744ANq1apF3bp1cXBwYNy4cYSGhmI2X/ulTJs2DU9Pz7zN39+/iBPfgGdV6PH6laboXV8GN1+4dAp+m3ylKfqqKZBy7WKbFJxeDSvh4+7ImUuZ/LpX77eIiIiIiEhZVcnTmV4N/QAIi9BK7YXN0KKUt7c3FouFpKT8czWTkpLw8/O75jkVK1Zk2bJlpKWlERsby8GDB3Fzc6N69erXPH7SpEkkJyfnbfHxxXCKlrMXdJgIE/bAfTPBuzZkpsDGD+D9RvDDE3DmkNEpSy0HOzNDWwcCGqIpIiIiIiJS1oW2CwJgWdQpzqepxU5hMrQo5eDgQLNmzVi9enXePqvVyurVq2nTps0Nz3VycqJKlSrk5OTw3Xff0bdv32se5+joiIeHR76t2LJzhKbD4fHNMHgRBLQBazbs/AJmt4SvBkNshJqiF4IhLQNwsJiJir/IzrgLRscRERERERERgzQNKEejKp5k5VhZtCXO6DilmuHT9yZOnMjcuXOZP38+Bw4cYOzYsaSlpREaGgrA8OHDmTRpUt7xmzdvZunSpRw/fpwNGzbQs2dPrFYrzz33nFEvoeCZzVD3HnhkBTzyG9TtDZjg8K8wrxd8djfsXw7WXKOTlhoV3R3pHVwJgPkRMcaGEREREREREcOYTKa83sMLI2PJzlXv4cJieFFq0KBBvPPOO0yZMoUmTZoQFRXFihUr8pqfx8XF5WtinpGRweTJk6lfvz79+/enSpUqhIeH4+XlZdArKGQBrWDwlzBuKzQbCRZHOLEVlgyDWS1g2+eQfdnolKVCaNtqAPy8J4HTKRkGpxERERERERGj9A6uhLebA4kpGazcd+2F2OTOmWy2sjUXLCUlBU9PT5KTk4v3VL7rST0Nmz+GrXMhI/nKPhdvaDUGWjwKLuWNzVfC3f9RBNtjL/Bk11pMvLu20XFERKSIlfj7hCKk90pEREq76asOM2P1EZoHluPbsW2NjlOi3Ox9guEjpeQWuflA15fg6f3Q8z/g6Q/pZ2Hta1dW7Pv1ebgQa3TKEuvqEM2vNseSmaPpkSIiIiIiImXV0FYB2FtMbIu9wJ4TyUbHKZVUlCqpHN2g9Vh4cicM+BR8G0F2OmyeAzNC4NtHIWGX0SlLnJ4N/fDzcOJsahY/7074+xNERERERESkVPLxcOLeRld6D8+LiDY4TemkolRJZ7GHxg/CmA0w7Huo3hlsubD3W/i4IyzoC0dXa8W+m2RvMTOsTSAA8zbGUMZmt4qIiIiIiMh/GdnuSu/hn3YlcOZSpsFpSh8VpUoLkwlq3AXDf4B/rIdGD4LJAsfXwRcDYE4H2L0EcrONTlrsDW7hj4OdmT0nk9kRd9HoOCIiIiIiImKQJv5eNPH3IivXyqItcUbHKXVUlCqNKgXD/Z9emdrXaizYu0DSHlg6+srUvsgPITPV6JTFVgU3R/oGVwYgLCLG2DAiIiIiIiJiqNB2QQAs3BRLVo7V2DCljIpSpVm5QOj1H3h6H9w1GVwrQnI8rJx0pSn66qlwKcnolMXSiD8bnv+6J4HE5Axjw4iIiIiIiIhhejWshI+7I2cuZfLrXvUeLkgqSpUFLuWh47MwYQ/0fg/K14CMi7DhXXi/ESx/Es4eMTplsdKwiictg8qTY7Xx5WatZigiIiIiIlJWOdiZGdr6/3sPS8FRUaossXeG5o/AuK0w6Auo2gJyM2HHfJjVAhY/DHGbjU5ZbIz8c4jmV5vjyMjONTaMiIiIiIiIGOahVgE4WMxExV9kZ9wFo+OUGipKlUVmC9TrA4+ugtAVULsXYIODP8Hn3eGzHnDwZ7CW7bmy3ev7UsnTiXNpWfy0W0M0RUREREREyipvN0f6qPdwgVNRqiwzmSCwDTy0GJ7YAiFDweIA8Ztg8UMwuyVsnw/ZZbOnkp3FzLA2V4doRmOz2QxOJCIiIiIiIkYZ+Wfv4Z93J5CUUjZ/Ty5oKkrJFRXrQN/Z8NRuaDcBHD3h3BH48Un4oPGV/lOXy94QxcEtAnC0M7PvVArbY8ve6xcREREREZErGlX1pHlguSu9hzep93BBUFFK8vOoBHe/Ck/vhe6vg0cVSE26slLfew1hxb/gYrzRKYtMeVcH+jWpAsA8DdEUEREREREp00LbVQPgy81xZOao9/CdUlFKrs3JA9qOgyejoP/H4FMfslJh02yY0QSWPgaJe41OWSRG/DlEc8XeRBKSLxsbRkRERERERAzTvcF/9R7epd7Dd0pFKbkxOwcIHgxjI+Dh7yCoA1hzYPfXMKcdLBwAx/+AUtxvqX5lD1pVK0+u1cbUH/erMCUiIiIiIlJG2VvMDG39Z+/hCPUevlMqSsnNMZmgVjcY+ROMXgsN+oPJDMdWw4L74JNOsPc7yM0xOmmh+Een6gD8ujeRDm+uZeKSKA4mphicSkRERERERIrakJZXeg/vPanew3dKRSm5dVWawoNhMH4HtBgNds6QsAu+fQRmhsDmjyErzeiU/9fencdFVe5/AP/MDMywbwLDKogoi4koKuK+ZuXtZquVpXkry7RreW+lt27Lr1vUbfPe8rrk0p6m7WmW4r7liiugLIoLDJswMMAAM+f3x8DgCKMDwhxm5vN+vc5LOfPM4fs8T+C37zznOR1qbKwSn/5lMJJ7+KFBL+C7wxdxy8KdmL5yP3Znl7A6TkRERERE5CC493DHkQgO9n/TarUa3t7eqKiogJeXl9jh2AdNKXDgY2D/MqC61HDO1ddQsBo8E/AIEDe+DpZ+vhwf78jFrycKoG/86bkp1AuPj4jCpL7BcJKx1ktEZKuYJ1iOY0VERI4so0CNW/+zEzKpBLteGINgb1exQ+pSLM0TWJSijlNXDaR/Cez9CLh81nDOyQVIfBBImQN06ylqeB3tXKkGK3bl4ZuD51FbrwcAhPq44tHhPTBlUDjcFU4iR0hERG3FPMFyHCsiInJ09y/bi325ZXhqdE88f0us2OF0KSxKmcEEygr0OiDjJ2D3f4FLhxtPSoC424Fhc4GwgaKG19HKNHX4Yt85fLrnLEo1dQAAb1dnPDSkO6YPjUSgp4vIERIRkaWYJ1iOY0VERI5u44lCPPnFIfi6OWPvgnFwcZaJHVKXwaKUGUygrEgQgLO7gD3/Bc783nw+YpihOBU9AZDaz61utfU6fHv4ApbvzENeiWFPLblMijv7h+LxkVGIDvQQOUIiIroe5gmW41gREZGj0+kFjPz3Vlwsr8G/707AfYPCxQ6py2BRygwmUCJRnQL2fAgcXwvo6w3nAmKBoX8F+t4LOMnFja8D6fQCNp1SYdmOHBzOLzeeHx8XiJkje2JQpC8kEol4ARIRkVnMEyzHsSIiIgKW7cjBmxsyERvkiV/njuD/6zViUcoMJlAiq7gI/LEYOPgJUFdpOOcZDAyZBSQ9Arh4ixldhzt4tgxLd+Ric4YKTT9pieE+mDkyChP7BEEm5S8sIqKuhHmC5ThWREREQEV1PYakpqGmXofVM4dgSFQ3sUPqEliUMoMJVBdRWwEcXAX8sQSoLDCck3sCA2cYClReIeLG18FyiquwfGcevj18AXUNhk3RI7q54bHhPXBPUjhc5bz3mIioK2CeYDmOFRERkcE/vj+Or/7Ixy19grDk4SSxw+kSWJQygwlUF9OgNdzSt+dDoDjTcE7qbLilb+jTgDJe3Pg6WHGlFp/tPYvP951DebXhNkY/dzkeHhKBaSkR6OahEDlCIiLHxjzBchwrIiIigzOqSkz4YAekEmD7c2MQ7ucmdkiiY1HKDCZQXZReb9gMfc9/gXO7m8/3utmwKXrEMMCO7s2trmvANwfOY/muPFy4XAMAUDhJcU9SGB4fEYVIf3eRIyQickzMEyzHsSIiImr20PI/sCu7BE+MjMKC2+LEDkd0LEqZwQTKBlw4COz+D5DxM4DG/zxDBhiKU3G3A1L7udWtQafHxpOFWLYjF8cuVAAw1N4mxgdh5qgoDOjuK3KERESOhXmC5ThWREREzTafUuGxzw7Cy8UJ+/4xDm5yJ7FDEhWLUmYwgbIhpTnA3o+AI18COq3hnG8PYOgcIHEq4OwqbnwdSBAE7Mstw7IdOdiaVWw8PyjSFzNH9sS42EBIuSk6EVGnY55gOY4VERFRM51ewJh3tyG/rBpv3HkTpiZHiB2SqFiUMoMJlA2qKgb2LwMOfAzUXDacc+sGDH4CGPw44OYnbnwd7LSqEh/vyMUP6RdRrzP8eEYFuOPxEVG4s38oXJztZ6UYEVFXwzzBchwrIiIiUyt25eH1X06hV6AHfn92JCR2tAVNW7EoZQYTKBtWpwGOfGFYPVWebzjn5AoMeBhImQ34RooaXkdTqWuxavdZfPnHOVTWNgAA/D0UeGRoBB4aEgEfN7nIERIR2R/mCZbjWBEREZlS19ZjyJtpqK7T4cvHkjEs2l/skETDopQZTKDsgK4BOPWDYVP0gqOGcxIpED8ZGPZXIKS/mNF1uCptA1bvz8fKXXm4VFELAHCTy3DfwHA8OrwHn+xARNSBmCdYjmNFRETU0ss/nsBne89hfFwglk8fJHY4omFRygwmUHZEEIC87cDu/wI5ac3ne4wEhs4FosfZ1RP76nV6rD9WgKU7cpFRoAYAyKQS3NY3GDNHRKFvmLfIERIR2T7mCZbjWBEREbWUU1yFce9th0QCbPv7aER0c8wnq1uaJ0itGBNRx5JIgKjRwMPfAU/uAhKmABIZkLcD+PJuYPEw4OhqQFcvdqQdwlkmxeT+odjw1+H4/NHBGNHLHzq9gJ+PXsLtH+3CA8v2YWtWERyszkxERERERNRl9AzwwKjeARAE4LO958QOp8vjSimyL+XngX2LgUOfAPUawzmvUGDIU0DSdEDhKWp4He3kpQp8vCMXPx8rgE5v+FGOUXri8ZFR+HO/EMidWHcmImoL5gmW41gRERG1bmtWEWasOgBPhRP2/WMc3BVOYodkdbx9zwwmUA6i5jJwcCWwbwmgKTKcU3gDg/4CJD8JeAaJG18Hu1heg1W78vD1/nxo6nQAAKWXAjOG9cCDyd3h5eIscoRERLaBeYLlOFZERESt0+sFjH9/O3JLNPi/O/pgWkqk2CFZHYtSZjCBcjD1tcCxNYZN0UuzDedkcsOtfkP/CgT0Fje+DlZRU4+v/sjHqt15KKrUAgA8FE54YHA4ZgzrgRAfV5EjJCLq2pgnWI5jRUREZN6ne87ilZ9OIirAHZufHQWp1H72O7YE95QiAgBnF8Nte7MPAPd/BYQnA7o64MjnwKJBwNcPAPn7xI6yw3i7OmPW6J7Y+cIY/PueBPQK9ECVtgEf78zDyH9vxbw16cZN0omIyLYsWrQIkZGRcHFxQXJyMvbv32+27XfffYeBAwfCx8cH7u7uSExMxOeff27SRiKRtHq88847nd0VIiIiu3d3Uhg8FU7ILdZgZ3aJ2OF0WSxKkWOQSoHYScCjvwN/+R2I/RMACZC1AVg5EVg+Acj4GdDrxY60QyicZLhvYDh+e2YkVj0yCEOi/NCgF/DdkYu49T878fCKP7DrTAk3RScishFr1qzBvHnz8Morr+Dw4cPo168fJk6ciKKiolbb+/n54cUXX8TevXtx7NgxzJgxAzNmzMBvv/1mbFNQUGByrFy5EhKJBHfffbe1ukVERGS3PBROuHdgOABg1e48kaPpurrE7XuLFi3CO++8g8LCQvTr1w8ffvghBg8ebLb9woULsXjxYuTn58Pf3x/33HMPUlNT4eLict3vxaXmZFRyBtjzIXD0a8PqKQDoFg2kzAH6PWBYZWVHjl0ox9Idufj1eAEa90RHfLAXnhgVhdv6BsNZxho1EVFXzROSk5MxaNAgfPTRRwAAvV6P8PBwPP3005g/f75F1xgwYAAmTZqE119/vdXXJ0+ejMrKSqSlpVl0va46VkRERF3F2RINxry3DYIAbPnbKEQFeIgdktXYzO17bf3k76uvvsL8+fPxyiuvICMjAytWrMCaNWvwj3/8w8qRk83z7wX8+b/AMyeAEX8DXLwN+0798gywsC+w4x2gukzsKDtMQpgPFj04ANv+PgaPDI2Eq7MMpwrUmLs6HaPf2YblO3NRpW0QO0wiIrpKXV0dDh06hPHjxxvPSaVSjB8/Hnv37r3u+wVBQFpaGrKysjBy5MhW26hUKqxfvx6PPvqo2etotVqo1WqTg4iIiMyL9HfH2JhAAMBne8+JHE3XJHpR6v3338fjjz+OGTNmID4+HkuWLIGbmxtWrlzZavs9e/Zg2LBhePDBBxEZGYmbb74ZDzzwwDX3VSC6Jk8lMO5l4NmTwMRUwCvM8MS+Lf8CPrgJ+HU+UJ4vdpQdpns3N7z65z7YM38s/jahN/w95LhYXoN/rc/A0NQ0vL0xE0XqWrHDJCKiRiUlJdDpdFAqlSbnlUolCgsLzb6voqICHh4ekMvlmDRpEj788ENMmDCh1baffvopPD09cdddd5m9XmpqKry9vY1HeHh4+zpERETkQB4ZFgkAWHvwPNS19eIG0wWJWpRqzyd/Q4cOxaFDh4xFqNzcXGzYsAG33XabVWImO6bwBFKeAuamA3d9DChvAuo1wB+Lgf8kAt8+BmSuB9SXAPHver1hvu5yPD2uF3a9MBZv3tkXUf7uUNc2YPG2HAx/eyueX3cU2UWVYodJRETt5OnpifT0dBw4cABvvPEG5s2bh23btrXaduXKlZg6deo1t0JYsGABKioqjMf58+c7KXIiIiL7MTzaH9GBHtDU6bDu4AWxw+lynMT85tf65C8zM7PV9zz44IMoKSnB8OHDIQgCGhoa8OSTT5q9fU+r1UKr1Rq/5lJzui6ZM5BwH9D3XiBnC7D7P0DeduD4WsMBAB5KIDgRCOkPhDT+6RkkZtTt5uIsw4PJ3XH/oHBszlBh2Y5cHDx3Gd8cvIBvDl7AuNhAPD4yCsk9/CCRONZjTImIugJ/f3/IZDKoVCqT8yqVCkFB5v/tkUqliI6OBgAkJiYiIyMDqampGD16tEm7nTt3IisrC2vWrLlmHAqFAgqFon2dICIiclASiQSPDI3ESz+cwKd7z+KRoZGQSvn/VU1Ev32vrbZt24Y333wT//vf/3D48GF89913WL9+vdlNO7nUnNpNIgGixwHTfwJmbgcGTAcC+wASKVClAs78Bmx/C/j6fuC9GODdGOCr+4FtbwGnfwMqVdf/Hl2IVCrBzX2CsG7WUHw7aygm9lFCIgHSMotw/7J9mLxoN9YfK4BOb/urxIiIbIlcLkdSUpLJBuR6vR5paWlISUmx+Dp6vd7kg7omK1asQFJSEvr169ch8RIREZGpuwaEwsvFCedKq7E1q/X9sx2VqE/fq6urg5ubG9atW4fJkycbz0+fPh3l5eX48ccfW7xnxIgRGDJkCN555x3juS+++AIzZ85EVVUVpFLTOltrK6XCw8P5pBhqv7pqQHUCuHQEuJRu+LMkCxD0Ldt6hpiupgpOBDwCrBxw++UWV2H5rjysO3QBdQ2G/nX3c8NjI3rg3qRwuMplIkdIRNSxuuoT5dasWYPp06dj6dKlGDx4MBYuXIhvvvkGmZmZUCqVmDZtGkJDQ5GamgrA8KHcwIED0bNnT2i1WmzYsAHz58/H4sWL8dhjjxmvq1arERwcjPfeew9PPvlkm2LqqmNFRETUFb25IQPLduRiRC9/fP5ostjhdDpL8wRRb9+78pO/pqJU0yd/c+bMafU91dXVLQpPMpnhf4xbq69xqTl1OLkbED7YcDSp0wCFx5uLVJeOACWngcpLQNYlIGt9c1uvsMYiVSIQ3Fiwcve3bh8sFBXggTfv7It5E3rjsz1n8dm+c8gvq8bLP57EB5tO4+EhEZg2NBL+HvwZIyLqTFOmTEFxcTFefvllFBYWIjExERs3bjRugZCfn2+SH2k0Gjz11FO4cOECXF1dERsbiy+++AJTpkwxue7q1ashCAIeeOABq/aHiIjI0Tw8JALLd+Zi55kSnFFVopfSU+yQugRRV0oBbf/k79VXX8X777+PZcuWITk5GdnZ2Zg1axaSkpKuuxcCwE/1yIq0VUDhseZCVUE6UHIGQCs/ct7hpqupQvoDbn5WDdcS1XUNWHfoApbvzEN+WTUAQOEkxd1JYXh8RBR6+LuLHCER0Y1hnmA5jhUREVHbzPzsIH4/pcLU5O54486+YofTqWxipRTQ9k/+XnrpJUgkErz00ku4ePEiAgICcPvtt+ONN94QqwtErVN4ABFDDUcTbSVQcKy5SHXpCFCaDVScNxwZPze39eluWqQKSQRcfa3cCVNucidMS4nE1OQIbDxRiGU7cnD0QgW++iMfX+/Px83xSswc2RNJEeLGSURERERE1NXMGNYDv59S4bvDF/H8xFh4uzmLHZLoRF8pZW38VI+6nNqKloWqstzW2/pGmhapghMBVx9rRdqCIAjYn1eGZTtykZbZvGFfUoQvZo6MwoQ4JZ8sQUQ2hXmC5ThWREREbSMIAm79z05kFlbixdvi8PjIKLFD6jSW5gksShF1RTXlQMHR5iLVpXTgcl7rbf2iripU9QNcvK0WapMzqkp8vDMXPxy5hDqdYVP0KH93PDYiCncNCIWLMzdFJ6Kuj3mC5ThWREREbbd6fz7mf3ccYb6u2P7cGMjs9EN8FqXMYAJFNqvmsqFQ1bSR+qV0oPxc6239epo+9S8oAXCxzn/vReparNpzFl/sO4fK2gYAgL+HHNNSIvHwkAj4usutEgcRUXswT7Acx4qIiKjtaut1GJKahvLqeix9OAkT+wSJHVKnYFHKDCZQZFeqy0xXU11KByryW2koAbpFtyxUKTw6LbQqbQPWHDiPlbvycLG8BgDg6izDfQPD8NiIKIT7uXXa9yYiai/mCZbjWBEREbXP2xszsXhbDlKiuuHrmUPEDqdTsChlBhMosnuaUqDgyBVP/Ttq2ES9BQng39v0qX/BCYC8Y5+gV6/TY8PxAizdnotTBWoAgFQC3No3GE+MjEJCmE+Hfj8iohvBPMFyHCsiIqL2uVRegxH/3gqdXsDGZ0YgNsj+/h1lUcoMJlDkkDQlzUWqpg3V1RdbtpNIAf8Y00JVUF9AfuOrmgRBwJ6cUizdkYsdp4uN55N7+OGJUVEY3TuQm6ITkeiYJ1iOY0VERNR+s788jPXHC3D/oHC8dXeC2OF0OBalzGACRdSoquiK1VSNf1YWtGwnkQIBsc1FqpD+QNBNgLNru791RoEaH+/IxU9HL6FBb/gV1CvQA4+PjMIdiSFQOHFTdCISB/MEy3GsiIiI2u/A2TLcu2QvFE5S7Fswzu723mVRygwmUETXUFloKFQZ96k6AlSpWraTyIDAuMYiVSIQMgBQ9gGcXdr07S6V12DV7jx8vf88qrSGTdEDPRWYMawHHkzuDm9X5xvtERFRmzBPsBzHioiIqP0EQcCfPtyFk5fUeOGWWMwa3VPskDoUi1JmMIEiaiN1gelqqktHAE1xy3ZSpysKVY0bqitvApwU1/8WtfX46o98rNqdB5VaCwBwl8vwwODumDG8B0J92r8qi4ioLZgnWI5jRUREdGPWHjyP59YdQ4i3C3Y8PwZOMqnYIXUYFqXMYAJFdIMEAVBfuuqpf0eA6pKWbaXOhkLVlU/9C+wDOLW+NLWuQY+fjl7CxztykaWqBAA4SSX4U0IwZo7sifgQ/swSUedinmA5jhUREdGNqa3XYdhbW1CqqcP/pg7AbX2DxQ6pw7AoZQYTKKJOIAhAxYWWhaqaspZtpc6GW/2uLFQFxJkUqgRBwLbTxVi2PRd7c0uN50f08sfMkVEYHu0PiYSbohNRx2OeYDmOFRER0Y177/csfLglG4Mj/fDNkylih9NhWJQygwkUkZUIAlBx3rRIVZAO1Fxu2VYmN9zqd+VT/wLjAJkzjl+owLKduVh/7BIa90RHXLAXZo7sgT8lhMDZjpa4EpH4mCdYjmNFRER041TqWgx7awsa9AJ+eXo4bgr1FjukDsGilBlMoIhEJAhA+bmWT/2rrWjZVqYwPOUvpD8Q0h+F7rFYluGMrw8WoKZeBwAI8XbBX4b3wP2Du8ND4WTVrhCRfWKeYDmOFRERUcf469dH8NPRS7gnKQzv3ttP7HA6BItSZjCBIupiBAG4nHdVoeoooG2lUOXkiobAPjiFKHxXGIA9Nd2RI4TAzUWBqckRmDEsEkqvtj0BkIjoSswTLMexIiIi6hiH8y/jrv/tgVwmxZ4FY+Hvcf2HRXV1luYJXFpAROKSSAC/KMNx012Gc3p9Y6GqqUiVDhQcBbRqOF06iAQcRAIAKIBayHFCH4nju3vg3d1RCIxJwR3jR6F3sI9oXSIiIiIiIrJU/3Af9AvzxtELFVi9Px9zxvYSOySr4UopIrINej1Qlmt621/BUaCuqkVTjaDARZde8OwxEEFxKZCE9Ae6RQNSmfXjJiKbwjzBchwrIiKijvPDkYt4Zk06lF4K7HphrM3vncuVUkRkX6RSwD/acCTcazin1wOl2cYiVWXeQTgXHYM7atFbewLIPAFkfgIAEOQekAQlmD71z6+n4bpEREREREQiuq1vMN7YkAGVWotfTxTiz/1CxA7JKliUIiLbJZUCAb0NR8J98AQAvQ4Xso9j365N0Jw9hHjkoI/kHNzqqoD8PYajidwTCO4HBPUFfMIBr1DAO8zwp4eSBSsiIiIiIrIKuZMUU5O7Y+HmM/hkdx6LUkRENkkqQ1jvRNzTOxGlVVp8tvccntyTA7/afCRIcpEkP4fRnhcQXJMNSV0lcG6X4WhxHSfAMwTwDm0sVoUCXmFXfB0GuHUz7IlFRERERER0g6YmR2DR1mwczi/H0fPl6BfuI3ZInY5FKSKyW908FHh2Qm88Oaon1h06j+W78vBtaTVQC7g6CXgirgEPhpcisCYXUF8EKi4a/qwsAPQNQEW+4TDHyQXwCjFdYXV18crFm4UrIiIiIiK6rgBPBW5PCMF3Ry7ikz1n8cGURLFD6nTc6JyIHIZOL+D3k4VYuiMX6efLARjqReNiA/GnhBCMiQmEt5szoGsAqgobi1QXmotVFReai1eaIsu+qdzjimKVmeKV3L3zOk1EbcI8wXIcKyIioo539Hw57li0G84yCXbPH4tATxexQ2oXbnRORHQVmVSCW/sG45abgnDg7GUs25GDzRlFxkMmlWBwpB/Gxytxc7wS4d3DACS3frEGLaC+1HhcVbBqKmTVlBmeDliSZTjMcfG5qlh1dfEqFHBSdMaQEBERERFRF9Iv3AcDuvvgcH45vtyXj2cn9BY7pE7FlVJE5NCyiyrx/ZGL2HRKhdOqKpPXYpSemBCvxPh4JRJCvSGVtvE2vLrqxqKVmdVW6ouAVm3ZtdwDWi9WNX3tGQzI+DkD0Y1inmA5jhUREVHn+PnoJTz99RH4eyiwe/4YKJxkYofUZpbmCSxKERE1OleqweaMImw6VYgDZy9Dp2/+9RjoqcC4OMMKqpSe3eDi3EH/MNSqW66wurp41VBz/etIpIBH0LVvE3QP5BMFia6DeYLlOFZERESdo16nx/C3t0Cl1uKDKf1wZ/8wsUNqMxalzGACRUSWKK+uw9asImw+VYRtWUXQ1OmMr7nJZRjZKwDj45UYGxsIP3d55wUiCEDN5SuKVFettKq4YFiNpa+//rWkzoBXsKFI5RXCJwoStYJ5guU4VkRERJ3noy1n8O7vp5EQ5o0fZw+DxMZydBalzGACRURtpW3QYV9uGTadKsTmU0UoVNcaX5NKgIERfhgfH4gJ8UHo4S/CpuV6PaApvvZtgpUFgKC//rX4REFycMwTLMexIiIi6jylVVqkvLUFdQ16fDtrKJIifMUOqU1YlDKDCRQR3QhBEHDiohqbMlTYfEqFUwWme0L1DHDHhPggTIgPRGK4L2Rt3Yeqs/CJgkQWYZ5gOY4VERFR53pu7VGsPXQBt/cLwYcP9Bc7nDZhUcoMJlBE1JEuXK5GWkYRNp1SYV9uKRqu2IfK30OOsbGGFVTDo/3hKu/iGxQanyjY2h5XjV/XXLbsWnyiINko5gmW41gRERF1rpOXKjDpv7vgJJVg1wtjEeTtInZIFmNRygwmUETUWdS19diWVYzNp1TYmlWEytoG42suzlIMjw7AhPhAjI1VIsDTRgsydRpD4crk9sCrild1lZZdi08UpC6IeYLlOFZERESd776le7E/rwxzxkTj7xNjxA7HYixKmcEEioisoa5BjwNny7DplAqbTqlwsbz5CXoSCdA/3Afj4w1P8+sZ4GFzGxdeU22Fmb2tGjdl5xMFqQtjnmA5jhUREVHn+/V4AWZ9eRh+7nLsmT+2454C3slYlDKDCRQRWZsgCMgsrMSmUypszlDh2IUKk9cju7lhQrwS4+OUSIrwhZPMzossnfVEwdaKV54hhicKsnBFFmKeYDmOFRERUedr0Okx6p1tuFheg3fuScC9A8PFDskiLEqZwQSKiMRWUFFj3Idqb04p6nTNT8XzdXPGmNhA3ByvxIheAXBXOOjtax35REGJDPAIBDyUhsNTecXfg5r/7qEEnG3nPn3qHMwTLMexIiIiso4l23Pw1q+ZiA/2wvq/DreJuyxYlDKDCRQRdSVV2gbsOG3Yh2pLVhHKq5tXB8mdpBjWsxvGN66iUnqxYGLCoicKFgNowz9zLt6GWwY9Ak0LVp6N55pec/U13IdJdod5guU4VkRERNZRXl2HIalpqK3X45snUjC4h5/YIV0Xi1JmMIEioq6qQafHwXOXsfmUCpsyVDhXWm3yer8wb4yPU2JCHyVilJ428QmJ6HT1gKbEULyqVAFVVxyVhVf8XQXotJZfV6ZoLFgFmhasrlyF1fS6zLnz+kcdjnmC5ThWRERE1rPgu2P4ev953NY3CP+bmiR2ONfFopQZTKCIyBYIgoDsoir83rgP1ZH8cpPXw/1cDQWqOCUG9fCDs73vQ9XZBMGwQbuxWFVkKGQ1FayqGs9VFgK15W24sMSwp1Wrtw02rbxqfE3h2Vm9ozZgnmA5jhUREZH1ZBVWYuLCHZBJJdjx/BiE+riKHdI1sShlBhMoIrJFRZW12NK4D9Wu7BJoG5r3UvJyccKY2EBMiFdiVO8AeLpwZU6natBeUay6qmB15corTRGgb7D8us7urdw22FTIumIVlps/N27vRMwTLMexIiIisq4HP96HPTmleHJUT8y/NVbscK6JRSkzmEARka2rrmvArjMl2HRKhS2ZRSjV1Blfc5ZJMCSqGybEKzEuTtnlP0Gxa3o9UFNmWqxqbeVVVRFQV2n5dSUywD2gsUh1rf2vlIAz57+tmCdYjmNFRERkXb+fLMTMzw/Bx80Ze+ePg6tcJnZIZrEoZQYTKCKyJzq9gCP5l7EpQ4VNp1TILdaYvN4nxMtwm1+8En1CvLgPVVdVp7nqtsFWVl5Vqdq+cbvC2/xtg1ee58btRswTLMexIiIisi6dXsDod7fifFkNUu/qiwcGdxc7JLNsqii1aNEivPPOOygsLES/fv3w4YcfYvDgwa22HT16NLZv397i/G233Yb169df93sxgSIie5ZTXIXNjftQHTp3GforfsOHeLsYn+Q3JKob5E68Bczm6BoMhalWV16pTDdzb6i1/LoyuekG7S1WYV3x5EE737ideYLlOFZERETWt3xnLv61PgMxSk9sfGZEl/3Q2WaKUmvWrMG0adOwZMkSJCcnY+HChVi7di2ysrIQGBjYon1ZWRnq6ppvVSktLUW/fv2wfPlyPPLII9f9fkygiMhRlFZpsSWzCJszVNhxugQ19Trjax4KJ4yKCcDN8UqM7h0Ibzf7LjQ4HOPG7VevvLrqtsGqQqDmctuu7dbNzG2DV+1/JfewydVXzBMsx7EiIiKyvoqaegx5Mw019Tp89Xgyhvb0FzukVtlMUSo5ORmDBg3CRx99BADQ6/UIDw/H008/jfnz51/3/QsXLsTLL7+MgoICuLu7X7c9EygickS19TrsyTHsQ7U5owjFlVrja05SCQb38DPe5hfu5yZipGR1DdrGAlXTyqtWbhtsOtq0cbvbNVZeXXE7oVs3QNp19kNgnmA5jhUREZE4XvrhOL7Yl48J8Up8PG2g2OG0yiaKUnV1dXBzc8O6deswefJk4/np06ejvLwcP/7443Wv0bdvX6SkpGDZsmUWfU8mUETk6PR6AUcvlGNz4z5Up1VVJq/HBnkaC1R9Q70hldreahfqBHq9YVWVya2Cray8qlS1b+N2sxu2X7H/lRU2bmeeYDmOFRERkTiyiyox/v0dkEiAHc+N6ZIfKluaJzhZMaYWSkpKoNPpoFQqTc4rlUpkZmZe9/379+/HiRMnsGLFCrNttFottNrmFQFqtbr9ARMR2QGpVIL+3X3Rv7svnpsYi3OlmsYVVCocOHsZmYWVyCysxEdbs6H0UmBcnBIT4pRI6dkNLs5dZ0ULWZlUCrh3MxzKPtduW6dpXF1lZsP2puKVphgQdI3FrUKg8Ni1r6vwbi5eJdwHDJjWcf0jIiIishHRgZ4Y0csfO8+U4LO9Z/HipHixQ2o3UYtSN2rFihXo27ev2U3RASA1NRWvvfaaFaMiIrItEd3c8diIKDw2Igrl1XXYmlWETadU2J5VDJVai6/+yMdXf+TDTS7DyF4BmBCvxJjYQPi5y8UOnboquTvgF2U4rkXXAFSXXHvD9qaiVkMtoK0wHKVngMjh1ukLERERURc0Y1gkdp4pweoD5/HM+N5wV9hmeUfUqP39/SGTyaBSqUzOq1QqBAUFXfO9Go0Gq1evxv/93/9ds92CBQswb94849dqtRrh4eHtD5qIyI75uMlxZ/8w3Nk/DNoGHfbllmHTqUJsPlWEQnUtNp4sxMaThZBKgIGRfpgQp8T4eCV6+F9/Tz+iFmROhlVPnkFA8DXaCQKgVZuuvAqItVqYRERERF3N6N6BiOzmhrOl1fjuyEU8PCRC7JDaRdSilFwuR1JSEtLS0ox7Sun1eqSlpWHOnDnXfO/atWuh1Wrx0EMPXbOdQqGAQqHoqJCJiByGwkmGUb0DMKp3AF6/Q8CJi2psatyHKqNAjf15ZdifV4Y3NmQgOtDDuA9V/3Af7kNFHUsiAVy8DYd/L7GjISIiIhKdVCrB9KGReO3nU/hkdx4eSu4OiQ0++Vj09V3z5s3D9OnTMXDgQAwePBgLFy6ERqPBjBkzAADTpk1DaGgoUlNTTd63YsUKTJ48Gd26dRMjbCIihyKRSNA3zBt9w7wxb0JvXLhcjc2NT/Lbl1uK7KIqZBdVYcn2HPh7yDEu1rCCani0P1zl3IeKiIiIiKij3ZMUhvd+P42cYg12ZZdgRK8AsUNqM9GLUlOmTEFxcTFefvllFBYWIjExERs3bjRufp6fnw+pVGrynqysLOzatQu///67GCETETm8MF83PDKsBx4Z1gPq2npsyyrG5lMqbM0qQklVHdYcPI81B8/DxVmK4dEBuDleibFxgfD34MpVIiIiIqKO4OnijHuSwvDJnrNYtfusTRalJIIgCGIHYU18fDERUeepa9DjwNkybDpluM3vYnmN8TWJBBjQ3bfxNr9A9AzwsMklxmTfmCdYjmNFREQkvrwSDca8uw0AsPXvo7vMXq+W5gksShERUacQBAEZBZXY3LgP1fGLFSav9/B3x/i4QEyID0JShC9k3IeKugDmCZbjWBEREXUNM1btx9asYswYFolXbu8jdjgAWJQyiwkUEZE4CipqkJZRhE2nVNibU4o6nd74mq+bM8bGGlZQjegVYLOPtCXbxzzBchwrIiKirmH76WJMX7kfHgon7F0wFp4uzmKHZHGewKyfiIisItjbFQ8NicBDQyJQpW3AjtOGfai2ZBXhcnU9vj18Ad8evgC5kxTDenbDhPggjI8LRKCXi9ihExERERF1WSN7+aNngDtyijX49tAFPDKsh9ghWYwrpYiISFQNOj0Onrts3Icqv6za5PV+4T6Y0HibX28l96GizsU8wXIcKyIioq7j871n8c8fT6KHvzvS5o2CVOStMXj7nhlMoIiIui5BEHCmqAqbTqmwOUOFI/nlJq+H+7k2bpSuxKBIPzjLpK1fiKidmCdYjmNFRETUdWi0DRiSmobK2gasemQQxsQGihoPb98jIiKbI5FI0Fvpid5KT8weE42iylpsadyHald2Cc6X1WDVbsMjb71dnTEmJgDj45UY1TugS9w7T0REREQkBneFE6YMDMfyXXlYtees6EUpS3GlFBER2YTqugbsPFOCzadUSMssQpmmzvias0yCIVHdcHO8EuPilAjxcRUxUrJlzBMsx7EiIiLqWvJLqzHq3a0QBGDzvFGIDvQQLRZL8wTe90BERDbBTe6EiX2C8M69/XDgxfFY92QKnhgVhagAd9TrBOw8U4J//ngSQ9/agj99uBMLN5/G/rwyqGvrxQ6dqEMsWrQIkZGRcHFxQXJyMvbv32+27XfffYeBAwfCx8cH7u7uSExMxOeff96iXUZGBv785z/D29sb7u7uGDRoEPLz8zuzG0RERNRJundzw7hYJQDg0z1nxQ3GQlwpRURENi+nuAqbG/ehOnjuMq7+ly3M1xVxwV6IC/JEXLAXYoO9EOHnJvoGkNT1dNU8Yc2aNZg2bRqWLFmC5ORkLFy4EGvXrkVWVhYCA1suz9+2bRsuX76M2NhYyOVy/PLLL/jb3/6G9evXY+LEiQCAnJwcDB48GI8++igeeOABeHl54eTJkxgyZEir17xaVx0rIiIiR7YnuwQPLv8DbnIZ9i4YB29Xcba44EbnZjCBIiKyb6VVWmzJLEJaRhGOXSjHpYraVtu5yWWICfJEbJAX4oMNxaqYIE/uTeXgumqekJycjEGDBuGjjz4CAOj1eoSHh+Ppp5/G/PnzLbrGgAEDMGnSJLz++usAgPvvvx/Ozs6trqCyRFcdKyIiIkcmCAImLtyB06oqvDQpDo+NiBIlDm50TkREDqmbhwL3DgzHvQPDAQDl1XXIKKhEZqEaGQVqZBRUIktVieo6HY7kl7f6hL+4IMNqqqZiVbgvV1WReOrq6nDo0CEsWLDAeE4qlWL8+PHYu3fvdd8vCAK2bNmCrKwsvP322wAMRa3169fj+eefx8SJE3HkyBH06NEDCxYswOTJk1u9jlarhVarNX6tVqtvrGNERETU4SQSCR4Z2gP/+P44Ptt7DjOG9YCsC+exLEoREZFd83GTI6VnN6T07GY816DT42ypBqcKKpFZ0FysKlTX4nxZDc6X1eD3Uypje/fGVVVNt/7FB3siJsgLHgr+M0qdr6SkBDqdDkql0uS8UqlEZmam2fdVVFQgNDQUWq0WMpkM//vf/zBhwgQAQFFREaqqqvDWW2/hX//6F95++21s3LgRd911F7Zu3YpRo0a1uF5qaipee+21ju0cERERdbg7+4fi7Y2ZyC+rxpbMIkyIV17/TSJhNk1ERA7HSSZFdKAnogM98ed+IcbzlzV1yCg0FKgyC9TIKFTjtKoKmjodDueX4/BVq6q6+7khrnE1leE2QC+E+bpyVRV1CZ6enkhPT0dVVRXS0tIwb948REVFYfTo0dDr9QCAO+64A88++ywAIDExEXv27MGSJUtaLUotWLAA8+bNM36tVqsRHh5unc4QERGRxVzlMtw/OBxLt+fikz15LEoRERHZAl93OYb29MfQnv7Gcw06PfJKNDjVuJqq6TZAlVqL/LJq5JdV47eTzauqPBROjauqmotVsUGecOeqKmonf39/yGQyqFQqk/MqlQpBQUFm3yeVShEdHQ3AUHDKyMhAamoqRo8eDX9/fzg5OSE+Pt7kPXFxcdi1a1er11MoFFAoFDfYGyIiIrKGh4dE4OMdudidXYrTqkr0VnqKHVKrmCETERFdg5NMil5KT/RSeuKOxObzZZo6ZBaoTYpVZ1RVqNI24NC5yzh07rKxrUQCRPi5ITbIy/AUwMaCVZivKyQSrqqia5PL5UhKSkJaWppxvye9Xo+0tDTMmTPH4uvo9XrjnlByuRyDBg1CVlaWSZvTp08jIiKiw2InIiIicYT5umFinyD8eqIQq3afRepdfcUOqVUsShEREbWDn7scQ6P9MTS6eVVVvU6P3GINMgsNxarMgkpkFKhRVKnF2dJqnC2txsaThcb2ngonxAZ7mhSrYoI84SbnP89kat68eZg+fToGDhyIwYMHY+HChdBoNJgxYwYAYNq0aQgNDUVqaioAw/5PAwcORM+ePaHVarFhwwZ8/vnnWLx4sfGazz33HKZMmYKRI0dizJgx2LhxI37++Wds27ZNjC4SERFRB3tkaCR+PVGI749cwAu3xMDHTS52SC0w6yUiIuogzjIpYoIMhaU7EkON50urtMgsNBSomlZWZRdVolLbgANnL+PAWdNVVZHd3BHbuLF6U7Eq1IerqhzZlClTUFxcjJdffhmFhYVITEzExo0bjZuf5+fnQyqVGttrNBo89dRTuHDhAlxdXREbG4svvvgCU6ZMMba58847sWTJEqSmpuKvf/0rYmJi8O2332L48OFW7x8RERF1vME9/BAX7IWMAjXWHDiPJ0b1FDukFiSCIAhiB2FNarUa3t7eqKiogJeXl9jhEBGRg6rX6ZFTXGVcTXWqQI3MwkoUV2pbbe/p4oS4IEOBKraxWBWj9ISrXGblyO0b8wTLcayIiIi6vm8OnMfz3x5DqI8rtj83Gk4y6fXf1AEszRO4UoqIiEgEzjJp4yboXpjcv3lVVUmVFhlX3Pp3qkCNnOIqVNY2YP/ZMuw/W2ZsK5EAPbq5N26o3riyKsQLId4uXFVFRERERPhzYgje2piJi+U12Jyhwi03BYsdkgkWpYiIiLoQfw8FRvQKwIheAcZzdQ2GVVUZjaupMgoMTwAsqapDbokGuSUarD9eYGzv5eKE2GAvxF9RrIoJ8oSLM1dVERERETkSF2cZHhgcjkVbc7Bq91kWpYiIiKht5E5S4/5SVyqu1BoLVE3FquyiKqhrG7A/rwz785pXVUklQKS/YVXVlcWqYK6qIiIiIrJrDw2JwJLtufgjrwwnL1WgT4i32CEZsShFRERkowI8FQjwDMDI3s2rqrQNOuQUaVoUq0o1dcgt1iC3WIP1x5pXVXm7OhsLVPHBXogN9kRvJVdVEREREdmLYG9X3HpTEH45VoBP95zFv+/pJ3ZIRixKERER2RGFkwzxIV6ID2leVSUIgmFV1RW3/mUWVCKnuAoVNfX4I68Mf1y1qioqwKNFsSrIi6uqiIiIiGzRjGGR+OVYAX5Iv4T5t8bBz10udkgAWJQiIiKyexKJBIFeLgj0csGoq1ZVnVFVmexTlVGgxuXqemQXVSG7qAq/XLGqysfNGXFBhgJVXLAX4oK80EvpwVVVRERERF3cgO6+6BvqjeMXK/D1/nzMHhMtdkgAWJQiIiJyWAonGW4K9cZNoc37CgiCgKJKLU5d8QTAzEI1coo1KK+ux97cUuzNLTW2l0kliPJ3R2ywF+KuKFYpvRRcVUVERETURUgkEswYFol53xzF53vPYebIKDjLpGKHxaIUERERNZNIJFB6uUDp5YIxMYHG87X1OmQXVTWupmpcWVWoRnl1Pc4UVeFMURV+Ptp8HV83Z8QFeyE2qLlYFR3IVVVEREREYpmUEIw3N2SgUF2L304W4k8JIWKHxKIUERERXZ+Lc+urqlRqrbFA1VSsyi2uwuXqeuzJKcWeHNNVVT0D3E2KVfHBXgjw5KoqIiIios6mcJLhweQI/DftDD7ZfZZFKSIiIrJdEokEQd4uCPJ2wZhY01VVZ1RVVxSrDAWripp6nFZV4bSqCj/ikrG9n7vcsJoqyMt4G2B0oAcUTlxVRURERNSRHkrujsXbsnHw3GUcv1CBvmHe139TJ2JRioiIiDqUi7MMfcO8TZIcQRBQqK41vf2vQI28Eg3KNHXYnV2K3dnNq6qcpBL0DPAw3vrXVKwK9HQRo0tEREREdiHQywWT+gbjh/RLWLUnD+/flyhqPCxKERERUaeTSCQI9nZFsLcrxsYqjedr6nQ4U1TZolilrm1AlqoSWapK/JDevKrK30OO2CAv3Nk/FHcnhYnRFSIiIiKb9siwHvgh/RJ+OVqABbfGIcBTIVosLEoRERGRaFzlMiSE+SAhzMd4ThAEXKqoRWZB861/GYWGVVUlVXXYlV2CgZG+4gVNREREZMMSw32QGO6D9PPlWHvoPJ4aHS1aLCxKERERUZcikUgQ6uOKUB9XjIszXVWVpapEZoHapIhFRERERG0zb0JvFFdq8ad+waLGwaIUERER2QRXucz4yR4RERERtd/I3gFihwAAkIodABEREREREREROR4WpYiIiIiIiIiIyOpYlCIiIiIiIiIiIqtjUYqIiIiIiIiIiKyuSxSlFi1ahMjISLi4uCA5ORn79++/Zvvy8nLMnj0bwcHBUCgU6N27NzZs2GClaImIiIiIiIiI6EaJ/vS9NWvWYN68eViyZAmSk5OxcOFCTJw4EVlZWQgMDGzRvq6uDhMmTEBgYCDWrVuH0NBQnDt3Dj4+PtYPnoiIiIiIiIiI2kX0otT777+Pxx9/HDNmzAAALFmyBOvXr8fKlSsxf/78Fu1XrlyJsrIy7NmzB87OzgCAyMhIa4ZMREREREREREQ3SNTb9+rq6nDo0CGMHz/eeE4qlWL8+PHYu3dvq+/56aefkJKSgtmzZ0OpVOKmm27Cm2++CZ1O12p7rVYLtVptchARERERERERkbhELUqVlJRAp9NBqVSanFcqlSgsLGz1Pbm5uVi3bh10Oh02bNiAf/7zn3jvvffwr3/9q9X2qamp8Pb2Nh7h4eEd3g8iIiIiIiIiImqbLrHReVvo9XoEBgZi2bJlSEpKwpQpU/Diiy9iyZIlrbZfsGABKioqjMf58+etHDEREREREREREV1N1D2l/P39IZPJoFKpTM6rVCoEBQW1+p7g4GA4OztDJpMZz8XFxaGwsBB1dXWQy+Um7RUKBRQKRccHT0RERERERERE7SbqSim5XI6kpCSkpaUZz+n1eqSlpSElJaXV9wwbNgzZ2dnQ6/XGc6dPn0ZwcHCLghQREREREREREXVNot++N2/ePHz88cf49NNPkZGRgVmzZkGj0Rifxjdt2jQsWLDA2H7WrFkoKyvD3Llzcfr0aaxfvx5vvvkmZs+eLVYXiIiIiIiIiIiojUS9fQ8ApkyZguLiYrz88ssoLCxEYmIiNm7caNz8PD8/H1Jpc+0sPDwcv/32G5599lkkJCQgNDQUc+fOxQsvvCBWF4iIiIiIiIiIqI0kgiAIYgdhTWq1Gt7e3qioqICXl5fY4RAREVEXwjzBchwrIiIiMsfSPEH02/eIiIiIiIiIiMjxiH77nrU1LQxTq9UiR0JERERdTVN+4GALyduFORURERGZY2lO5XBFqcrKSgCGvamIiIiIWlNZWQlvb2+xw+jSmFMRERHR9Vwvp3K4PaX0ej0uXboET09PSCSSDr++Wq1GeHg4zp8/b/f7KzhSXwHH6i/7ar8cqb/sq33q7L4KgoDKykqEhISYPGiFWmJO1XEcqa+AY/WXfbVPjtRXwLH6y752HEtzKodbKSWVShEWFtbp38fLy8vu/yNu4kh9BRyrv+yr/XKk/rKv9qkz+8oVUpZhTtXxHKmvgGP1l321T47UV8Cx+su+dgxLcip+BEhERERERERERFbHohQREREREREREVkdi1IdTKFQ4JVXXoFCoRA7lE7nSH0FHKu/7Kv9cqT+sq/2yZH66ugcaa4dqa+AY/WXfbVPjtRXwLH6y75an8NtdE5EREREREREROLjSikiIiIiIiIiIrI6FqWIiIiIiIiIiMjqWJQiIiIiIiIiIiKrY1GqjXbs2IHbb78dISEhkEgk+OGHH677nm3btmHAgAFQKBSIjo7GJ5980ulxdoS29nXbtm2QSCQtjsLCQusEfANSU1MxaNAgeHp6IjAwEJMnT0ZWVtZ137d27VrExsbCxcUFffv2xYYNG6wQ7Y1pT18/+eSTFvPq4uJipYjbb/HixUhISICXlxe8vLyQkpKCX3/99ZrvscU5bdLW/trqvLbmrbfegkQiwTPPPHPNdrY8v00s6astz+2rr77aIvbY2Nhrvsce5tXROFI+BTCnYk7VzFZ/PzOncoycypHyKcC+cypbyqdYlGojjUaDfv36YdGiRRa1z8vLw6RJkzBmzBikp6fjmWeewWOPPYbffvutkyO9cW3ta5OsrCwUFBQYj8DAwE6KsONs374ds2fPxr59+7Bp0ybU19fj5ptvhkajMfuePXv24IEHHsCjjz6KI0eOYPLkyZg8eTJOnDhhxcjbrj19BQAvLy+TeT137pyVIm6/sLAwvPXWWzh06BAOHjyIsWPH4o477sDJkydbbW+rc9qkrf0FbHNer3bgwAEsXboUCQkJ12xn6/MLWN5XwLbntk+fPiax79q1y2xbe5hXR+RI+RTAnIo5lSlb/P3MnMr+cypHyqcAx8ipbCafEqjdAAjff//9Nds8//zzQp8+fUzOTZkyRZg4cWInRtbxLOnr1q1bBQDC5cuXrRJTZyoqKhIACNu3bzfb5r777hMmTZpkci45OVl44oknOju8DmVJX1etWiV4e3tbL6hO5OvrKyxfvrzV1+xlTq90rf7aw7xWVlYKvXr1EjZt2iSMGjVKmDt3rtm2tj6/bemrLc/tK6+8IvTr18/i9rY+r+RY+ZQgMKdqjb38HDOnamYvc3ole86pHCmfEgTHyKlsKZ/iSqlOtnfvXowfP97k3MSJE7F3716RIup8iYmJCA4OxoQJE7B7926xw2mXiooKAICfn5/ZNvYyt5b0FQCqqqoQERGB8PDw635S1BXpdDqsXr0aGo0GKSkprbaxlzkFLOsvYPvzOnv2bEyaNKnFvLXG1ue3LX0FbHtuz5w5g5CQEERFRWHq1KnIz88329bW55Us46jzzJzKtuaXOVUze5lTwDFyKkfKpwDHyalsJZ9y6vTv4OAKCwuhVCpNzimVSqjVatTU1MDV1VWkyDpecHAwlixZgoEDB0Kr1WL58uUYPXo0/vjjDwwYMEDs8Cym1+vxzDPPYNiwYbjpppvMtjM3t7aw30MTS/saExODlStXIiEhARUVFXj33XcxdOhQnDx5EmFhYVaMuO2OHz+OlJQU1NbWwsPDA99//z3i4+NbbWsPc9qW/tryvALA6tWrcfjwYRw4cMCi9rY8v23tqy3PbXJyMj755BPExMSgoKAAr732GkaMGIETJ07A09OzRXtbnleynCPlUwBzKsD2fo6ZU5myhzl1lJzKkfIpwHFyKlvKp1iUog4TExODmJgY49dDhw5FTk4OPvjgA3z++eciRtY2s2fPxokTJ655z629sLSvKSkpJp8MDR06FHFxcVi6dClef/31zg7zhsTExCA9PR0VFRVYt24dpk+fju3bt5tNKmxdW/pry/N6/vx5zJ07F5s2bbKJzSZvRHv6astze+uttxr/npCQgOTkZEREROCbb77Bo48+KmJkRNbDnMr2MKeyP46QUzlSPgU4Vk5lS/kUi1KdLCgoCCqVyuScSqWCl5eX3X2q15rBgwfbVCIyZ84c/PLLL9ixY8d1K9/m5jYoKKgzQ+wwbenr1ZydndG/f39kZ2d3UnQdRy6XIzo6GgCQlJSEAwcO4D//+Q+WLl3aoq2tzynQtv5ezZbm9dChQygqKjJZMaDT6bBjxw589NFH0Gq1kMlkJu+x1fltT1+vZktzezUfHx/07t3bbOy2Oq/UNo6eTwHMqboy5lTMqa5mK/PqSPkU4Ng5VVfOp7inVCdLSUlBWlqayblNmzZd835ke5Keno7g4GCxw7guQRAwZ84cfP/999iyZQt69Ohx3ffY6ty2p69X0+l0OH78uE3M7dX0ej20Wm2rr9nqnF7Ltfp7NVua13HjxuH48eNIT083HgMHDsTUqVORnp7eakJhq/Pbnr5ezZbm9mpVVVXIyckxG7utziu1DeeZOVVXxJyKOZU5tjKvjpRPAY6dU3XpfKrTt1K3M5WVlcKRI0eEI0eOCACE999/Xzhy5Ihw7tw5QRAEYf78+cLDDz9sbJ+bmyu4ubkJzz33nJCRkSEsWrRIkMlkwsaNG8XqgsXa2tcPPvhA+OGHH4QzZ84Ix48fF+bOnStIpVJh8+bNYnXBYrNmzRK8vb2Fbdu2CQUFBcajurra2Obhhx8W5s+fb/x69+7dgpOTk/Duu+8KGRkZwiuvvCI4OzsLx48fF6MLFmtPX1977TXht99+E3JycoRDhw4J999/v+Di4iKcPHlSjC5YbP78+cL27duFvLw84dixY8L8+fMFiUQi/P7774Ig2M+cNmlrf211Xs25+ukp9ja/V7peX215bv/2t78J27ZtE/Ly8oTdu3cL48ePF/z9/YWioiJBEOx7Xh2JI+VTgsCcijmV7f9+Zk7lODmVI+VTgmC/OZUt5VMsSrVR0yN6rz6mT58uCIIgTJ8+XRg1alSL9yQmJgpyuVyIiooSVq1aZfW426OtfX377beFnj17Ci4uLoKfn58wevRoYcuWLeIE30at9ROAyVyNGjXK2Pcm33zzjdC7d29BLpcLffr0EdavX2/dwNuhPX195plnhO7duwtyuVxQKpXCbbfdJhw+fNj6wbfRX/7yFyEiIkKQy+VCQECAMG7cOGMyIQj2M6dN2tpfW51Xc65OKuxtfq90vb7a8txOmTJFCA4OFuRyuRAaGipMmTJFyM7ONr5uz/PqSBwpnxIE5lTMqaYbv7bV38/MqRwnp3KkfEoQ7DensqV8SiIIgtDx66+IiIiIiIiIiIjM455SRERERERERERkdSxKERERERERERGR1bEoRUREREREREREVseiFBERERERERERWR2LUkREREREREREZHUsShERERERERERkdWxKEVERERERERERFbHohQREREREREREVkdi1JERO0gkUjwww8/iB0GERERkU1jTkXk2FiUIiKb88gjj0AikbQ4brnlFrFDIyIiIrIZzKmISGxOYgdARNQet9xyC1atWmVyTqFQiBQNERERkW1iTkVEYuJKKSKySQqFAkFBQSaHr68vAMMy8MWLF+PWW2+Fq6sroqKisG7dOpP3Hz9+HGPHjoWrqyu6deuGmTNnoqqqyqTNypUr0adPHygUCgQHB2POnDkmr5eUlODOO++Em5sbevXqhZ9++qlzO01ERETUwZhTEZGYWJQiIrv0z3/+E3fffTeOHj2KqVOn4v7770dGRgYAQKPRYOLEifD19cWBAwewdu1abN682SRBWrx4MWbPno2ZM2fi+PHj+OmnnxAdHW3yPV577TXcd999OHbsGG677TZMnToVZWVlVu0nERERUWdiTkVEnUogIrIx06dPF2QymeDu7m5yvPHGG4IgCAIA4cknnzR5T3JysjBr1ixBEARh2bJlgq+vr1BVVWV8ff369YJUKhUKCwsFQRCEkJAQ4cUXXzQbAwDhpZdeMn5dVVUlABB+/fXXDusnERERUWdiTkVEYuOeUkRkk8aMGYPFixebnPPz8zP+PSUlxeS1lJQUpKenAwAyMjLQr18/uLu7G18fNmwY9Ho9srKyIJFIcOnSJYwbN+6aMSQkJBj/7u7uDi8vLxQVFbW3S0RERERWx5yKiMTEohQR2SR3d/cWS787iqurq0XtnJ2dTb6WSCTQ6/WdERIRERFRp2BORURi4p5SRGSX9u3b1+LruLg4AEBcXByOHj0KjUZjfH337t2QSqWIiYmBp6cnIiMjkZaWZtWYiYiIiLoa5lRE1Jm4UoqIbJJWq0VhYaHJOScnJ/j7+wMA1q5di4EDB2L48OH48ssvsX//fqxYsQIAMHXqVLzyyiuYPn06Xn31VRQXF+Ppp5/Gww8/DKVSCQB49dVX8eSTTyIwMBC33norKisrsXv3bjz99NPW7SgRERFRJ2JORURiYlGKiGzSxo0bERwcbHIuJiYGmZmZAAxPcVm9ejWeeuopBAcH4+uvv0Z8fDwAwM3NDb/99hvmzp2LQYMGwc3NDXfffTfef/9947WmT5+O2tpafPDBB/j73/8Of39/3HPPPdbrIBEREZEVMKciIjFJBEEQxA6CiKgjSSQSfP/995g8ebLYoRARERHZLOZURNTZuKcUERERERERERFZHYtSRERERERERERkdbx9j4iIiIiIiIiIrI4rpYiIiIiIiIiIyOpYlCIiIiIiIiIiIqtjUYqIiIiIiIiIiKyORSkiIiIiIiIiIrI6FqWIiIiIiIiIiMjqWJQiIiIiIiIiIiKrY1GKiIiIiIiIiIisjkUpIiIiIiIiIiKyOhaliIiIiIiIiIjI6v4fWAz5UU3ycyMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:17,  3.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:17,  3.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:17,  3.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:16,  3.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:16,  3.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:15,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:15,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:14,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:14,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:02<00:14,  3.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:03<00:13,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:13,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:03<00:12,  3.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:03<00:12,  3.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:12,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:04<00:12,  3.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:04<00:12,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:05<00:11,  3.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:05<00:11,  3.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:11,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:05<00:10,  3.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:06<00:10,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:06<00:10,  3.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:06<00:10,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:07<00:09,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:07<00:09,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:07<00:09,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:07<00:09,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:08<00:08,  3.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:08<00:08,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:08<00:08,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:09<00:07,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:09<00:07,  3.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:09<00:07,  3.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:09<00:06,  3.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:10<00:06,  3.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:10<00:06,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:10<00:06,  3.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:11<00:06,  3.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:11<00:05,  3.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:11<00:05,  3.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:11<00:04,  3.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:12<00:04,  3.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:12<00:04,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:12<00:04,  3.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:13<00:03,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:13<00:03,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:13<00:03,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:13<00:03,  3.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:14<00:02,  3.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:14<00:02,  3.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:14<00:02,  3.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:15<00:01,  3.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:15<00:01,  3.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:15<00:01,  3.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:15<00:01,  3.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:16<00:00,  3.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:16<00:00,  3.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:16<00:00,  3.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdefbc7ee1684b719212771b58a6ed94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dd07c8cceb40d589b55a83b5559188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss          : 0.6372\n",
      "Test Semantic Sim  : 0.3645\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 47 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "probable fracture with callus formation in right 2nd metatarsal shaft._x000D_\n",
      "no change of other findings since last study._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "probable fracture with callus formation in right 2nd metatarsal shaft. no change of other findings since last study. <|endoftext|>\n",
      "Generated Report :\n",
      " bony abnormality. \n",
      "\n",
      "--- Example 70 ---\n",
      "Raw Report       :\n",
      "[FINDING       ]_x000D_-_x000D__x000D_[CONCLUSION    ]_x000D_soft tissue swelling around Rt 1st IP joint \n",
      "accessory navicular bone with erosions at Rt \n",
      "-> r/o gout involvement \n",
      "\n",
      "mild degenerative change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   :\n",
      "- soft tissue swelling around Rt 1st IP joint accessory navicular bone with erosions at Rt -> r/o gout involvement mild degenerative change <|endoftext|>\n",
      "Generated Report :\n",
      "use osteopenia degenerative change \n",
      "\n",
      "--- Example 7 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "large sunchondral cyst, left navicular bone._x000D_\n",
      "bony protrusion in the anterolateral aspect of right calcaneus._x000D_\n",
      " -> r/o trauma-related malunion._x000D_\n",
      "probable old fracture with deformity, right 5th metatarsal bone._x000D_\n",
      "bipartitite sesamide, left 1st metatarsal bone._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "large sunchondral cyst, left navicular bone. bony protrusion in the anterolateral aspect of right calcaneus. -> r/o trauma-related malunion. probable old fracture with deformity, right 5th metatarsal bone. bipartitite sesamide, left 1st metatarsal bone. <|endoftext|>\n",
      "Generated Report :\n",
      "steophyte of right 1st MTP joint. \n",
      "\n",
      "--- Example 144 ---\n",
      "Raw Report       :\n",
      "[FINDING       ]_x000D_degenerative change_x000D__x000D_[CONCLUSION    ]_x000D_degenerative change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   :\n",
      "degenerative change <|endoftext|>\n",
      "Generated Report :\n",
      "enerative change \n",
      "\n",
      "--- Example 178 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "pes planus, both._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "pes planus, both. <|endoftext|>\n",
      "Generated Report :\n",
      "steophyte of right 1st MTP joint. \n",
      "\n",
      "--- Example 147 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "Erosive change in both 5th metatarsal head._x000D_\n",
      "  --> RA involvement._x000D_\n",
      "degenerative changes_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Erosive change in both 5th metatarsal head._x000D_\n",
      "  --> RA involvement._x000D_\n",
      "degenerative changes_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "Erosive change in both 5th metatarsal head. --> RA involvement. degenerative changes <|endoftext|>\n",
      "Generated Report :\n",
      "steophyte of right 1st MTP joint. \n",
      "\n",
      "--- Example 237 ---\n",
      "Raw Report       :\n",
      "[FINDING       ]_x000D_Degenerative changes_x000D__x000D_[CONCLUSION    ]_x000D_Degenerative changes_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   :\n",
      "Degenerative changes <|endoftext|>\n",
      "Generated Report :\n",
      "use osteopenia degenerative change \n",
      "\n",
      "--- Example 207 ---\n",
      "Raw Report       :\n",
      " 임시판독 결과 입니다 추후에 판독결과가 수정 될수 있으므로 확인 바랍니다._x000D_\n",
      "------------------------------------------------------------------------ _x000D_\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "enthesophyte in posterior calcaneal tuberosity, both._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      ". ------------------------------------------------------------------------ enthesophyte in posterior calcaneal tuberosity, both. <|endoftext|>\n",
      "Generated Report :\n",
      "steophyte of right 1st MTP joint. \n",
      "\n",
      "--- Example 168 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "mild Osteoarthritis in right 1st MTP joint._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "mild Osteoarthritis in right 1st MTP joint. <|endoftext|>\n",
      "Generated Report :\n",
      " bony abnormality. \n",
      "\n",
      "--- Example 179 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "no significant bony lesion_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no significant bony lesion_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "no significant bony lesion <|endoftext|>\n",
      "Generated Report :\n",
      "steophyte of right 1st MTP joint. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B, N, 768)\n",
    "\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute loss against ground truth\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # build an empty \"start\" prompt (just the eos_token) for generation\n",
    "            B = imgs.size(0)\n",
    "            start_token = tokenizer.eos_token_id\n",
    "            prompt = torch.full((B, 1), start_token, dtype=torch.long, device=device)\n",
    "\n",
    "            # re-encode for cross-attention\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "            p = pts.view(B * pts.size(1), pts.size(2), pts.size(3), pts.size(4))\n",
    "            pf = model._pool(model.patch_encoder(p))\n",
    "            pf = model.patch_proj(pf).view(B, pts.size(1), 768)\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            # generate without any reference tokens\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=comb,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                eos_token_id=start_token,\n",
    "                pad_token_id=start_token\n",
    "            )\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "# split into train / val / test\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "print(f\"\\nNumber of training samples:   {len(train_ds)}\")\n",
    "print(f\"Number of validation samples: {len(val_ds)}\")\n",
    "print(f\"Number of test samples:       {len(test_ds)}\")\n",
    "print(f\"Total samples:                {len(train_ds)+len(val_ds)+len(test_ds)}\\n\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model     = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler    = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "# final test—again only the eos token as prompt\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss          : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim  : {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw   = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi    = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa    = ex['patches'].unsqueeze(0).to(device)\n",
    "\n",
    "    # only a single eos token as decoder prompt\n",
    "    prompt = torch.tensor([[tokenizer.eos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    # encode image + patches\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "    cat, _ = model.attn(torch.cat([g, pf], dim=1),\n",
    "                        torch.cat([g, pf], dim=1),\n",
    "                        torch.cat([g, pf], dim=1))\n",
    "    comb = model.norm(cat)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=120,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       :\\n{raw}\")\n",
    "    print(f\"Cleaned Report   :\\n{clean}\")\n",
    "    print(f\"Generated Report :\\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches':   patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports':     [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        # global image encoder (Swin Transformer)\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        # patch encoder (ResNet50)\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        # cross-attention & layer norm\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        # GPT-2 decoder with cross-attention\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        # spatial average pooling if needed\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        # global encoding\n",
    "        g = self.global_encoder(imgs)                 # (B, feat)\n",
    "        g = self.global_proj(g).unsqueeze(1)          # (B, 1, 768)\n",
    "\n",
    "        # patch encoding\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)                           # (B*N, feat)\n",
    "        pf = self.patch_proj(pf)                      # (B*N, 768)\n",
    "        pf = pf.view(B, N, 768)                       # (B, N, 768)\n",
    "\n",
    "        # cross-attention input\n",
    "        cat, _ = self.attn(\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1)\n",
    "        )\n",
    "        cat = self.norm(cat)\n",
    "\n",
    "        # decode\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        # 1) compute teacher-forced loss\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "        # 2) prepare a pure start token for generation\n",
    "        B = imgs.size(0)\n",
    "        bos = torch.full((B, 1), tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "        bos_mask = torch.ones_like(bos)\n",
    "\n",
    "        # 3) re-encode image features & generate\n",
    "        with torch.no_grad():\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf = (model.patch_encoder.forward_features(p)\n",
    "                  if hasattr(model.patch_encoder, 'forward_features')\n",
    "                  else model.patch_encoder(p))\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "            cat, _ = model.attn(\n",
    "                torch.cat([g, pf], dim=1),\n",
    "                torch.cat([g, pf], dim=1),\n",
    "                torch.cat([g, pf], dim=1)\n",
    "            )\n",
    "            cat = model.norm(cat)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos,\n",
    "                attention_mask=bos_mask,\n",
    "                encoder_hidden_states=cat,\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # decode texts\n",
    "        gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "        gt_txt  = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "        all_gen.extend(gen_txt)\n",
    "        all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # config\n",
    "    class Cfg: pass\n",
    "    cfg = Cfg()\n",
    "    cfg.DATASET = Cfg()\n",
    "    cfg.DATASET.JSON           = 'final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.USE_RAW        = True\n",
    "    cfg.DATASET.USE_PATCH      = True\n",
    "    cfg.DATASET.REPORT         = True\n",
    "    cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE        = False\n",
    "    cfg.DATASET.AUGMENT        = False\n",
    "\n",
    "    # tokenizer & device\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # prepare dataset\n",
    "    dataset = FinalSamplesDataset(cfg)\n",
    "    dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "    dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "    print(\"\\nDataset class distribution:\")\n",
    "    for cls, cnt in dist.items():\n",
    "        print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "    # train/val/test split\n",
    "    n = len(dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "\n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "    print(f\"\\n# train: {len(train_ds)}, # val: {len(val_ds)}, # test: {len(test_ds)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # model, optimizer, scheduler, scaler\n",
    "    model = MultiModalModel().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # training loop\n",
    "    num_epochs = 10\n",
    "    train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "        val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "        sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        sems.append(sem)\n",
    "\n",
    "        print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "        print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "        print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # plot training metrics\n",
    "    plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "    # final evaluation on test set\n",
    "    test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "    test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "    print(\"\\n========== TEST RESULTS ==========\")\n",
    "    print(f\"Test Loss          : {test_loss:.4f}\")\n",
    "    print(f\"Test Semantic Sim  : {test_sem:.4f}\")\n",
    "\n",
    "    # random examples\n",
    "    print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "    for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "        ex = test_ds[idx]\n",
    "        raw   = ex['raw_report']\n",
    "        clean = ex['cleaned_report']\n",
    "\n",
    "        fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "        pa = ex['patches'].unsqueeze(0).to(device)\n",
    "\n",
    "        # re-encode features\n",
    "        g = model.global_encoder(fi)\n",
    "        g = model.global_proj(g).unsqueeze(1)\n",
    "        B, N, C, H, W = pa.shape\n",
    "        p = pa.view(B * N, C, H, W)\n",
    "        pf = (model.patch_encoder.forward_features(p)\n",
    "              if hasattr(model.patch_encoder, 'forward_features')\n",
    "              else model.patch_encoder(p))\n",
    "        pf = model._pool(pf)\n",
    "        pf = model.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "        cat, _ = model.attn(\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1)\n",
    "        )\n",
    "        cat = model.norm(cat)\n",
    "\n",
    "        # pure start token\n",
    "        bos = torch.full((1, 1), tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "        bos_mask = torch.ones_like(bos)\n",
    "\n",
    "        gen_ids = model.decoder.generate(\n",
    "            input_ids=bos,\n",
    "            attention_mask=bos_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            max_length=120,\n",
    "            do_sample=True,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            temperature=0.5,\n",
    "            repetition_penalty=1.3,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\n--- Example {idx} ---\")\n",
    "        print(f\"Raw Report       :\\n{raw}\")\n",
    "        print(f\"Cleaned Report   :\\n{clean}\")\n",
    "        print(f\"Generated Report :\\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n",
      "# train=1915, val=239, test=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29687/799046983.py:485: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_29687/799046983.py:351: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:34,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:33,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:33,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:32,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:30,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:28,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:15,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:15,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:14,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:11,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:07,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0011f06339448a69f78d4c712749b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a19cdc34e674066b2db1a895827e5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.3109\n",
      "  Validation Loss     : 0.8524\n",
      "  Semantic Similarity : 0.4177\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAU/NJREFUeJzt3Xl0Tff+//HXSSLzhMiAENQ8xKyqpkobQ80t9XVjplq0mmorX7NWczspSutet6betmiL63agBDVUqSGutmgpomSgrUSiEnL27w8/53tPkxBxsk/C87HWXit778/e5733idVPX/nsz7YYhmEIAAAAAAAAMJGLswsAAAAAAADA3YdQCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOjdnF2A2q9Wqs2fPys/PTxaLxdnlAACAEsQwDF28eFEVK1aUiwt/u7sR+lQAAKAghe1T3XWh1NmzZxUeHu7sMgAAQAl2+vRpVa5c2dlllGj0qQAAwM3crE9114VSfn5+kq7dGH9/fydXAwAASpKMjAyFh4fb+gsoGH0qAABQkML2qe66UOr68HJ/f386UAAAIF8l8XG0BQsW6LXXXlNKSooiIyP11ltvqWXLljc9bsWKFRowYIB69uyptWvX5ttm9OjR+tvf/qY333xT48ePL1Q99KkAAMDN3KxPxWQJAAAAJdzKlSsVGxuradOmaf/+/YqMjFR0dLTS0tJueNzJkyc1YcIEtW3btsA2a9as0TfffKOKFSs6umwAAIAbIpQCAAAo4WbPnq2RI0dq6NChqlevnhYuXChvb28tXry4wGNyc3M1cOBAzZgxQ9WrV8+3zZkzZzRu3Di9//77KlOmTHGVDwAAkC9CKQAAgBIsJydH+/btU1RUlG2bi4uLoqKitGvXrgKPmzlzpoKDgzV8+PB891utVsXExOi5555T/fr1b1pHdna2MjIy7BYAAIDbcdfNKQUAKD1yc3N15coVZ5eBO0iZMmXk6urq7DJuyfnz55Wbm6uQkBC77SEhITpy5Ei+x+zYsUPvvvuuEhMTCzzvK6+8Ijc3Nz311FOFqiM+Pl4zZswodN0AgGusVqtycnKcXQbgUI7qUxFKAQBKHMMwlJKSogsXLji7FNyBAgMDFRoaWiInM3eEixcvKiYmRosWLVJQUFC+bfbt26e5c+dq//79hb4PcXFxio2Nta1ff6sOAKBgOTk5OnHihKxWq7NLARzOEX0qQikAQIlzPZAKDg6Wt7f3HRsewFyGYejSpUu2ycHDwsKcXFHhBAUFydXVVampqXbbU1NTFRoamqf98ePHdfLkSXXv3t227fr/DLm5ueno0aPavn270tLSVKVKFVub3NxcPfvss5ozZ45OnjyZ57weHh7y8PBw0FUBwJ3PMAwlJyfL1dVV4eHhcnFh9hzcGRzZpyKUAgCUKLm5ubZAqnz58s4uB3cYLy8vSVJaWpqCg4NLxaN87u7uatasmRISEtSrVy9J10KmhIQEjR07Nk/7OnXq6NChQ3bbJk+erIsXL2ru3LkKDw9XTEyM3RxVkhQdHa2YmBgNHTq02K4FAO4mV69e1aVLl1SxYkV5e3s7uxzAoRzVpyKUAgCUKNfnkKLzhuJy/XfrypUrpSKUkqTY2FgNHjxYzZs3V8uWLTVnzhxlZWXZAqRBgwapUqVKio+Pl6enpxo0aGB3fGBgoCTZtpcvXz5P6FumTBmFhoaqdu3axX9BAHAXyM3NlXTtjwvAncgRfSpCKQBAicQjeygupfF3q3///jp37pymTp2qlJQUNW7cWOvXr7dNfp6UlMRjIQBQQpXG/+4AheGI321CKQAAgFJg7Nix+T6uJ0lbt2694bFLly696fnzm0cKAACgOPEnNQAASqiIiAjNmTPH2WUAAACUaidPnpTFYlFiYmKxfcb06dPVuHHj2zrHn+vcunWrLBaLQ95IbbFYtHbt2ts+j6MRSgEAcJssFssNl+nTpxfpvN9++61GjRp1W7V16NBB48ePv61zAACAu8e5c+f0xBNPqEqVKvLw8FBoaKiio6O1c+dOZ5dWKEOGDLG9GOS68PBwJScn55lz8VasWbNG9957rwICAuTn56f69evb9bEmTJighISEIp/fUXUWJDk5WV26dJFkTkhXWDy+BwDAbUpOTrb9vHLlSk2dOlVHjx61bfP19bX9bBiGcnNz5eZ28/8EV6hQwbGFAgAA3ETfvn2Vk5OjZcuWqXr16kpNTVVCQoJ+/fVXZ5dWZK6urgoNDS3y8QkJCerfv79mzZqlHj16yGKx6IcfftDGjRttbXx9fe36fM6oMz85OTlyd3d3+HkdhZFSAADcptDQUNsSEBAgi8ViWz9y5Ij8/Pz0xRdfqFmzZvLw8NCOHTt0/Phx9ezZUyEhIfL19VWLFi20adMmu/P++fE9i8Wif/zjH+rdu7e8vb1Vs2ZNrVu37rZq/+STT1S/fn15eHgoIiJCb7zxht3+t99+WzVr1pSnp6dCQkL0yCOP2PZ9/PHHatiwoby8vFS+fHlFRUUpKyvrtuoBAADOc+HCBW3fvl2vvPKKOnbsqKpVq6ply5aKi4tTjx497NqNGDFCFSpUkL+/vx544AEdPHjQtv/6o2yLFy9WlSpV5OvrqyeffFK5ubl69dVXFRoaquDgYM2aNcvu82fPnq2GDRvKx8dH4eHhevLJJ5WZmWnbv3TpUgUGBmrDhg2qW7eufH191blzZ9sfCKdPn65ly5bpX//6l23E+tatW/MdGfT999/r4Ycflr+/v/z8/NS2bVsdP3483/vy73//W23atNFzzz2n2rVrq1atWurVq5cWLFiQ55qvuz5i6+WXX1ZISIgCAwM1c+ZMXb16Vc8995zKlSunypUra8mSJbZjbjaC6ddff9WAAQNUqVIleXt7q2HDhvrwww/t2nTo0EFjx47V+PHjFRQUpOjoaEn2j+9Vq1ZNktSkSRNZLBZ16NBB27ZtU5kyZZSSkmJ3vvHjx6tt27b51uMIhFIAgBLNMAxdyrnqlMUwDIddx8SJE/XXv/5Vhw8fVqNGjZSZmamuXbsqISFBBw4cUOfOndW9e3clJSXd8DwzZsxQv3799J///Eddu3bVwIED9dtvvxWppn379qlfv3567LHHdOjQIU2fPl1TpkyxTYq9d+9ePfXUU5o5c6aOHj2q9evXq127dpKujQ4bMGCAhg0bpsOHD2vr1q3q06ePQ+8ZAAB3ktLQp7k+2mft2rXKzs4usN2jjz6qtLQ0ffHFF9q3b5+aNm2qTp062fVJjh8/ri+++ELr16/Xhx9+qHfffVfdunXTL7/8oq+++kqvvPKKJk+erN27d9uOcXFx0bx58/T9999r2bJl2rx5s55//nm7z7506ZJef/11vffee9q2bZuSkpI0YcIESdceoevXr58tqEpOTtZ9992Xp/4zZ86oXbt28vDw0ObNm7Vv3z4NGzZMV69ezfd6Q0ND9f333+u7774r1H28bvPmzTp79qy2bdum2bNna9q0aXr44YdVtmxZ7d69W6NHj9bjjz+uX375pVDnu3z5spo1a6bPPvtM3333nUaNGqWYmBjt2bPHrt2yZcvk7u6unTt3auHChXnOc739pk2blJycrNWrV6tdu3aqXr263nvvPVu7K1eu6P3339ewYcNu6bpvBY/vAQBKtD+u5Kre1A1O+ewfZkbL290x/6mcOXOmHnzwQdt6uXLlFBkZaVt/8cUXtWbNGq1bt67AN6xJ1/7qNmDAAEnSyy+/rHnz5mnPnj3q3LnzLdc0e/ZsderUSVOmTJEk1apVSz/88INee+01DRkyRElJSfLx8dHDDz8sPz8/Va1aVU2aNJF0LZS6evWq+vTpo6pVq0qSGjZseMs1AABwtygNfRo3NzctXbpUI0eO1MKFC9W0aVO1b99ejz32mBo1aiRJ2rFjh/bs2aO0tDR5eHhIkl5//XWtXbtWH3/8sW0+TKvVqsWLF8vPz0/16tVTx44ddfToUX3++edycXFR7dq19corr2jLli1q1aqVJNnN0RQREaGXXnpJo0eP1ttvv23bfuXKFS1cuFA1atSQdO3ttDNnzpR0LVTz8vJSdnb2DR9XW7BggQICArRixQqVKVNG0rV+UEHGjRun7du3q2HDhqpataruvfdePfTQQxo4cKDtHuSnXLlymjdvnu16X331VV26dEn/+7//K0mKi4vTX//6V+3YsUOPPfZYgee5rlKlSrYA7npdGzZs0KpVq9SyZUvb9po1a+rVV18t8DzXp4goX7683X0aPny4lixZoueee07StRFily9fVr9+/W5aW1ExUgoAABM0b97cbj0zM1MTJkxQ3bp1FRgYKF9fXx0+fPimI6WudwglycfHR/7+/kpLSytSTYcPH1abNm3strVp00Y//fSTcnNz9eCDD6pq1aqqXr26YmJi9P777+vSpUuSpMjISHXq1EkNGzbUo48+qkWLFun3338vUh0AAKDk6Nu3r86ePat169apc+fO2rp1q5o2bWobSX3w4EFlZmaqfPnytpFVvr6+OnHihN3jbxEREfLz87Oth4SEqF69enJxcbHb9t/9mE2bNqlTp06qVKmS/Pz8FBMTo19//dXW/5Akb29vWyAlSWFhYbfcF0pMTFTbtm1tgdTN+Pj46LPPPtOxY8c0efJk+fr66tlnn1XLli3tavuz+vXr57ne//4jnqurq8qXL1/o+nNzc/Xiiy+qYcOGKleunHx9fbVhw4Y8/cdmzZoV6nx/NmTIEB07dkzffPONpGuPS/br108+Pj5FOl9hMFIKAFCieZVx1Q8zo5322Y7y5/+YT5gwQRs3btTrr7+ue+65R15eXnrkkUeUk5Nzw/P8ufNksVhktVodVud/8/Pz0/79+7V161Z9+eWXmjp1qqZPn65vv/1WgYGB2rhxo77++mt9+eWXeuuttzRp0iTt3r3bNk8BAAD4P6WpT+Pp6akHH3xQDz74oKZMmaIRI0Zo2rRpGjJkiDIzMxUWFqatW7fmOS4wMND2c359lhv1Y06ePKmHH35YTzzxhGbNmqVy5cppx44dGj58uHJycuTt7V3geW91+gAvL69ban9djRo1VKNGDY0YMUKTJk1SrVq1tHLlSg0dOjTf9rd6D27mtdde09y5czVnzhzb3Fvjx4/P038saogUHBys7t27a8mSJapWrZq++OKLfL9nRyKUAgCUaBaLxWGP0JUkO3fu1JAhQ9S7d29J10ZOnTx50tQa6tatm+f1zjt37lStWrXk6nqt8+rm5qaoqChFRUVp2rRpCgwM1ObNm9WnTx9ZLBa1adNGbdq00dSpU1W1alWtWbNGsbGxpl4HAAClQWnu09SrV882SXbTpk2VkpIiNzc3RUREOOwz9u3bJ6vVqjfeeMM2umjVqlW3fB53d3fl5ubesE2jRo20bNkyXblypdCjpf4sIiJC3t7epr7kZefOnerZs6f+8pe/SLr2iOSPP/6oevXq3dJ53N3dJSnf+zRixAgNGDBAlStXVo0aNfKMqne00vkvAgCAUq5mzZpavXq1unfvLovFoilTphTbiKdz587leYtLWFiYnn32WbVo0UIvvvii+vfvr127dmn+/Pm2eRs+/fRT/fzzz2rXrp3Kli2rzz//XFarVbVr19bu3buVkJCghx56SMHBwdq9e7fOnTununXrFss1AACA4vfrr7/q0Ucf1bBhw9SoUSP5+flp7969evXVV9WzZ09JUlRUlFq3bq1evXrp1VdfVa1atXT27Fl99tln6t27d54pCwrrnnvu0ZUrV/TWW2+pe/fuBU7SfTMRERHasGGDjh49qvLlyysgICBPm7Fjx+qtt97SY489pri4OAUEBOibb75Ry5YtVbt27Tztp0+frkuXLqlr166qWrWqLly4oHnz5unKlSt2c4YWt5o1a+rjjz/W119/rbJly2r27NlKTU295VAqODhYXl5eWr9+vSpXrixPT0/bfYqOjpa/v79eeukl21xdxYk5pQAAcILZs2erbNmyuu+++9S9e3dFR0eradOmxfJZH3zwgZo0aWK3LFq0SE2bNtWqVau0YsUKNWjQQFOnTtXMmTM1ZMgQSdeG4K9evVoPPPCA6tatq4ULF+rDDz9U/fr15e/vr23btqlr166qVauWJk+erDfeeENdunQplmsAAADFz9fXV61atdKbb76pdu3aqUGDBpoyZYpGjhyp+fPnS7o24uvzzz9Xu3btNHToUNWqVUuPPfaYTp06pZCQkCJ/dmRkpGbPnq1XXnlFDRo00Pvvv6/4+PhbPs/IkSNVu3ZtNW/eXBUqVMgzKly6NsH35s2blZmZqfbt26tZs2ZatGhRgaOm2rdvr59//lmDBg1SnTp11KVLF6WkpOjLL7/MN8QqLpMnT1bTpk0VHR2tDh06KDQ0VL169brl87i5uWnevHn629/+pooVK9oCR+naGxCHDBmi3NxcDRo0yIHV589iOPHdzdu2bdNrr72mffv2KTk5WWvWrLnhDd2xY4deeOEFHTlyRJcuXVLVqlX1+OOP65lnnin0Z2ZkZCggIEDp6eny9/d3wFUAABzp8uXLOnHihKpVqyZPT09nl4M70I1+x+gnFB73CgBujD4NSqvhw4fr3LlzWrdu3Q3bOaJP5dTH97KyshQZGalhw4apT58+N23v4+OjsWPHqlGjRvLx8dGOHTv0+OOPy8fHx/baSQAAAAAAANya9PR0HTp0SB988MFNAylHcWoo1aVLl1sa5n/9kYPrIiIitHr1am3fvp1QCgAAAAAAoIh69uypPXv2aPTo0abNlVWqJzo/cOCAvv76a7300kvOLgUAAAAAAKDU2rp1q+mfWSpDqcqVK+vcuXO6evWqpk+frhEjRhTYNjs7W9nZ2bb1jIwMM0oEAAAAAADADZTKt+9t375de/fu1cKFCzVnzhx9+OGHBbaNj49XQECAbQkPDzexUgAAAADA3cyJ7xYDipUjfrdL5UipatWqSZIaNmyo1NRUTZ8+XQMGDMi3bVxcnGJjY23rGRkZBFMAAAAAgGLl6uoqScrJyZGXl5eTqwEc79KlS5KkMmXKFPkcpTKU+m9Wq9Xu8bw/8/DwkIeHh4kVAQAAAADudm5ubvL29ta5c+dUpkwZubiUygeVgDwMw9ClS5eUlpamwMBAWwBbFE4NpTIzM3Xs2DHb+okTJ5SYmKhy5cqpSpUqiouL05kzZ7R8+XJJ0oIFC1SlShXVqVNHkrRt2za9/vrreuqpp5xSPwAAAAAA+bFYLAoLC9OJEyd06tQpZ5cDOFxgYKBCQ0Nv6xxODaX27t2rjh072tavP2Y3ePBgLV26VMnJyUpKSrLtt1qtiouL04kTJ+Tm5qYaNWrolVde0eOPP2567QAAOFqHDh3UuHFjzZkzR5IUERGh8ePHa/z48QUeY7FYtGbNGvXq1eu2PttR5wEAAP/H3d1dNWvWVE5OjrNLARyqTJkytzVC6jqnhlIdOnS44cRYS5cutVsfN26cxo0bV8xVAQBwa7p3764rV65o/fr1efZt375d7dq108GDB9WoUaNbOu+3334rHx8fR5UpSZo+fbrWrl2rxMREu+3JyckqW7asQz/rz5YuXarx48frwoULxfo5AACUJC4uLvL09HR2GUCJxEOtAADcpuHDh2vjxo365Zdf8uxbsmSJmjdvfsuBlCRVqFBB3t7ejijxpkJDQ5mDEQAAAKYilAIA4DY9/PDDqlChQp4RvpmZmfroo480fPhw/frrrxowYIAqVaokb29vNWzYUB9++OENzxsREWF7lE+SfvrpJ7Vr106enp6qV6+eNm7cmOeYF154QbVq1ZK3t7eqV6+uKVOm6MqVK5KujVSaMWOGDh48KIvFIovFYqvZYrFo7dq1tvMcOnRIDzzwgLy8vFS+fHmNGjVKmZmZtv1DhgxRr1699PrrryssLEzly5fXmDFjbJ9VFElJSerZs6d8fX3l7++vfv36KTU11bb/4MGD6tixo/z8/OTv769mzZpp7969kqRTp06pe/fuKlu2rHx8fFS/fn19/vnnRa4FAAAAxa/Uv30PAABnc3Nz06BBg7R06VJNmjRJFotFkvTRRx8pNzdXAwYMUGZmppo1a6YXXnhB/v7++uyzzxQTE6MaNWqoZcuWN/0Mq9WqPn36KCQkRLt371Z6enq+c035+flp6dKlqlixog4dOqSRI0fKz89Pzz//vPr376/vvvtO69ev16ZNmyRJAQEBec6RlZWl6OhotW7dWt9++63S0tI0YsQIjR071i5427Jli8LCwrRlyxYdO3ZM/fv3V+PGjTVy5MhbvodWq9UWSH311Ve6evWqxowZo/79+2vr1q2SpIEDB6pJkyZ655135OrqqsTERNsriMeMGaOcnBxt27ZNPj4++uGHH+Tr63vLdQAAAMA8hFIAgJLNMKQrl5zz2WW8pf8fMN3MsGHD9Nprr+mrr75Shw4dJF17dK9v374KCAhQQECAJkyYYGs/btw4bdiwQatWrSpUKLVp0yYdOXJEGzZsUMWKFSVJL7/8srp06WLXbvLkybafIyIiNGHCBK1YsULPP/+8vLy85OvrKzc3txu+KeWDDz7Q5cuXtXz5ctucVvPnz1f37t31yiuvKCQkRJJUtmxZzZ8/X66urqpTp466deumhISEIoVSCQkJOnTokE6cOKHw8HBJ0vLly1W/fn19++23atGihZKSkvTcc8/Z3sJbs2ZN2/FJSUnq27evGjZsKEmqXr36LdcAAAAAcxFKAQBKtiuXpJcrOuez//es5F64icbr1Kmj++67T4sXL1aHDh107Ngxbd++XTNnzpQk5ebm6uWXX9aqVat05swZ5eTkKDs7u9BzRh0+fFjh4eG2QEqSWrdunafdypUrNW/ePB0/flyZmZm6evWq/P39C/UZ//1ZkZGRdpOst2nTRlarVUePHrWFUvXr17d760pYWJgOHTp0S5/1358ZHh5uC6QkqV69egoMDNThw4fVokULxcbGasSIEXrvvfcUFRWlRx99VDVq1JAkPfXUU3riiSf05ZdfKioqSn379i3SPF4AAAAwD3NKAQDgIMOHD9cnn3yiixcvasmSJapRo4bat28vSXrttdc0d+5cvfDCC9qyZYsSExMVHR3t0FdE79q1SwMHDlTXrl316aef6sCBA5o0aVKxvYb6+qNz11ksFlmt1mL5LOnamwO///57devWTZs3b1a9evW0Zs0aSdKIESP0888/KyYmRocOHVLz5s311ltvFVstAAAAuH2MlAIAlGxlvK+NWHLWZ9+Cfv366emnn9YHH3yg5cuX64knnrDNL7Vz50717NlTf/nLXyRdm0Ppxx9/VL169Qp17rp16+r06dNKTk5WWFiYJOmbb76xa/P111+ratWqmjRpkm3bqVOn7Nq4u7srNzf3pp+1dOlSZWVl2UZL7dy5Uy4uLqpdu3ah6r1V16/v9OnTttFSP/zwgy5cuGB3j2rVqqVatWrpmWee0YABA7RkyRL17t1bkhQeHq7Ro0dr9OjRiouL06JFizRu3LhiqRcAAAC3j1AKAFCyWSyFfoTO2Xx9fdW/f3/FxcUpIyNDQ4YMse2rWbOmPv74Y3399dcqW7asZs+erdTU1EKHUlFRUapVq5YGDx6s1157TRkZGXbh0/XPSEpK0ooVK9SiRQt99tlntpFE10VEROjEiRNKTExU5cqV5efnJw8PD7s2AwcO1LRp0zR48GBNnz5d586d07hx4xQTE2N7dK+ocnNzlZiYaLfNw8NDUVFRatiwoQYOHKg5c+bo6tWrevLJJ9W+fXs1b95cf/zxh5577jk98sgjqlatmn755Rd9++236tu3ryRp/Pjx6tKli2rVqqXff/9dW7ZsUd26dW+rVgAAABQvHt8DAMCBhg8frt9//13R0dF28z9NnjxZTZs2VXR0tDp06KDQ0FD16tWr0Od1cXHRmjVr9Mcff6hly5YaMWKEZs2aZdemR48eeuaZZzR27Fg1btxYX3/9taZMmWLXpm/fvurcubM6duyoChUq6MMPP8zzWd7e3tqwYYN+++03tWjRQo888og6deqk+fPn39rNyEdmZqaaNGlit3Tv3l0Wi0X/+te/VLZsWbVr105RUVGqXr26Vq5cKUlydXXVr7/+qkGDBqlWrVrq16+funTpohkzZki6FnaNGTNGdevWVefOnVWrVi29/fbbt10vAAAAio/FMAzD2UWYKSMjQwEBAUpPT7/liV8BAMXv8uXLOnHihKpVqyZPT09nl4M70I1+x+gnFB73CgAAFKSw/QRGSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAgBLpLns5LEzE7xYAAEDJQCgFAChRypQpI0m6dOmSkyvBner679b13zUAAAA4h5uzCwAA4L+5uroqMDBQaWlpkiRvb29ZLBYnV4U7gWEYunTpktLS0hQYGChXV1dnlwQAAHBXI5QCAJQ4oaGhkmQLpgBHCgwMtP2OAQAAwHkIpQAAJY7FYlFYWJiCg4N15coVZ5eDO0iZMmUYIQUAAFBCEEoBAEosV1dXAgQAAADgDsVE5wAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAlAILFixQRESEPD091apVK+3Zs6dQx61YsUIWi0W9evWy2z59+nTVqVNHPj4+Klu2rKKiorR79+5iqBwAACB/hFIAAAAl3MqVKxUbG6tp06Zp//79ioyMVHR0tNLS0m543MmTJzVhwgS1bds2z75atWpp/vz5OnTokHbs2KGIiAg99NBDOnfuXHFdBgAAgB2LYRiGs4swU0ZGhgICApSeni5/f39nlwMAAEqQktpPaNWqlVq0aKH58+dLkqxWq8LDwzVu3DhNnDgx32Nyc3PVrl07DRs2TNu3b9eFCxe0du3aAj/j+rVv2rRJnTp1umlNJfVeAQAA5ytsP4GRUgAAACVYTk6O9u3bp6ioKNs2FxcXRUVFadeuXQUeN3PmTAUHB2v48OGF+oy///3vCggIUGRkZL5tsrOzlZGRYbcAAADcDkIpAACAEuz8+fPKzc1VSEiI3faQkBClpKTke8yOHTv07rvvatGiRTc896effipfX195enrqzTff1MaNGxUUFJRv2/j4eAUEBNiW8PDwol0QAADA/0coBQAAcAe5ePGiYmJitGjRogIDpus6duyoxMREff311+rcubP69etX4DxVcXFxSk9Pty2nT58ujvIBAMBdxM3ZBQAAAKBgQUFBcnV1VWpqqt321NRUhYaG5ml//PhxnTx5Ut27d7dts1qtkiQ3NzcdPXpUNWrUkCT5+Pjonnvu0T333KN7771XNWvW1Lvvvqu4uLg85/Xw8JCHh4cjLw0AANzlGCkFAABQgrm7u6tZs2ZKSEiwbbNarUpISFDr1q3ztK9Tp44OHTqkxMRE29KjRw/bqKgbPXZntVqVnZ1dLNcBAADwZ4yUAgAAKOFiY2M1ePBgNW/eXC1bttScOXOUlZWloUOHSpIGDRqkSpUqKT4+Xp6enmrQoIHd8YGBgZJk256VlaVZs2apR48eCgsL0/nz57VgwQKdOXNGjz76qKnXBgAA7l6EUgAAACVc//79de7cOU2dOlUpKSlq3Lix1q9fb5v8PCkpSS4uhR8A7+rqqiNHjmjZsmU6f/68ypcvrxYtWmj79u2qX79+cV0GAACAHYthGIazizBTRkaGAgIClJ6eLn9/f2eXAwAAShD6CYXHvQIAAAUpbD+BOaUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKZzaii1bds2de/eXRUrVpTFYtHatWtv2H716tV68MEHVaFCBfn7+6t169basGGDOcUCAAAAAADAYZwaSmVlZSkyMlILFiwoVPtt27bpwQcf1Oeff659+/apY8eO6t69uw4cOFDMlQIAAAAAAMCR3Jz54V26dFGXLl0K3X7OnDl26y+//LL+9a9/6d///reaNGni4OoAAAAAAABQXJwaSt0uq9Wqixcvqly5cgW2yc7OVnZ2tm09IyPDjNIAAAAAAABwA6V6ovPXX39dmZmZ6tevX4Ft4uPjFRAQYFvCw8NNrBAAAAAAAAD5KbWh1AcffKAZM2Zo1apVCg4OLrBdXFyc0tPTbcvp06dNrBIAAAAAAAD5KZWP761YsUIjRozQRx99pKioqBu29fDwkIeHh0mVAQAAAAAAoDBK3UipDz/8UEOHDtWHH36obt26ObscAAAAAAAAFIFTR0plZmbq2LFjtvUTJ04oMTFR5cqVU5UqVRQXF6czZ85o+fLlkq49sjd48GDNnTtXrVq1UkpKiiTJy8tLAQEBTrkGAAAAAAAA3DqnjpTau3evmjRpoiZNmkiSYmNj1aRJE02dOlWSlJycrKSkJFv7v//977p69arGjBmjsLAw2/L00087pX4AAAAAAAAUjVNHSnXo0EGGYRS4f+nSpXbrW7duLd6CAAAAAAAAYIpSN6cUAAAAAAAASj9CKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAKAYbNmyxdklAAAAlGiEUgAAAMWgc+fOqlGjhl566SWdPn3a2eUAAACUOIRSAAAAxeDMmTMaO3asPv74Y1WvXl3R0dFatWqVcnJynF0aAABAiUAoBQAAUAyCgoL0zDPPKDExUbt371atWrX05JNPqmLFinrqqad08OBBZ5cIAADgVIRSAAAAxaxp06aKi4vT2LFjlZmZqcWLF6tZs2Zq27atvv/+e2eXBwAA4BSEUgAAAMXkypUr+vjjj9W1a1dVrVpVGzZs0Pz585Wamqpjx46patWqevTRRwt1rgULFigiIkKenp5q1aqV9uzZU6jjVqxYIYvFol69etnV9cILL6hhw4by8fFRxYoVNWjQIJ09e7YolwkAAFAkhFIAAADFYNy4cQoLC9Pjjz+uWrVq6cCBA9q1a5dGjBghHx8fRURE6PXXX9eRI0dueq6VK1cqNjZW06ZN0/79+xUZGano6GilpaXd8LiTJ09qwoQJatu2rd32S5cuaf/+/ZoyZYr279+v1atX6+jRo+rRo8dtXTMAAMCtsBiGYTi7CDNlZGQoICBA6enp8vf3d3Y5AACgBHFkP6FTp04aMWKE+vTpIw8Pj3zbXL16VTt37lT79u1veK5WrVqpRYsWmj9/viTJarUqPDxc48aN08SJE/M9Jjc3V+3atdOwYcO0fft2XbhwQWvXri3wM7799lu1bNlSp06dUpUqVW56ffSpAABAQQrbT2CkFAAAQDGYNm2aHn300TyB1NWrV7Vt2zZJkpub200DqZycHO3bt09RUVG2bS4uLoqKitKuXbsKPG7mzJkKDg7W8OHDC1Vvenq6LBaLAgMDC9UeAADgdrk5uwAAAIA7UceOHZWcnKzg4GC77enp6erYsaNyc3MLdZ7z588rNzdXISEhdttDQkIKfPRvx44devfdd5WYmFioz7h8+bJeeOEFDRgwoMC/ZmZnZys7O9u2npGRUahzAwAAFISRUgAAAMXAMAxZLJY823/99Vf5+PgU2+devHhRMTExWrRokYKCgm7a/sqVK+rXr58Mw9A777xTYLv4+HgFBATYlvDwcEeWDQAA7kJOHSm1bds2vfbaa9q3b5+Sk5O1Zs0auzfD/FlycrKeffZZ7d27V8eOHdNTTz2lOXPmmFYvAADAzfTp00eSZLFYNGTIELvH93Jzc/Wf//xH9913X6HPFxQUJFdXV6WmptptT01NVWhoaJ72x48f18mTJ9W9e3fbNqvVKuna44JHjx5VjRo1JP1fIHXq1Clt3rz5hnM+xMXFKTY21raekZFBMAUAAG6LU0dKZWVlKTIyUgsWLChU++zsbFWoUEGTJ09WZGRkMVcHAABw666PJDIMQ35+fnaji0JDQzVq1Cj985//LPT53N3d1axZMyUkJNi2Wa1WJSQkqHXr1nna16lTR4cOHVJiYqJt6dGjhzp27KjExERbkHQ9kPrpp5+0adMmlS9f/oZ1eHh4yN/f324BAAC4HU4dKdWlSxd16dKl0O0jIiI0d+5cSdLixYuLqywAAIAiW7JkiaRr/ZYJEyY45FG92NhYDR48WM2bN1fLli01Z84cZWVlaejQoZKkQYMGqVKlSoqPj5enp6caNGhgd/z1ycuvb79y5YoeeeQR7d+/X59++qlyc3OVkpIiSSpXrpzc3d1vu2YAAICbYaJzAACAYjBt2jSHnat///46d+6cpk6dqpSUFDVu3Fjr16+3TX6elJQkF5fCD4A/c+aM1q1bJ0lq3Lix3b4tW7aoQ4cOjiodAACgQHd8KMWbYgAAgFmaNm2qhIQElS1bVk2aNMl3ovPr9u/ff0vnHjt2rMaOHZvvvq1bt97w2KVLl9qtR0REyDCMW/p8AAAAR7vjQ6n4+HjNmDHD2WUAAIC7QM+ePW0Tm9/o5S0AAAC4C0Ip3hQDAADMcv2RvdzcXHXs2FGNGjWyzecEAAAAe3d8KOXh4WH3KmYAAIDi5urqqoceekiHDx8mlAIAAChAkUKp06dPy2KxqHLlypKkPXv26IMPPlC9evU0atSoQp8nMzNTx44ds62fOHFCiYmJKleunKpUqaK4uDidOXNGy5cvt7VJTEy0HXvu3DklJibK3d1d9erVK8qlAAAAFIsGDRro559/VrVq1ZxdCgAAQIlU+Ne0/Jf/+Z//0ZYtWyRJKSkpevDBB7Vnzx5NmjRJM2fOLPR59u7dqyZNmqhJkyaSrr3uuEmTJpo6daokKTk5WUlJSXbHXG+/b98+ffDBB2rSpIm6du1alMsAAAAoNi+99JImTJigTz/9VMnJycrIyLBbAAAA7nYWowivXilbtqy++eYb1a5dW/PmzdPKlSu1c+dOffnllxo9erR+/vnn4qjVITIyMhQQEKD09HT5+/s7uxwAAFCCOLKf4OLyf3/7+++38BmGIYvFotzc3Ns6v7PRpwIAAAUpbD+hSI/vXblyxTZP06ZNm9SjRw9JUp06dZScnFyUUwIAANxRro8qBwAAQP6KFErVr19fCxcuVLdu3bRx40a9+OKLkqSzZ8+qfPnyDi0QAACgNGrfvr2zSwAAACjRihRKvfLKK+rdu7dee+01DR48WJGRkZKkdevWqWXLlg4tEAAAoDS7dOmSkpKSlJOTY7e9UaNGTqoIAACgZChSKNWhQwedP39eGRkZKlu2rG37qFGj5O3t7bDiAAAASqtz585p6NCh+uKLL/LdX9rnlAIAALhdRXr73h9//KHs7GxbIHXq1CnNmTNHR48eVXBwsEMLBAAAKI3Gjx+vCxcuaPfu3fLy8tL69eu1bNky1axZU+vWrXN2eQAAAE5XpJFSPXv2VJ8+fTR69GhduHBBrVq1UpkyZXT+/HnNnj1bTzzxhKPrBAAAKFU2b96sf/3rX2revLlcXFxUtWpVPfjgg/L391d8fLy6devm7BIBAACcqkgjpfbv36+2bdtKkj7++GOFhITo1KlTWr58uebNm+fQAgEAAEqjrKws2wjysmXL6ty5c5Kkhg0bav/+/c4sDQAAoEQoUih16dIl+fn5SZK+/PJL9enTRy4uLrr33nt16tQphxYIAABQGtWuXVtHjx6VJEVGRupvf/ubzpw5o4ULFyosLMzJ1QEAADhfkUKpe+65R2vXrtXp06e1YcMGPfTQQ5KktLQ0+fv7O7RAAACA0ujpp59WcnKyJGnatGn64osvVKVKFc2bN08vv/yyk6sDAABwviLNKTV16lT9z//8j5555hk98MADat26taRro6aaNGni0AIBAABKo7/85S+2n5s1a6ZTp07pyJEjqlKlioKCgpxYGQAAQMlQpFDqkUce0f3336/k5GRFRkbatnfq1Em9e/d2WHEAAAB3Cm9vbzVt2tTZZQAAAJQYRQqlJCk0NFShoaH65ZdfJEmVK1dWy5YtHVYYAABAaRMbG1votrNnzy7GSgAAAEq+IoVSVqtVL730kt544w1lZmZKkvz8/PTss89q0qRJcnEp0lRVAAAApdqBAwcK1c5isRRzJQAAACVfkUKpSZMm6d1339Vf//pXtWnTRpK0Y8cOTZ8+XZcvX9asWbMcWiQAAEBpsGXLFmeXAAAAUGoUKZRatmyZ/vGPf6hHjx62bY0aNVKlSpX05JNPEkoBAAAAAADghooUSv3222+qU6dOnu116tTRb7/9dttFAQAAlEZ9+vTR0qVL5e/vrz59+tyw7erVq02qCgAAoGQq0uRPkZGRmj9/fp7t8+fPV6NGjW67KAAAgNIoICDANl9UQEDADRcAAIC7XZFGSr366qvq1q2bNm3apNatW0uSdu3apdOnT+vzzz93aIEAAAClxZIlS/L9GQAAAHkVaaRU+/bt9eOPP6p37966cOGCLly4oD59+uj777/Xe++95+gaAQAAAAAAcIexGIZhOOpkBw8eVNOmTZWbm+uoUzpcRkaGAgIClJ6eLn9/f2eXAwAAShBH9hN+/fVXTZ06VVu2bFFaWpqsVqvd/tI+Dyd9KgAAUJDC9hOK9PgeAAAAbiwmJkbHjh3T8OHDFRISYptrCgAAANcQSgEAABSD7du3a8eOHYqMjHR2KQAAACVSkeaUAgAAwI3VqVNHf/zxh7PLAAAAKLFuaaRUnz59brj/woULt1MLAADAHePtt9/WxIkTNXXqVDVo0EBlypSx2888TAAA4G53S6FUQEDATfcPGjTotgoCAAC4EwQGBiojI0MPPPCA3XbDMGSxWEr0i2EAAADMcEuh1JIlS4qrDgAAgDvKwIEDVaZMGX3wwQdMdA4AAJAPJjoHAAAoBt99950OHDig2rVrO7sUAACAEomJzgEAAIpB8+bNdfr0aWeXAQAAUGIxUgoAAKAYjBs3Tk8//bSee+45NWzYMM9E540aNXJSZQAAACUDoRQAAEAx6N+/vyRp2LBhtm0Wi4WJzgEAAP4/QikAAIBicOLECWeXAAAAUKIRSgEAABSDqlWrOrsEAACAEo1QCgAAwEHWrVunLl26qEyZMlq3bt0N2/bo0cOkqgAAAEomQikAAAAH6dWrl1JSUhQcHKxevXoV2I45pQAAAAilAAAAHMZqteb7MwAAAPJycXYBAAAAd5Jdu3bp008/tdu2fPlyVatWTcHBwRo1apSys7OdVB0AAEDJQSgFAADgQDNnztT3339vWz906JCGDx+uqKgoTZw4Uf/+978VHx/vxAoBAABKBkIpAAAAB0pMTFSnTp1s6ytWrFCrVq20aNEixcbGat68eVq1apUTKwQAACgZCKUAAAAc6Pfff1dISIht/auvvlKXLl1s6y1atNDp06edURoAAECJQigFAADgQCEhITpx4oQkKScnR/v379e9995r23/x4kWVKVPGWeUBAACUGIRSAAAADtS1a1dNnDhR27dvV1xcnLy9vdW2bVvb/v/85z+qUaOGEysEAAAoGdycXQAAAMCd5MUXX1SfPn3Uvn17+fr6atmyZXJ3d7ftX7x4sR566CEnVggAAFAyEEoBAAA4UFBQkLZt26b09HT5+vrK1dXVbv9HH30kX19fJ1UHAABQchBKAQAAFIOAgIB8t5crV87kSgAAAEom5pQCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDpCKQAAAAAAAJiOUAoAAAAAAACmI5QCAAAAAACA6QilAAAAAAAAYDqnhlLbtm1T9+7dVbFiRVksFq1du/amx2zdulVNmzaVh4eH7rnnHi1durTY6wQAAAAAAIBjOTWUysrKUmRkpBYsWFCo9idOnFC3bt3UsWNHJSYmavz48RoxYoQ2bNhQzJUCAAAAAADAkZwaSnXp0kUvvfSSevfuXaj2CxcuVLVq1fTGG2+obt26Gjt2rB555BG9+eabxVwpAACAcy1YsEARERHy9PRUq1attGfPnkIdt2LFClksFvXq1ctu++rVq/XQQw+pfPnyslgsSkxMdHzRAAAAN1Cq5pTatWuXoqKi7LZFR0dr165dBR6TnZ2tjIwMuwUAAKA0WblypWJjYzVt2jTt379fkZGRio6OVlpa2g2PO3nypCZMmKC2bdvm2ZeVlaX7779fr7zySnGVDQAAcEOlKpRKSUlRSEiI3baQkBBlZGTojz/+yPeY+Ph4BQQE2Jbw8HAzSgUAAHCY2bNna+TIkRo6dKjq1aunhQsXytvbW4sXLy7wmNzcXA0cOFAzZsxQ9erV8+yPiYnR1KlT8/zBDwAAwCylKpQqiri4OKWnp9uW06dPO7skAACAQsvJydG+ffvswiMXFxdFRUXdcLT4zJkzFRwcrOHDh5tRJgAAwC1zc3YBtyI0NFSpqal221JTU+Xv7y8vL698j/Hw8JCHh4cZ5QEAADjc+fPnlZubm+9o8SNHjuR7zI4dO/Tuu+86dJ6o7OxsZWdn29aZEgEAANyuUjVSqnXr1kpISLDbtnHjRrVu3dpJFQEAAJQsFy9eVExMjBYtWqSgoCCHnZcpEQAAgKM5daRUZmamjh07Zls/ceKEEhMTVa5cOVWpUkVxcXE6c+aMli9fLkkaPXq05s+fr+eff17Dhg3T5s2btWrVKn322WfOugQAAIBiFRQUJFdX13xHi4eGhuZpf/z4cZ08eVLdu3e3bbNarZIkNzc3HT16VDVq1LjlOuLi4hQbG2tbz8jIIJgCAAC3xamh1N69e9WxY0fb+vWOzuDBg7V06VIlJycrKSnJtr9atWr67LPP9Mwzz2ju3LmqXLmy/vGPfyg6Otr02gEAAMzg7u6uZs2aKSEhQb169ZJ0LWRKSEjQ2LFj87SvU6eODh06ZLdt8uTJunjxoubOnVvkIIkpEQAAgKM5NZTq0KGDDMMocP/SpUvzPebAgQPFWBUAAEDJEhsbq8GDB6t58+Zq2bKl5syZo6ysLA0dOlSSNGjQIFWqVEnx8fHy9PRUgwYN7I4PDAyUJLvtv/32m5KSknT27FlJ0tGjRyVdm8MzvxFYAAAAjlaqJjoHAAC4G/Xv31/nzp3T1KlTlZKSosaNG2v9+vW2yc+TkpLk4nJrU4WuW7fOFmpJ0mOPPSZJmjZtmqZPn+6w2gEAAApiMW40VOkOlJGRoYCAAKWnp8vf39/Z5QAAgBKEfkLhca8AAEBBCttPKFVv3wMAAAAAAMCdgVAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGC6EhFKLViwQBEREfL09FSrVq20Z8+eAtteuXJFM2fOVI0aNeTp6anIyEitX7/exGoBAAAAAABwu5weSq1cuVKxsbGaNm2a9u/fr8jISEVHRystLS3f9pMnT9bf/vY3vfXWW/rhhx80evRo9e7dWwcOHDC5cgAAAAAAABSVxTAMw5kFtGrVSi1atND8+fMlSVarVeHh4Ro3bpwmTpyYp33FihU1adIkjRkzxratb9++8vLy0j//+c+bfl5GRoYCAgKUnp4uf39/x10IAAAo9egnFB73CgAAFKSw/QSnjpTKycnRvn37FBUVZdvm4uKiqKgo7dq1K99jsrOz5enpabfNy8tLO3bsKLB9RkaG3QIAAAAAAADncmoodf78eeXm5iokJMRue0hIiFJSUvI9Jjo6WrNnz9ZPP/0kq9WqjRs3avXq1UpOTs63fXx8vAICAmxLeHi4w68DAAAAAAAAt8bpc0rdqrlz56pmzZqqU6eO3N3dNXbsWA0dOlQuLvlfSlxcnNLT023L6dOnTa4YAAAAAAAAf+bUUCooKEiurq5KTU21256amqrQ0NB8j6lQoYLWrl2rrKwsnTp1SkeOHJGvr6+qV6+eb3sPDw/5+/vbLQAAAAAAAHAup4ZS7u7uatasmRISEmzbrFarEhIS1Lp16xse6+npqUqVKunq1av65JNP1LNnz+IuFwAAAAAAAA7i5uwCYmNjNXjwYDVv3lwtW7bUnDlzlJWVpaFDh0qSBg0apEqVKik+Pl6StHv3bp05c0aNGzfWmTNnNH36dFmtVj3//PPOvAwAAAAAAADcAqeHUv3799e5c+c0depUpaSkqHHjxlq/fr1t8vOkpCS7+aIuX76syZMn6+eff5avr6+6du2q9957T4GBgU66AgAAAAAAANyqEjHR+dixY3Xq1CllZ2dr9+7datWqlW3f1q1btXTpUtt6+/bt9cMPP+jy5cs6f/68li9frooVKzqhagAAAPMsWLBAERER8vT0VKtWrbRnz55CHbdixQpZLBb16tXLbrthGJo6darCwsLk5eWlqKgo/fTTT8VQOQAAQP5KRCgFAACAgq1cuVKxsbGaNm2a9u/fr8jISEVHRystLe2Gx508eVITJkxQ27Zt8+x79dVXNW/ePC1cuFC7d++Wj4+PoqOjdfny5eK6DAAAADuEUgAAACXc7NmzNXLkSA0dOlT16tXTwoUL5e3trcWLFxd4TG5urgYOHKgZM2bkeUuxYRiaM2eOJk+erJ49e6pRo0Zavny5zp49q7Vr1xbz1QAAAFxDKAUAAFCC5eTkaN++fYqKirJtc3FxUVRUlHbt2lXgcTNnzlRwcLCGDx+eZ9+JEyeUkpJid86AgAC1atXqhucEAABwJKdPdA4AAICCnT9/Xrm5ubaXwFwXEhKiI0eO5HvMjh079O677yoxMTHf/SkpKbZz/Pmc1/f9WXZ2trKzs23rGRkZhb0EAACAfDFSCgAA4A5y8eJFxcTEaNGiRQoKCnLYeePj4xUQEGBbwsPDHXZuAABwd2KkFAAAQAkWFBQkV1dXpaam2m1PTU1VaGhonvbHjx/XyZMn1b17d9s2q9UqSXJzc9PRo0dtx6WmpiosLMzunI0bN863jri4OMXGxtrWMzIyCKYAAMBtYaQUAABACebu7q5mzZopISHBts1qtSohIUGtW7fO075OnTo6dOiQEhMTbUuPHj3UsWNHJSYmKjw8XNWqVVNoaKjdOTMyMrR79+58zylJHh4e8vf3t1sAAABuByOlAAAASrjY2FgNHjxYzZs3V8uWLTVnzhxlZWVp6NChkqRBgwapUqVKio+Pl6enpxo0aGB3fGBgoCTZbR8/frxeeukl1axZU9WqVdOUKVNUsWJF9erVy6zLAgAAdzlCKQAAgBKuf//+OnfunKZOnaqUlBQ1btxY69evt01UnpSUJBeXWxsA//zzzysrK0ujRo3ShQsXdP/992v9+vXy9PQsjksAAADIw2IYhuHsIsyUkZGhgIAApaenM+wcAADYoZ9QeNwrAABQkML2E5hTCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDp3JxdgNkMw5AkZWRkOLkSAABQ0lzvH1zvL6Bg9KkAAEBBCtunuutCqYsXL0qSwsPDnVwJAAAoqS5evKiAgABnl1Gi0acCAAA3c7M+lcW4y/4UaLVadfbsWfn5+clisTi7nBInIyND4eHhOn36tPz9/Z1dzl2D++483Hvn4d47D/e+YIZh6OLFi6pYsaJcXJjl4EboUxWMf2POw713Hu6983DvnYd7X7DC9qnuupFSLi4uqly5srPLKPH8/f35R+UE3Hfn4d47D/feebj3+WOEVOHQp7o5/o05D/feebj3zsO9dx7uff4K06fiT4AAAAAAAAAwHaEUAAAAAAAATEcoBTseHh6aNm2aPDw8nF3KXYX77jzce+fh3jsP9x4oXvwbcx7uvfNw752He+883Pvbd9dNdA4AAAAAAADnY6QUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFJ3uAULFigiIkKenp5q1aqV9uzZU2DbK1euaObMmapRo4Y8PT0VGRmp9evX52l35swZ/eUvf1H58uXl5eWlhg0bau/evcV5GaWSo+99bm6upkyZomrVqsnLy0s1atTQiy++KKaF+z/btm1T9+7dVbFiRVksFq1du/amx2zdulVNmzaVh4eH7rnnHi1dujRPm1v5Lu9WxXHv4+Pj1aJFC/n5+Sk4OFi9evXS0aNHi+cCSrHi+r2/7q9//assFovGjx/vsJqB0og+lfPQpzIffSrnoU/lPPSpnMTAHWvFihWGu7u7sXjxYuP77783Ro4caQQGBhqpqan5tn/++eeNihUrGp999plx/Phx4+233zY8PT2N/fv329r89ttvRtWqVY0hQ4YYu3fvNn7++Wdjw4YNxrFjx8y6rFKhOO79rFmzjPLlyxuffvqpceLECeOjjz4yfH19jblz55p1WSXe559/bkyaNMlYvXq1IclYs2bNDdv//PPPhre3txEbG2v88MMPxltvvWW4uroa69evt7W51e/yblUc9z46OtpYsmSJ8d133xmJiYlG165djSpVqhiZmZnFfDWlS3Hc++v27NljREREGI0aNTKefvrp4rkAoBSgT+U89Kmcgz6V89Cnch76VM5BKHUHa9mypTFmzBjbem5urlGxYkUjPj4+3/ZhYWHG/Pnz7bb16dPHGDhwoG39hRdeMO6///7iKfgOUhz3vlu3bsawYcNu2Ab/pzD/IXn++eeN+vXr223r37+/ER0dbVu/1e8Sjrv3f5aWlmZIMr766itHlHlHcuS9v3jxolGzZk1j48aNRvv27elA4a5Gn8p56FM5H30q56FP5Tz0qczD43t3qJycHO3bt09RUVG2bS4uLoqKitKuXbvyPSY7O1uenp5227y8vLRjxw7b+rp169S8eXM9+uijCg4OVpMmTbRo0aLiuYhSqrju/X333aeEhAT9+OOPkqSDBw9qx44d6tKlSzFcxd1h165ddt+TJEVHR9u+p6J8lyicm937/KSnp0uSypUrV6y13ekKe+/HjBmjbt265WkL3G3oUzkPfarSgz6V89Cnch76VI5BKHWHOn/+vHJzcxUSEmK3PSQkRCkpKfkeEx0drdmzZ+unn36S1WrVxo0btXr1aiUnJ9va/Pzzz3rnnXdUs2ZNbdiwQU888YSeeuopLVu2rFivpzQprns/ceJEPfbYY6pTp47KlCmjJk2aaPz48Ro4cGCxXs+dLCUlJd/vKSMjQ3/88UeRvksUzs3u/Z9ZrVaNHz9ebdq0UYMGDcwq845UmHu/YsUK7d+/X/Hx8c4oEShR6FM5D32q0oM+lfPQp3Ie+lSOQSgFm7lz56pmzZqqU6eO3N3dNXbsWA0dOlQuLv/3a2K1WtW0aVO9/PLLatKkiUaNGqWRI0dq4cKFTqy89CvMvV+1apXef/99ffDBB9q/f7+WLVum119/nc4r7gpjxozRd999pxUrVji7lDve6dOn9fTTT+v999/PM9oAQOHQp3Ie+lTAjdGnMg99qsIhlLpDBQUFydXVVampqXbbU1NTFRoamu8xFSpU0Nq1a5WVlaVTp07pyJEj8vX1VfXq1W1twsLCVK9ePbvj6tatq6SkJMdfRClVXPf+ueees/1lr2HDhoqJidEzzzxD6n4bQkND8/2e/P395eXlVaTvEoVzs3v/38aOHatPP/1UW7ZsUeXKlc0s8450s3u/b98+paWlqWnTpnJzc5Obm5u++uorzZs3T25ubsrNzXVS5YBz0KdyHvpUpQd9KuehT+U89Kkcg1DqDuXu7q5mzZopISHBts1qtSohIUGtW7e+4bGenp6qVKmSrl69qk8++UQ9e/a07WvTpk2e14f++OOPqlq1qmMvoBQrrnt/6dIlu7/ySZKrq6usVqtjL+Au0rp1a7vvSZI2btxo+55u57vEjd3s3kuSYRgaO3as1qxZo82bN6tatWpml3lHutm979Spkw4dOqTExETb0rx5cw0cOFCJiYlydXV1RtmA09Cnch76VKUHfSrnoU/lPPSpHMTZM62j+KxYscLw8PAwli5davzwww/GqFGjjMDAQCMlJcUwDMOIiYkxJk6caGv/zTffGJ988olx/PhxY9u2bcYDDzxgVKtWzfj9999tbfbs2WO4ubkZs2bNMn766Sfj/fffN7y9vY1//vOfZl9eiVYc937w4MFGpUqVbK8vXr16tREUFGQ8//zzZl9eiXXx4kXjwIEDxoEDBwxJxuzZs40DBw4Yp06dMgzDMCZOnGjExMTY2l9/jetzzz1nHD582FiwYEG+ry++0XeJa4rj3j/xxBNGQECAsXXrViM5Odm2XLp0yfTrK8mK497/GW+Kwd2OPpXz0KdyDvpUzkOfynnoUzkHodQd7q233jKqVKliuLu7Gy1btjS++eYb27727dsbgwcPtq1v3brVqFu3ruHh4WGUL1/eiImJMc6cOZPnnP/+97+NBg0aGB4eHkadOnWMv//972ZcSqnj6HufkZFhPP3000aVKlUMT09Po3r16sakSZOM7Oxssy6pxNuyZYshKc9y/V4PHjzYaN++fZ5jGjdubLi7uxvVq1c3lixZkue8N/oucU1x3Pv8zicp3+/oblZcv/f/jQ4UQJ/KmehTmY8+lfPQp3Ie+lTOYTEMw3D8+CsAAAAAAACgYMwpBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQBFYLFYtHbtWmeXAQAAUKrRpwLuboRSAEqdIUOGyGKx5Fk6d+7s7NIAAABKDfpUAJzNzdkFAEBRdO7cWUuWLLHb5uHh4aRqAAAASif6VACciZFSAEolDw8PhYaG2i1ly5aVdG0Y+DvvvKMuXbrIy8tL1atX18cff2x3/KFDh/TAAw/Iy8tL5cuX16hRo5SZmWnXZvHixapfv748PDwUFhamsWPH2u0/f/68evfuLW9vb9WsWVPr1q0r3osGAABwMPpUAJyJUArAHWnKlCnq27evDh48qIEDB+qxxx7T4cOHJUlZWVmKjo5W2bJl9e233+qjjz7Spk2b7DpI77zzjsaMGaNRo0bp0KFDWrdune655x67z5gxY4b69eun//znP+ratasGDhyo3377zdTrBAAAKE70qQAUKwMASpnBgwcbrq6uho+Pj90ya9YswzAMQ5IxevRou2NatWplPPHEE4ZhGMbf//53o2zZskZmZqZt/2effWa4uLgYKSkphmEYRsWKFY1JkyYVWIMkY/Lkybb1zMxMQ5LxxRdfOOw6AQAAihN9KgDOxpxSAEqljh076p133rHbVq5cOdvPrVu3ttvXunVrJSYmSpIOHz6syMhI+fj42Pa3adNGVqtVR48elcVi0dmzZ9WpU6cb1tCoUSPbzz4+PvL391daWlpRLwkAAMB09KkAOBOhFIBSycfHJ8/Qb0fx8vIqVLsyZcrYrVssFlmt1uIoCQAAoFjQpwLgTMwpBeCO9M033+RZr1u3riSpbt26OnjwoLKysmz7d+7cKRcXF9WuXVt+fn6KiIhQQkKCqTUDAACUNPSpABQnRkoBKJWys7OVkpJit83NzU1BQUGSpI8++kjNmzfX/fffr/fff1979uzRu+++K0kaOHCgpk2bpsGDB2v69Ok6d+6cxo0bp5iYGIWEhEiSpk+frtGjRys4OFhdunTRxYsXtXPnTo0bN87cCwUAAChG9KkAOBOhFIBSaf369QoLC7PbVrt2bR05ckTStbe4rFixQk8++aTCwsL04Ycfql69epIkb29vbdiwQU8//bRatGghb29v9e3bV7Nnz7ada/Dgwbp8+bLefPNNTZgwQUFBQXrkkUfMu0AAAAAT0KcC4EwWwzAMZxcBAI5ksVi0Zs0a9erVy9mlAAAAlFr0qQAUN+aUAgAAAAAAgOkIpQAAAAAAAGA6Ht8DAAAAAACA6RgpBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANP9P1l88pVJhfzSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:34,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:33,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:33,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:30,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:28,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:11,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901e430f929d4034b5e764b6e41a59be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d596873f7ad94a439f08340da73314ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss          : 0.9046\n",
      "Test Semantic Sim  : 0.4283\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 118 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "both accessory navicular bone, type II with OA_x000D_\n",
      "Lt. navicular bone, step off_x000D_\n",
      "  --> R/O fracture._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both accessory navicular bone, type II with OA_x000D_\n",
      "Lt. navicular bone, step off_x000D_\n",
      "  --> R/O fracture._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "both accessory navicular bone, type II with OA Lt. navicular bone, step off --> R/O fracture. <|endoftext|>\n",
      "Generated Report :\n",
      " 1st MTP joint, OA. ------------------------- No significant bony abnormality since last study . no definite correlation with RA involvement in both knee joints and foot (Et.) R/O gout arthritis -> possible accessory navicular bone erosion or degenerative change of left ankle type II Lt toe soft tissue swelling --> rheumatoid osteopenia hallux valgus , right 5th MT head - suggested interval progression to suspicious erosions at 3rd metatarsal base ------------------------------------------------------------------------ --- probable inflammatoryosis state periarticulare subchondral cysts\n",
      "\n",
      "--- Example 109 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "acrolysis, both feet, 1st-3rd distal phalanges._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "acrolysis, both feet, 1st-3rd distal phalanges._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "acrolysis, both feet, 1st-3rd distal phalanges. <|endoftext|>\n",
      "Generated Report :\n",
      "\n",
      "Copyright © 2018. All rights reserved, both feet and hands with degenerative change __________________ R/O gout arthritis rt 1st MTP joint -> RA involvement Lt 5th MT head --> OA inflammatory osteopenia LT foot > PIP ulnar cysts erosions at medial side of right 2nd metatarsal joints - possible multiplex enthesophyte in left 3rd IP base ------------------------------------------------------------------------ No bony abnormality or suspicious erosion since last study no significant interval alteration suggested to be excluded as periarticular calcanear area Degenerous os\n",
      "\n",
      "--- Example 79 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "No visible crystal deposition in both MTP joint._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "No visible crystal deposition in both MTP joint. <|endoftext|>\n",
      "Generated Report :\n",
      "t. 1st MTP joint, soft tissue swelling with suspicious erosion of medial portion and lateral malleolus R/O gout ______________________________________________________________ tophi in both left 5th MT head RA involvement suggested at Lt calcaneal spur -> probable os naviculare type II OA Probable Osnavicularis insertion site (PIP) - Possible PPE or pes planum ------------------------------------------------------------------------ hallux valgiforme degenerative change rheumatoid arthritis inflammatory reconditionation state suggestive of intermetatarsar tendonitis --> possible osteopenia fracture --- no\n",
      "\n",
      "--- Example 148 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "Degenerative change _x000D_\n",
      "Osteopenia_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Degenerative change _x000D_\n",
      "Osteopenia_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "Degenerative change Osteopenia <|endoftext|>\n",
      "Generated Report :\n",
      " 1st MTP joint soft tissue swelling.  (A) both knee joints, suspicious erosion of medial malleolus with RA involvement at Lt 2nd MT head -> R/O gouty arthritis in rt 5th to base2 degenerative change and type II interferonation hallux valgare Degenerally mild erosions on radiographs No significant interval progression since last study Osteopenia rec> posterior navicular bone thickening around left 3rd metatarsal bones , suggested - possible bony abnormality --- probable inflammatory reaction diffuse osteotomy fracture\n",
      "\n",
      "--- Example 15 ---\n",
      "Raw Report       :\n",
      "[FINDING       ]_x000D_-_x000D__x000D_[CONCLUSION    ]_x000D_periarticular osteopenia, both MTP joint \n",
      "soft tissue swelling around Lt 5th MTP joint with suspicious erosion \n",
      "-> r/o RA involvement _x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   :\n",
      "- periarticular osteopenia, both MTP joint soft tissue swelling around Lt 5th MTP joint with suspicious erosion -> r/o RA involvement <|endoftext|>\n",
      "Generated Report :\n",
      "t. knee, mild degenerative change ___________________________________________________ gout arthritis and erosion in both 1st MTP joint -> R/O RA involvement Lt ankle soft tissue swelling with suspicious erosions at left 2nd MT head level of right 5th toe foot ------------------------------------------------------------------------ no bony abnormality No significant interval progression since last study rheumatoid osteopenia periarticular bone type II - OA Probable subcortical inflammatory lesion on radiographs suggested os naviculare base fracture site or calcification area --- probable large fibula accessory malleolus\n",
      "\n",
      "--- Example 209 ---\n",
      "Raw Report       :\n",
      " 임시판독 결과 입니다 추후에 판독결과가 수정 될수 있으므로 확인 바랍니다._x000D_\n",
      "------------------------------------------------------------------------ _x000D_\n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      ". ------------------------------------------------------------------------ No bony abnormality No bony abnormality <|endoftext|>\n",
      "Generated Report :\n",
      " bony abnormality. __________________, both feet and ankles with soft tissue swelling of left 1st MTP joint -> no significant interval change since last study No definite correlation at all between calcification site erosion --> R/O gout arthritis in medial aspect ------------------------------------------------------------------------ erosions to right 5th MT head hallux valgus - Lt 2nd DIP joints periarticular osteopenia degenerative changes Degenerous cysts on radiographs --- probable accessory navicula type II os trigonum subluxation state (Rt.) suspiciously large lateral malle\n",
      "\n",
      "--- Example 194 ---\n",
      "Raw Report       :\n",
      "[FINDING       ]_x000D_-_x000D__x000D_[CONCLUSION    ]_x000D_tophi with bone erosions at both 1st, 2nd MTP joints, Lt 2nd IP joint, Lt 5th MTP joint \n",
      "possible tophi deposition around both medial, lateral malleoli \n",
      "-> tophaceous gout, multiple sites _x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   :\n",
      "- tophi with bone erosions at both 1st, 2nd MTP joints, Lt 2nd IP joint, Lt 5th MTP joint possible tophi deposition around both medial, lateral malleoli -> tophaceous gout, multiple sites <|endoftext|>\n",
      "Generated Report :\n",
      "/rt. knee, OA with erosion and calcification of medial malleolus joint both feet RA involvement Lt ankle soft tissue swelling in right 1st MTP area R TMT joints -> rheumatoid arthritis ____________________________________________________________ gouty degenerative change bony abnormality No significant interval progression since last study ------------------------------------------------------------------------ no definite state otherwise possible to consider other than suspicious erosions or old fracture site periarticular osteopenia - enthesophyte os naviculare Degenerable head space narrowing at left 2nd MT base --> type II inflammatory\n",
      "\n",
      "--- Example 4 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "Lt. knee loose body._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Lt. knee loose body._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "Lt. knee loose body. <|endoftext|>\n",
      "Generated Report :\n",
      "egenerative change. ___________________________________________ ------------------------------------------------------------------------ No bony abnormality since last study, no significant interval difference at both 5th and 1st MTP joint space narrowing with degeneration of medial portion to left 2nd MT head -> RA involvement in right lateral aspectal coalition hallux valgus Degenerate osteopenia - type II accessory navicular bone Rt ankle erosion --> OA Lt knee soft tissue swelling around posterior calcaneum (E) probable gout arthritis rather than suspiciously old fracture erosions on radiographs Diffuse plantar tendonitis r/\n",
      "\n",
      "--- Example 56 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "Rt. rotator cuff repair._x000D_\n",
      "both AC joint hypertrophy with subacromial spur._x000D_\n",
      "_x000D_\n",
      "both MTP joint space narrowing._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Rt. rotator cuff repair._x000D_\n",
      "both AC joint hypertrophy with subacromial spur._x000D_\n",
      "_x000D_\n",
      "both MTP joint space narrowing._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "Rt. rotator cuff repair. both AC joint hypertrophy with subacromial spur. both MTP joint space narrowing. <|endoftext|>\n",
      "Generated Report :\n",
      "t. ankle, OA with degenerative change _______________________________________________ no bony abnormality No significant interval changes since last study or rec> erosions at Lt 1st MTP joint -> R/O gout arthritis rheumatoid osteopenia type II both feet and wrist ------------------------------------------------------------------------ mild inflammatory cystic lesion in left 5th MT head --> RA involvement probable old fracture subchondralangeal bone erosion of right 2nd IP joints - possibility suspiciously soft tissue swelling around base level hallux valgus Degenerous plantar calcaneiform area on radi\n",
      "\n",
      "--- Example 172 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "minimal OA, both knee joints._x000D_\n",
      "Rt. knee soft tissue swelling and calcifications._x000D_\n",
      "_x000D_\n",
      "Rt. ulnar styloid process, bony erosions_x000D_\n",
      "  --> R/O RA_x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "minimal OA, both knee joints._x000D_\n",
      "Rt. knee soft tissue swelling and calcifications._x000D_\n",
      "_x000D_\n",
      "Rt. ulnar styloid process, bony erosions_x000D_\n",
      "  --> R/O RA_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "minimal OA, both knee joints. Rt. knee soft tissue swelling and calcifications. Rt. ulnar styloid process, bony erosions --> R/O RA <|endoftext|>\n",
      "Generated Report :\n",
      "t. 1st MTP joint, erosion and soft tissue swelling with suspicious erosions at both feet (R/O gout arthritis). Lt knee OA  (E) degenerative change of left 5th MT head -> RK ankle RA involvement --- probable bony abnormality Degeneral cysts in medial aspect os trigonum --> PIP interval progression to subcortical osteopenia rheumatoid calcaneus on radiographs No significant lesion or other signifciance ------------------------------------------------------------------------ no definite correlation between clinical state & type II fracture site Os nav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        # 1) normalize\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "\n",
    "        # 2) replace Windows line‐break markers early, so that “---- _x000D_” turns into “----  ”\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "\n",
    "        # 3) remove any lines that consist only of dashes + spaces\n",
    "        text = re.sub(r'(?m)^\\s*-+\\s*$', '', text)\n",
    "\n",
    "        # 4) strip out non-ASCII\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        # 5) collapse repeated punctuation\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "\n",
    "        # 6) normalize your section tags\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "\n",
    "        # 7) drop everything after “[Recommend]”\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "\n",
    "        # 8) if FINDING == CONCLUSION, drop the duplicated block\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "        # 9) strip out the remaining tag markers\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "        # 10) collapse all whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # 11) **new**: remove any leading dashes that survived (e.g. \"---- …\")\n",
    "        text = re.sub(r'^-+\\s*', '', text)\n",
    "\n",
    "        # 12) append EOS\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs':       imgs,\n",
    "        'patches':         patches,\n",
    "        'input_ids':       ids,\n",
    "        'attention_mask':  masks,\n",
    "        'raw_reports':     [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2,3]) if feats.ndim>2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        # global\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        # patches\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B*N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "        # cross-attention\n",
    "        cat, _ = self.attn(torch.cat([g,pf], dim=1),\n",
    "                           torch.cat([g,pf], dim=1),\n",
    "                           torch.cat([g,pf], dim=1))\n",
    "        cat = self.norm(cat)\n",
    "\n",
    "        # decode\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        # compute loss with teacher forcing\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "        # prepare pure BOS tokens\n",
    "        B = imgs.size(0)\n",
    "        bos = torch.full((B, 1), tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "        bos_mask = torch.ones_like(bos)\n",
    "\n",
    "        # re-encode image features & generate\n",
    "        with torch.no_grad():\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B*N, C, H, W)\n",
    "            pf = (model.patch_encoder.forward_features(p)\n",
    "                  if hasattr(model.patch_encoder, 'forward_features')\n",
    "                  else model.patch_encoder(p))\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "            cat, _ = model.attn(torch.cat([g,pf], dim=1),\n",
    "                                torch.cat([g,pf], dim=1),\n",
    "                                torch.cat([g,pf], dim=1))\n",
    "            cat = model.norm(cat)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos,\n",
    "                attention_mask=bos_mask,\n",
    "                encoder_hidden_states=cat,\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "        gt_txt  = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "        all_gen.extend(gen_txt)\n",
    "        all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Similarity\"); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON           = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW        = True\n",
    "cfg.DATASET.USE_PATCH      = True\n",
    "cfg.DATASET.REPORT         = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "cfg.DATASET.BALANCE        = False\n",
    "cfg.DATASET.AUGMENT        = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "n       = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val   = int(0.1 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "print(f\"# train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model     = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler    = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss          : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim  : {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw   = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "\n",
    "    # re-encode image features\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B*N, C, H, W)\n",
    "    pf = (model.patch_encoder.forward_features(p)\n",
    "          if hasattr(model.patch_encoder, 'forward_features')\n",
    "          else model.patch_encoder(p))\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "    cat, _ = model.attn(torch.cat([g,pf], dim=1),\n",
    "                        torch.cat([g,pf], dim=1),\n",
    "                        torch.cat([g,pf], dim=1))\n",
    "    cat = model.norm(cat)\n",
    "\n",
    "    # pure BOS token\n",
    "    bos = torch.full((1, 1), tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "    bos_mask = torch.ones_like(bos)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=bos,\n",
    "        attention_mask=bos_mask,\n",
    "        encoder_hidden_states=cat,\n",
    "        max_length=120,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       :\\n{raw}\")\n",
    "    print(f\"Cleaned Report   :\\n{clean}\")\n",
    "    print(f\"Generated Report :\\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f16e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n",
      "# train=1915, val=239, test=240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29687/890416701.py:495: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_29687/890416701.py:354: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:33,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:31,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:30,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:30,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:25,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:23,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:21,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:19,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:19,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:18,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:15,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:15,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:11,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69df1a4638b94099b1cb3a2bf38d0503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87476a7e85e4bf0bd201a6ae4f6b020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.3263\n",
      "  Validation Loss     : 0.7924\n",
      "  Semantic Similarity : 0.3890\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKYAAAHpCAYAAAC4KDT4AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaZVJREFUeJzt3XlcVmX+//H3DcimcCMuLIqiuKCpoKJki2JiuIxpWlGZC7mMuYxKVjKu2UKrmfuM31yyTLNRc9I0I00t0tJIKyUzDS1AzZE7MEHh/P7w5z3dA6gocEBfz8fjPB7c132dcz7nQA+v3vd1rttiGIYhAAAAAAAAoJw5mV0AAAAAAAAAbk4EUwAAAAAAADAFwRQAAAAAAABMQTAFAAAAAAAAUxBMAQAAAAAAwBQEUwAAAAAAADAFwRQAAAAAAABMQTAFAAAAAAAAUxBMAQAAAAAAwBQuZhdQ3goKCvTrr7/Ky8tLFovF7HIAAICJDMPQ77//rsDAQDk58Xnd9WKcBQAApBKOsYybzLFjxwxJbGxsbGxsbGz27dixY2YPURzMnTvXqF+/vuHm5ma0b9/e2LVr11Xt98477xiSjN69ezu0/+tf/zK6du1q+Pr6GpKMr7/+utC+f/zxhzFy5EjD19fXqFq1qtG3b18jIyOjRHUzzmJjY2NjY2P783Y1Y6ybbsaUl5eXJOnYsWPy9vY2uRoAAGAmm82moKAg+/igIli1apXi4+O1cOFCRUZGatasWYqJiVFqaqpq165d7H5Hjx7VhAkTdOeddxZ6LycnR3fccYceeOABDRs2rMj9x48frw0bNmj16tWyWq0aPXq0+vbtq88+++yqa2ecBQAApJKNsSyGYRjlUFOFYbPZZLValZWVxYAJAICbXEUcF0RGRqpdu3aaO3eupIuPxwUFBWnMmDGaOHFikfvk5+erY8eOevTRR7Vjxw6dOXNG69atK9Tv6NGjatCggb7++muFh4fb27OyslSrVi2tWLFC9913nyTp4MGDatasmZKTk3XrrbdeVe0V8X4CAIDyV5IxAYspAAAAVBB5eXnas2ePoqOj7W1OTk6Kjo5WcnJysfvNmDFDtWvX1pAhQ67pvHv27NH58+cdzhsaGqp69epd9ry5ubmy2WwOGwAAQEkQTAEAAFQQp06dUn5+vvz8/Bza/fz8lJGRUeQ+O3fu1BtvvKFFixZd83kzMjLk6uoqHx+fqz6vJCUmJspqtdq3oKCga64BAADcnAimAAAAKqnff/9dAwYM0KJFi1SzZs1yP39CQoKysrLs27Fjx8q9BgAAULnddIufAwAqpvz8fJ0/f97sMnCDqVKlipydnc0u46rVrFlTzs7OyszMdGjPzMyUv79/of6HDx/W0aNH1atXL3tbQUGBJMnFxUWpqakKCQm54nn9/f2Vl5enM2fOOMyaKu68l7i5ucnNze2KxweAm1lBQYHy8vLMLgMoVaU5xiKYAgCYyjAMZWRk6MyZM2aXghuUj4+P/P39ZbFYzC7lilxdXdW2bVslJSWpT58+ki7+D01SUpJGjx5dqH9oaKj279/v0DZ58mT9/vvvev3116/60bq2bduqSpUqSkpKUr9+/SRJqampSktLU4cOHa7vogDgJpaXl6cjR47YPzQAbiSlNcYimAIAmOpSKFW7dm15enpWivAAlYNhGDp79qxOnDghSQoICDC5oqsTHx+vQYMGKSIiQu3bt9esWbOUk5OjuLg4SdLAgQNVp04dJSYmyt3dXS1atHDY/9KMpz+3nz59Wmlpafr1118lXQydpIszpfz9/WW1WjVkyBDFx8fL19dX3t7eGjNmjDp06HDV38gHAHBkGIbS09Pl7OysoKAgOTmxkg5uDKU9xiKYAgCYJj8/3x5K1ahRw+xycAPy8PCQJJ04cUK1a9euFI/1xcbG6uTJk5o6daoyMjIUHh6uTZs22RdET0tLK/H/3Kxfv94ebEnSgw8+KEmaNm2apk+fLkl67bXX5OTkpH79+ik3N1cxMTGaP39+6VwUANyELly4oLNnzyowMFCenp5mlwOUqtIcY1kMwzBKq7DKwGazyWq1KisrS97e3maXAwA3tXPnzunIkSMKDg62/+MGlLY//vhDR48eVYMGDeTu7u7wHuOC0sX9BID/YpyDG11pjbGYSwgAMB2P76Es8fcFADAT/w7hRlVaf9sEUwAAAAAAADAFwRQAABVEcHCwZs2aZXYZAAAAN5SjR4/KYrEoJSWlzM4xffp0hYeHX9cx/rfObdu2yWKxlMq3V1ssFq1bt+66j1MWCKYAACghi8Vy2e3SYtIl9eWXX2r48OHXVVtUVJTGjRt3XccAAAA3r5MnT+qxxx5TvXr15ObmJn9/f8XExOizzz4zu7SrMnjwYPXp08ehLSgoSOnp6YW+ybYk1q5dq1tvvVVWq1VeXl665ZZbHMZcEyZMUFJS0jUfv7TqLE56erq6d+8uqXyCupLgW/kAACih9PR0+8+rVq3S1KlTlZqaam+rVq2a/WfDMJSfny8Xlyv/k1urVq3SLRQAAKCE+vXrp7y8PC1btkwNGzZUZmamkpKS9Ntvv5ld2jVzdnaWv7//Ne+flJSk2NhYPffcc7rnnntksVj0/fffa8uWLfY+1apVcxgDmlFnUfLy8uTq6lrqxy1NzJgCAKCE/P397ZvVapXFYrG/PnjwoLy8vPThhx+qbdu2cnNz086dO3X48GH17t1bfn5+qlatmtq1a6ePP/7Y4bj/+yifxWLR//3f/+nee++Vp6enGjdurPXr119X7f/61790yy23yM3NTcHBwXr11Vcd3p8/f74aN24sd3d3+fn56b777rO/995776lly5by8PBQjRo1FB0drZycnOuqBwAAVBxnzpzRjh079OKLL6pz586qX7++2rdvr4SEBN1zzz0O/YYOHapatWrJ29tbd911l7755hv7+5cea1u8eLHq1aunatWqaeTIkcrPz9dLL70kf39/1a5dW88995zD+WfOnKmWLVuqatWqCgoK0siRI5WdnW1/f+nSpfLx8dHmzZvVrFkzVatWTd26dbN/aDh9+nQtW7ZM77//vn0m+7Zt24qcIfTdd9/pL3/5i7y9veXl5aU777xThw8fLvK+/Pvf/9btt9+uJ554Qk2bNlWTJk3Up08fzZs3r9A1X3Jp5tbzzz8vPz8/+fj4aMaMGbpw4YKeeOIJ+fr6qm7dulqyZIl9nyvNZPrtt9/00EMPqU6dOvL09FTLli31zjvvOPSJiorS6NGjNW7cONWsWVMxMTGSHB/la9CggSSpdevWslgsioqK0vbt21WlShVlZGQ4HG/cuHG68847i6yntBBMAQAqFMMwdDbvgimbYRildh0TJ07UCy+8oAMHDqhVq1bKzs5Wjx49lJSUpK+//lrdunVTr169lJaWdtnjPP3003rggQe0b98+9ejRQ/3799fp06evqaY9e/bogQce0IMPPqj9+/dr+vTpmjJlipYuXSpJ+uqrr/S3v/1NM2bMUGpqqjZt2qSOHTtKujhL7KGHHtKjjz6qAwcOaNu2berbt2+p3jMAAG5klWGMc2nWz7p165Sbm1tsv/vvv18nTpzQhx9+qD179qhNmzbq0qWLwxjl8OHD+vDDD7Vp0ya98847euONN9SzZ08dP35cn376qV588UVNnjxZu3btsu/j5OSk2bNn67vvvtOyZcv0ySef6Mknn3Q499mzZ/XKK69o+fLl2r59u9LS0jRhwgRJFx+ne+CBB+xhVXp6um677bZC9f/yyy/q2LGj3Nzc9Mknn2jPnj169NFHdeHChSKv19/fX999952+/fbbq7qPl3zyySf69ddftX37ds2cOVPTpk3TX/7yF1WvXl27du3SiBEj9Ne//lXHjx+/quOdO3dObdu21YYNG/Ttt99q+PDhGjBggHbv3u3Qb9myZXJ1ddVnn32mhQsXFjrOpf4ff/yx0tPTtWbNGnXs2FENGzbU8uXL7f3Onz+vt99+W48++miJrrukeJQPAFCh/HE+X82nbjbl3N/PiJGna+n80zhjxgx17drV/trX11dhYWH2188884zWrl2r9evXa/To0cUeZ/DgwXrooYckSc8//7xmz56t3bt3q1u3biWuaebMmerSpYumTJkiSWrSpIm+//57vfzyyxo8eLDS0tJUtWpV/eUvf5GXl5fq16+v1q1bS7oYTF24cEF9+/ZV/fr1JUktW7YscQ0AANysKsMYx8XFRUuXLtWwYcO0cOFCtWnTRp06ddKDDz6oVq1aSZJ27typ3bt368SJE3Jzc5MkvfLKK1q3bp3ee+89+3qZBQUFWrx4sby8vNS8eXN17txZqamp2rhxo5ycnNS0aVO9+OKL2rp1qyIjIyXJYc2m4OBgPfvssxoxYoTmz59vbz9//rwWLlyokJAQSdLo0aM1Y8YMSReDNQ8PD+Xm5l720bV58+bJarVq5cqVqlKliqSL46LijBkzRjt27FDLli1Vv3593Xrrrbr77rvVv39/+z0oiq+vr2bPnm2/3pdeeklnz57V3//+d0lSQkKCXnjhBe3cuVMPPvhgsce5pE6dOvYQ7lJdmzdv1rvvvqv27dvb2xs3bqyXXnqp2ONcWj6iRo0aDvdpyJAhWrJkiZ544glJF2eKnTt3Tg888MAVa7sezJgCAKAMREREOLzOzs7WhAkT1KxZM/n4+KhatWo6cODAFWdMXRoESlLVqlXl7e2tEydOXFNNBw4c0O233+7Qdvvtt+vQoUPKz89X165dVb9+fTVs2FADBgzQ22+/rbNnz0qSwsLC1KVLF7Vs2VL333+/Fi1apP/85z/XVAcAAKi4+vXrp19//VXr169Xt27dtG3bNrVp08Y+w/qbb75Rdna2atSoYZ9hVa1aNR05csThUbjg4GB5eXnZX/v5+al58+ZycnJyaPvzuObjjz9Wly5dVKdOHXl5eWnAgAH67bff7OMRSfL09LSHUpIUEBBQ4rFRSkqK7rzzTnsodSVVq1bVhg0b9OOPP2ry5MmqVq2aHn/8cbVv396htv91yy23FLreP3+w5+zsrBo1alx1/fn5+XrmmWfUsmVL+fr6qlq1atq8eXOh8WTbtm2v6nj/a/Dgwfrxxx/1xRdfSLr46OQDDzygqlWrXtPxrhYzpgAAFYpHFWd9PyPGtHOXlv/9B3zChAnasmWLXnnlFTVq1EgeHh667777lJeXd9nj/O+AyWKxqKCgoNTq/DMvLy/t3btX27Zt00cffaSpU6dq+vTp+vLLL+Xj46MtW7bo888/10cffaQ5c+Zo0qRJ2rVrl32dAgAAULzKNMZxd3dX165d1bVrV02ZMkVDhw7VtGnTNHjwYGVnZysgIEDbtm0rtJ+Pj4/956LGMJcb1xw9elR/+ctf9Nhjj+m5556Tr6+vdu7cqSFDhigvL0+enp7FHrekSwt4eHiUqP8lISEhCgkJ0dChQzVp0iQ1adJEq1atUlxcXJH9S3oPruTll1/W66+/rlmzZtnX4ho3blyh8eS1Bkm1a9dWr169tGTJEjVo0EAffvhhkb/n0kYwBQCoUCwWS6k9TleRfPbZZxo8eLDuvfdeSRdnUB09erRca2jWrFmhr3r+7LPP1KRJEzk7Xxywuri4KDo6WtHR0Zo2bZp8fHz0ySefqG/fvrJYLLr99tt1++23a+rUqapfv77Wrl2r+Pj4cr0OAAAqo8o8xmnevLl94ew2bdooIyNDLi4uCg4OLrVz7NmzRwUFBXr11Vfts4zefffdEh/H1dVV+fn5l+3TqlUrLVu2TOfPn7/qWVP/Kzg4WJ6enuX6RTCfffaZevfurUceeUTSxcclf/jhBzVv3rxEx3F1dZWkIu/T0KFD9dBDD6lu3boKCQkpNNu+LFTO/yoAAKhkGjdurDVr1qhXr16yWCyaMmVKmc18OnnyZKFvcwkICNDjjz+udu3a6ZlnnlFsbKySk5M1d+5c+7oNH3zwgX766Sd17NhR1atX18aNG1VQUKCmTZtq165dSkpK0t13363atWtr165dOnnypJo1a1Ym1wAAAMrfb7/9pvvvv1+PPvqoWrVqJS8vL3311Vd66aWX1Lt3b0lSdHS0OnTooD59+uill15SkyZN9Ouvv2rDhg269957Cy1ncLUaNWqk8+fPa86cOerVq1exC3dfSXBwsDZv3qzU1FTVqFFDVqu1UJ/Ro0drzpw5evDBB5WQkCCr1aovvvhC7du3V9OmTQv1nz59us6ePasePXqofv36OnPmjGbPnq3z5887rCla1ho3bqz33ntPn3/+uapXr66ZM2cqMzOzxMFU7dq15eHhoU2bNqlu3bpyd3e336eYmBh5e3vr2Wefta/dVdZYYwoAgHIwc+ZMVa9eXbfddpt69eqlmJgYtWnTpkzOtWLFCrVu3dphW7Rokdq0aaN3331XK1euVIsWLTR16lTNmDFDgwcPlnRx+v2aNWt01113qVmzZlq4cKHeeecd3XLLLfL29tb27dvVo0cPNWnSRJMnT9arr76q7t27l8k1AACA8letWjVFRkbqtddeU8eOHdWiRQtNmTJFw4YN09y5cyVdnPm1ceNGdezYUXFxcWrSpIkefPBB/fzzz/Lz87vmc4eFhWnmzJl68cUX1aJFC7399ttKTEws8XGGDRumpk2bKiIiQrVq1So0W1y6uOj3J598ouzsbHXq1Elt27bVokWLip091alTJ/30008aOHCgQkND1b17d2VkZOijjz4qMsgqK5MnT1abNm0UExOjqKgo+fv7q0+fPiU+jouLi2bPnq1//OMfCgwMtIeO0sVvRhw8eLDy8/M1cODAUqy+eBbjJvueZ5vNJqvVqqysLHl7e5tdDgDc1M6dO6cjR46oQYMGcnd3N7sc3KAu93fGuKB0cT8B4L8Y56CyGjJkiE6ePKn169dftl9pjbF4lA8AAAAAAOAml5WVpf3792vFihVXDKVKE8EUAAAAAADATa53797avXu3RowYUa5rZxFMAQAAAAAA3OS2bdtmynlZ/BwAAAAAAACmIJgCAAAAAKCM3GTfN4abSGn9bRNMAQAAAABQypydnSVJeXl5JlcClI2zZ89KkqpUqXJdx2GNKQAAAAAASpmLi4s8PT118uRJValSRU5OzAvBjcEwDJ09e1YnTpyQj4+PPYS9VgRTAAAAAACUMovFooCAAB05ckQ///yz2eUApc7Hx0f+/v7XfRyCKQAAAAAAyoCrq6saN27M43y44VSpUuW6Z0pdYmowtX37dr388svas2eP0tPTtXbtWvXp06fY/jt37tRTTz2lgwcP6uzZs6pfv77++te/avz48eVXNAAApSQqKkrh4eGaNWuWJCk4OFjjxo3TuHHjit3HYrFc8d/Lq1FaxwEAAJfn5OQkd3d3s8sAKixTH3LNyclRWFiY5s2bd1X9q1atqtGjR2v79u06cOCAJk+erMmTJ+uf//xnGVcKAMB/9erVS926dSvyvR07dshisWjfvn0lPu6XX36p4cOHX295DqZPn67w8PBC7enp6erevXupnut/LV26VD4+PmV6DgAAAFRups6Y6t69e4kGxa1bt1br1q3tr4ODg7VmzRrt2LGj1AfyAAAUZ8iQIerXr5+OHz+uunXrOry3ZMkSRUREqFWrViU+bq1atUqrxCsqjfUAAAAAgOtVqb8W4Ouvv9bnn3+uTp06FdsnNzdXNpvNYQMA4Hr85S9/Ua1atbR06VKH9uzsbK1evVpDhgzRb7/9poceekh16tSRp6enWrZsqXfeeeeyxw0ODrY/1idJhw4dUseOHeXu7q7mzZtry5YthfZ56qmn1KRJE3l6eqphw4aaMmWKzp8/L+nijKWnn35a33zzjSwWiywWi71mi8WidevW2Y+zf/9+3XXXXfLw8FCNGjU0fPhwZWdn298fPHiw+vTpo1deeUUBAQGqUaOGRo0aZT/XtUhLS1Pv3r1VrVo1eXt764EHHlBmZqb9/W+++UadO3eWl5eXvL291bZtW3311VeSpJ9//lm9evVS9erVVbVqVd1yyy3auHHjNdcCAAAAc1TKxc/r1q2rkydP6sKFC5o+fbqGDh1abN/ExEQ9/fTT5VgdAOC6GIZ0/qw5567iKVksV+zm4uKigQMHaunSpZo0aZIs/3+f1atXKz8/Xw899JCys7PVtm1bPfXUU/L29taGDRs0YMAAhYSEqH379lc8R0FBgfr27Ss/Pz/t2rVLWVlZRa495eXlpaVLlyowMFD79+/XsGHD5OXlpSeffFKxsbH69ttvtWnTJn388ceSJKvVWugYOTk5iomJUYcOHfTll1/qxIkTGjp0qEaPHu0Qvm3dulUBAQHaunWrfvzxR8XGxio8PFzDhg274vUUdX2XQqlPP/1UFy5c0KhRoxQbG6tt27ZJkvr376/WrVtrwYIFcnZ2VkpKiqpUqSJJGjVqlPLy8rR9+3ZVrVpV33//vapVq1biOgAAAGCuShlM7dixQ9nZ2friiy80ceJENWrUSA899FCRfRMSEhQfH29/bbPZFBQUVF6lAgBK6vxZ6flAc879918l16pX1fXRRx/Vyy+/rE8//VRRUVGSLj7G169fP1mtVlmtVk2YMMHef8yYMdq8ebPefffdqwqmPv74Yx08eFCbN29WYODF+/H8888XegR+8uTJ9p+Dg4M1YcIErVy5Uk8++aQ8PDxUrVo1ubi4XPbRvRUrVujcuXN68803VbXqxeufO3euevXqpRdffFF+fn6SpOrVq2vu3LlydnZWaGioevbsqaSkpGsKppKSkrR//34dOXLE/u/ym2++qVtuuUVffvml2rVrp7S0ND3xxBMKDQ2VJDVu3Ni+f1pamvr166eWLVtKkho2bFjiGgAAAGC+ShlMNWjQQJLUsmVLZWZmavr06cUGU25ubnJzcyvP8gAAN4HQ0FDddtttWrx4saKiovTjjz9qx44dmjFjhiQpPz9fzz//vN5991398ssvysvLU25urjw9Pa/q+AcOHFBQUJA9lJKkDh06FOq3atUqzZ49W4cPH1Z2drYuXLggb2/vEl3LgQMHFBYWZg+lJOn2229XQUGBUlNT7cHULbfc4vC1wAEBAdq/f3+JzvXncwYFBTl8WNS8eXP5+PjowIEDateuneLj4zV06FAtX75c0dHRuv/++xUSEiJJ+tvf/qbHHntMH330kaKjo9WvX79rWtcLAAAA5qqUwdSfFRQUKDc31+wyAAClpYrnxZlLZp27BIYMGaIxY8Zo3rx5WrJkiUJCQuzrHr788st6/fXXNWvWLLVs2VJVq1bVuHHjlJeXV2rlJicnq3///nr66acVExMjq9WqlStX6tVXXy21c/zZpcfoLrFYLCooKCiTc0kXv1Hw4Ycf1oYNG/Thhx9q2rRpWrlype69914NHTpUMTEx2rBhgz766CMlJibq1Vdf1ZgxY8qsHgAAAJQ+Uxc/z87OVkpKilJSUiRJR44cUUpKitLS0iRdfAxv4MCB9v7z5s3Tv//9bx06dEiHDh3SG2+8oVdeeUWPPPKIGeUDAMqCxXLxcToztqtYX+rPHnjgATk5OWnFihV688039eijj9rXm/rss8/Uu3dvPfLIIwoLC1PDhg31ww8/XPWxmzVrpmPHjik9Pd3e9sUXXzj0+fzzz1W/fn1NmjRJERERaty4sX7++WeHPq6ursrPz7/iub755hvl5OTY2z777DM5OTmpadOmV11zSVy6vmPHjtnbvv/+e505c0bNmze3tzVp0kTjx4/XRx99pL59+2rJkiX294KCgjRixAitWbNGjz/+uBYtWlQmtQIAAKDsmDpj6quvvlLnzp3try+tBTVo0CAtXbpU6enp9pBKujg7KiEhQUeOHJGLi4tCQkL04osv6q9//Wu51w4AQLVq1RQbG6uEhATZbDYNHjzY/l7jxo313nvv6fPPP1f16tU1c+ZMZWZmOoQulxMdHa0mTZpo0KBBevnll2Wz2TRp0iSHPo0bN1ZaWppWrlypdu3aacOGDVq7dq1Dn+DgYPsHP3Xr1pWXl1ehR9z79++vadOmadCgQZo+fbpOnjypMWPGaMCAAfbH+K5Vfn6+/QOoS9zc3BQdHa2WLVuqf//+mjVrli5cuKCRI0eqU6dOioiI0B9//KEnnnhC9913nxo0aKDjx4/ryy+/VL9+/SRJ48aNU/fu3dWkSRP95z//0datW9WsWbPrqhUAAADlz9RgKioqSoZhFPv+/34N95gxY5iiDwCoUIYMGaI33nhDPXr0cFgPavLkyfrpp58UExMjT09PDR8+XH369FFWVtZVHdfJyUlr167VkCFD1L59ewUHB2v27Nnq1q2bvc8999yj8ePHa/To0crNzVXPnj01ZcoUTZ8+3d6nX79+WrNmjTp37qwzZ85oyZIlDgGaJHl6emrz5s0aO3as2rVrJ09PT/Xr108zZ868rnsjXZwd3bp1a4e2kJAQ/fjjj3r//fc1ZswYdezYUU5OTurWrZvmzJkjSXJ2dtZvv/2mgQMHKjMzUzVr1lTfvn3t37Sbn5+vUaNG6fjx4/L29la3bt302muvXXe9AAAAKF8W43LJ0A3IZrPJarUqKyurxIvDAgBK17lz53TkyBE1aNBA7u7uZpeDG9Tl/s4YF5Qu7icAAJBKNiYwdY0pAAAAAAAA3LwIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgCY7ib7Hg6Us8r49zVv3jwFBwfL3d1dkZGR2r1791Xtt3LlSlksFvXp08eh3TAMTZ06VQEBAfLw8FB0dLQOHTrk0Cc4OFgWi8Vhe+GFF0rrkgAAAIpEMAUAME2VKlUkSWfPnjW5EtzILv19Xfp7q+hWrVql+Ph4TZs2TXv37lVYWJhiYmJ04sSJy+539OhRTZgwQXfeeWeh91566SXNnj1bCxcu1K5du1S1alXFxMTo3LlzDv1mzJih9PR0+zZmzJhSvTYAAID/5WJ2AQCAm5ezs7N8fHzs/8Pt6ekpi8ViclW4URiGobNnz+rEiRPy8fGRs7Oz2SVdlZkzZ2rYsGGKi4uTJC1cuFAbNmzQ4sWLNXHixCL3yc/PV//+/fX0009rx44dOnPmjP09wzA0a9YsTZ48Wb1795Ykvfnmm/Lz89O6dev04IMP2vt6eXnJ39//qmvNzc1Vbm6u/bXNZivJpQIAABBMAQDMdel/gq80GwS4Vj4+PiUKW8yUl5enPXv2KCEhwd7m5OSk6OhoJScnF7vfjBkzVLt2bQ0ZMkQ7duxweO/IkSPKyMhQdHS0vc1qtSoyMlLJyckOwdQLL7ygZ555RvXq1dPDDz+s8ePHy8Wl+OFiYmKinn766Wu5VAAAAEkEUwAAk1ksFgUEBKh27do6f/682eXgBlOlSpVKM1NKkk6dOqX8/Hz5+fk5tPv5+engwYNF7rNz50698cYbSklJKfL9jIwM+zH+95iX3pOkv/3tb2rTpo18fX31+eefKyEhQenp6Zo5c2ax9SYkJCg+Pt7+2mazKSgo6LLXCAAA8GcEUwCACsHZ2blSBQhARfD7779rwIABWrRokWrWrHldx/pzwNSqVSu5urrqr3/9qxITE+Xm5lbkPm5ubsW+BwAAcDUIpgAAACqImjVrytnZWZmZmQ7tmZmZRT6OePjwYR09elS9evWytxUUFEiSXFxclJqaat8vMzNTAQEBDscMDw8vtpbIyEhduHBBR48eVdOmTa/nsgAAAIrFt/IBAABUEK6urmrbtq2SkpLsbQUFBUpKSlKHDh0K9Q8NDdX+/fuVkpJi3+655x517txZKSkpCgoKUoMGDeTv7+9wTJvNpl27dhV5zEtSUlLk5OSk2rVrl+5FAgAA/AkzpgAAACqQ+Ph4DRo0SBEREWrfvr1mzZqlnJwc+7f0DRw4UHXq1FFiYqLc3d3VokULh/19fHwkyaF93LhxevbZZ9W4cWM1aNBAU6ZMUWBgoPr06SNJSk5O1q5du9S5c2d5eXkpOTlZ48eP1yOPPKLq1auXy3UDAICbE8EUAABABRIbG6uTJ09q6tSpysjIUHh4uDZt2mRfvDwtLU1OTiWb9P7kk08qJydHw4cP15kzZ3THHXdo06ZNcnd3l3RxraiVK1dq+vTpys3NVYMGDTR+/HiHdacAAADKgsUwDMPsIsqTzWaT1WpVVlaWvL29zS4HAACYiHFB6eJ+AgAAqWRjAtaYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAIAKZt68eQoODpa7u7siIyO1e/fuq9pv5cqVslgs6tOnj0O7YRiaOnWqAgIC5OHhoejoaB06dMihz+nTp9W/f395e3vLx8dHQ4YMUXZ2dmldEgAAQJEIpgAAACqQVatWKT4+XtOmTdPevXsVFhammJgYnThx4rL7HT16VBMmTNCdd95Z6L2XXnpJs2fP1sKFC7Vr1y5VrVpVMTExOnfunL1P//799d1332nLli364IMPtH37dg0fPrzUrw8AAODPLIZhGGYXUZ5sNpusVquysrLk7e1tdjkAAMBEFXFcEBkZqXbt2mnu3LmSpIKCAgUFBWnMmDGaOHFikfvk5+erY8eOevTRR7Vjxw6dOXNG69atk3RxtlRgYKAef/xxTZgwQZKUlZUlPz8/LV26VA8++KAOHDig5s2b68svv1RERIQkadOmTerRo4eOHz+uwMDAq6q9It5PAABQ/koyJmDGFAAAQAWRl5enPXv2KDo62t7m5OSk6OhoJScnF7vfjBkzVLt2bQ0ZMqTQe0eOHFFGRobDMa1WqyIjI+3HTE5Olo+Pjz2UkqTo6Gg5OTlp165dxZ43NzdXNpvNYQMAACgJgikAAIAK4tSpU8rPz5efn59Du5+fnzIyMorcZ+fOnXrjjTe0aNGiIt+/tN/ljpmRkaHatWs7vO/i4iJfX99izytJiYmJslqt9i0oKOjyFwgAAPA/CKYAAAAqqd9//10DBgzQokWLVLNmzXI/f0JCgrKysuzbsWPHyr0GAABQubmYXQAAAAAuqlmzppydnZWZmenQnpmZKX9//0L9Dx8+rKNHj6pXr172toKCAkkXZzylpqba98vMzFRAQIDDMcPDwyVJ/v7+hRZXv3Dhgk6fPl3keS9xc3OTm5tbyS4SAADgT5gxBQAAUEG4urqqbdu2SkpKsrcVFBQoKSlJHTp0KNQ/NDRU+/fvV0pKin2755571LlzZ6WkpCgoKEgNGjSQv7+/wzFtNpt27dplP2aHDh105swZ7dmzx97nk08+UUFBgSIjI8vwigEAwM3O1GBq+/bt6tWrlwIDA2WxWOzfHlOcNWvWqGvXrqpVq5a8vb3VoUMHbd68uXyKBQAAKAfx8fFatGiRli1bpgMHDuixxx5TTk6O4uLiJEkDBw5UQkKCJMnd3V0tWrRw2Hx8fOTl5aUWLVrI1dVVFotF48aN07PPPqv169dr//79GjhwoAIDA9WnTx9JUrNmzdStWzcNGzZMu3fv1meffabRo0frwQcfvOpv5AMAALgWpj7Kl5OTo7CwMD366KPq27fvFftv375dXbt21fPPPy8fHx8tWbJEvXr10q5du9S6detyqBgAAKBsxcbG6uTJk5o6daoyMjIUHh6uTZs22RcvT0tLk5NTyT5bfPLJJ5WTk6Phw4frzJkzuuOOO7Rp0ya5u7vb+7z99tsaPXq0unTpIicnJ/Xr10+zZ88u1WsDAAD4XxbDMAyzi5Aki8WitWvX2j+5u1q33HKLYmNjNXXq1Kvqb7PZZLValZWVJW9v72uoFAAA3CgYF5Qu7icAAJBKNiao1IufFxQU6Pfff5evr2+xfXJzc5Wbm2t/bbPZyqM0AAAAAAAAXEGlXvz8lVdeUXZ2th544IFi+yQmJspqtdq3oKCgcqwQAAAAAAAAxam0wdSKFSv09NNP691331Xt2rWL7ZeQkKCsrCz7duzYsXKsEgAAAAAAAMWplI/yrVy5UkOHDtXq1asVHR192b5ubm5yc3Mrp8oAAAAAAABwtSrdjKl33nlHcXFxeuedd9SzZ0+zywEAAAAAAMA1MnXGVHZ2tn788Uf76yNHjiglJUW+vr6qV6+eEhIS9Msvv+jNN9+UdPHxvUGDBun1119XZGSkMjIyJEkeHh6yWq2mXAMAAAAAAACujakzpr766iu1bt1arVu3liTFx8erdevWmjp1qiQpPT1daWlp9v7//Oc/deHCBY0aNUoBAQH2bezYsabUDwAAAAAAgGtn6oypqKgoGYZR7PtLly51eL1t27ayLQgAAAAAAADlptKtMQUAAAAAAIAbA8EUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAFQw8+bNU3BwsNzd3RUZGandu3cX23fNmjWKiIiQj4+PqlatqvDwcC1fvtyhT2ZmpgYPHqzAwEB5enqqW7duOnTokEOfqKgoWSwWh23EiBFlcn0AAACXEEwBAABUIKtWrVJ8fLymTZumvXv3KiwsTDExMTpx4kSR/X19fTVp0iQlJydr3759iouLU1xcnDZv3ixJMgxDffr00U8//aT3339fX3/9terXr6/o6Gjl5OQ4HGvYsGFKT0+3by+99FKZXy8AALi5EUwBAABUIDNnztSwYcMUFxen5s2ba+HChfL09NTixYuL7B8VFaV7771XzZo1U0hIiMaOHatWrVpp586dkqRDhw7piy++0IIFC9SuXTs1bdpUCxYs0B9//KF33nnH4Vienp7y9/e3b97e3mV+vQAA4OZGMAUAAFBB5OXlac+ePYqOjra3OTk5KTo6WsnJyVfc3zAMJSUlKTU1VR07dpQk5ebmSpLc3d0djunm5mYPry55++23VbNmTbVo0UIJCQk6e/bsZc+Xm5srm83msAEAAJSEi9kFAAAA4KJTp04pPz9ffn5+Du1+fn46ePBgsftlZWWpTp06ys3NlbOzs+bPn6+uXbtKkkJDQ1WvXj0lJCToH//4h6pWrarXXntNx48fV3p6uv0YDz/8sOrXr6/AwEDt27dPTz31lFJTU7VmzZpiz5uYmKinn376Oq8aAADczAimAAAAKjkvLy+lpKQoOztbSUlJio+PV8OGDRUVFaUqVapozZo1GjJkiHx9feXs7Kzo6Gh1795dhmHYjzF8+HD7zy1btlRAQIC6dOmiw4cPKyQkpMjzJiQkKD4+3v7aZrMpKCio7C4UAADccAimAAAAKoiaNWvK2dlZmZmZDu2ZmZny9/cvdj8nJyc1atRIkhQeHq4DBw4oMTFRUVFRkqS2bdsqJSVFWVlZysvLU61atRQZGamIiIhijxkZGSlJ+vHHH4sNptzc3OTm5laSSwQAAHDAGlMAAAAVhKurq9q2baukpCR7W0FBgZKSktShQ4erPk5BQYF9bak/s1qtqlWrlg4dOqSvvvpKvXv3LvYYKSkpkqSAgICrvwAAAIASYsYUAABABRIfH69BgwYpIiJC7du316xZs5STk6O4uDhJ0sCBA1WnTh0lJiZKurjOU0REhEJCQpSbm6uNGzdq+fLlWrBggf2Yq1evVq1atVSvXj3t379fY8eOVZ8+fXT33XdLkg4fPqwVK1aoR48eqlGjhvbt26fx48erY8eOatWqVfnfBAAAcNMgmAIAAKhAYmNjdfLkSU2dOlUZGRkKDw/Xpk2b7Auip6Wlycnpv5Pec3JyNHLkSB0/flweHh4KDQ3VW2+9pdjYWHuf9PR0xcfHKzMzUwEBARo4cKCmTJlif9/V1VUff/yxPQQLCgpSv379NHny5PK7cAAAcFOyGH9e9fImYLPZZLValZWVJW9vb7PLAQAAJmJcULq4nwAAQCrZmIA1pgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApiCYAgAAAAAAgCkIpgAAAAAAAGAKgikAAAAAAACYgmAKAAAAAAAApjA1mNq+fbt69eqlwMBAWSwWrVu37rL909PT9fDDD6tJkyZycnLSuHHjyqVOAAAAAAAAlD5Tg6mcnByFhYVp3rx5V9U/NzdXtWrV0uTJkxUWFlbG1QEAAAAAAKAsuZh58u7du6t79+5X3T84OFivv/66JGnx4sVlVRYAAAAAAADKwQ2/xlRubq5sNpvDBgAAUNq2bt1qdgkAAACVzg0fTCUmJspqtdq3oKAgs0sCAAA3oG7duikkJETPPvusjh07ZnY5AAAAlcINH0wlJCQoKyvLvjFQBAAAZeGXX37R6NGj9d5776lhw4aKiYnRu+++q7y8PLNLAwAAqLBu+GDKzc1N3t7eDhsAAEBpq1mzpsaPH6+UlBTt2rVLTZo00ciRIxUYGKi//e1v+uabb8wuEQAAoMK54YMpAACA8tamTRslJCRo9OjRys7O1uLFi9W2bVvdeeed+u6778wuDwAAoMIwNZjKzs5WSkqKUlJSJElHjhxRSkqK0tLSJF18DG/gwIEO+1zqn52drZMnTyolJUXff/99eZcOAABQyPnz5/Xee++pR48eql+/vjZv3qy5c+cqMzNTP/74o+rXr6/777/f7DIBAAAqDIthGIZZJ9+2bZs6d+5cqH3QoEFaunSpBg8erKNHj2rbtm329ywWS6H+9evX19GjR6/qnDabTVarVVlZWTzWBwDATa40xwVjxozRO++8I8MwNGDAAA0dOlQtWrRw6JORkaHAwEAVFBRc17kqKsZZAABAKtmYwKWcaipSVFSULpeLLV26tFCbiTkaAABAsb7//nvNmTNHffv2lZubW5F9atasqa1bt5ZzZQAAABUXa0wBAACUgmnTpun+++8vFEpduHBB27dvlyS5uLioU6dOZpQHAABQIRFMAQAAlILOnTvr9OnThdqzsrKKXLoAAAAABFMAAAClwjCMItfC/O2331S1alUTKgIAAKj4TF1jCgAAoLLr27evpItf0DJ48GCHR/ny8/O1b98+3XbbbWaVBwAAUKERTAEAAFwHq9Uq6eKMKS8vL3l4eNjfc3V11a233qphw4aZVR4AAECFRjAFAABwHZYsWSJJCg4O1oQJE3hsDwAAoAQIpgAAAErBtGnTzC4BAACg0iGYAgAAuEZt2rRRUlKSqlevrtatWxe5+Pkle/fuLcfKAAAAKgeCKQAAgGvUu3dv+2Lnffr0MbcYAACASohgCgAA4BpdenwvPz9fnTt3VqtWreTj43Pdx503b55efvllZWRkKCwsTHPmzFH79u2L7LtmzRo9//zz+vHHH3X+/Hk1btxYjz/+uAYMGGDvk5mZqaeeekofffSRzpw5o44dO2rOnDlq3Lixvc+5c+f0+OOPa+XKlcrNzVVMTIzmz58vPz+/674eAACA4jiZXQAAAEBl5+zsrLvvvlv/+c9/rvtYq1atUnx8vKZNm6a9e/cqLCxMMTExOnHiRJH9fX19NWnSJCUnJ2vfvn2Ki4tTXFycNm/eLOnitwX26dNHP/30k95//319/fXXql+/vqKjo5WTk2M/zvjx4/Xvf/9bq1ev1qeffqpff/1Vffv2ve7rAQAAuByLYRhGSXc6duyYLBaL6tatK0navXu3VqxYoebNm2v48OGlXmRpstlsslqtysrKkre3t9nlAAAAE5XmuCAiIkIvvviiunTpcl3HiYyMVLt27TR37lxJUkFBgYKCgjRmzBhNnDjxqo7Rpk0b9ezZU88884x++OEHNW3aVN9++61uueUW+zH9/f31/PPPa+jQocrKylKtWrW0YsUK3XfffZKkgwcPqlmzZkpOTtatt95a5Hlyc3OVm5trf22z2RQUFMQ4CwCAm1xJxljXNGPq4Ycf1tatWyVJGRkZ6tq1q3bv3q1JkyZpxowZ13JIAACASu3ZZ5/VhAkT9MEHHyg9PV02m81huxp5eXnas2ePoqOj7W1OTk6Kjo5WcnLyFfc3DENJSUlKTU1Vx44dJckeHLm7uzsc083NTTt37pQk7dmzR+fPn3c4b2hoqOrVq3fZ8yYmJspqtdq3oKCgq7pOAACAS64pmPr222/t6xy8++67atGihT7//HO9/fbbWrp0aWnWBwAAUCn06NFD33zzje655x7VrVtX1atXV/Xq1eXj46Pq1atf1TFOnTql/Pz8Qus6+fn5KSMjo9j9srKyVK1aNbm6uqpnz56aM2eOunbtKum/AVNCQoL+85//KC8vTy+++KKOHz+u9PR0SRc/aHR1dS20PtaVzpuQkKCsrCz7duzYsau6TgAAgEuuafHz8+fP27+B5uOPP9Y999wj6eLA59IABwAA4GZyaTa5Gby8vJSSkqLs7GwlJSUpPj5eDRs2VFRUlKpUqaI1a9ZoyJAh8vX1lbOzs6Kjo9W9e3ddw4oODtzc3OxjQgAAgGtxTcHULbfcooULF6pnz57asmWLnnnmGUnSr7/+qho1apRqgQAAAJVBp06drvsYNWvWlLOzszIzMx3aMzMz5e/vX+x+Tk5OatSokSQpPDxcBw4cUGJioqKioiRJbdu2VUpKirKyspSXl6datWopMjJSERERkiR/f3/l5eXpzJkzDrOmrnReAACA63VNj/K9+OKL+sc//qGoqCg99NBDCgsLkyStX7++2K8yBgAAuBmcPXtWBw8e1L59+xy2q+Hq6qq2bdsqKSnJ3lZQUKCkpCR16NDhqmsoKChwWJT8EqvVqlq1aunQoUP66quv1Lt3b0kXg6sqVao4nDc1NVVpaWklOi8AAEBJXdOMqaioKJ06dUo2m81hzYThw4fL09Oz1IoDAACoLE6ePKm4uDh9+OGHRb6fn59/VceJj4/XoEGDFBERofbt22vWrFnKyclRXFycJGngwIGqU6eOEhMTJV1cgDwiIkIhISHKzc3Vxo0btXz5ci1YsMB+zNWrV6tWrVqqV6+e9u/fr7Fjx6pPnz66++67JV0MrIYMGaL4+Hj5+vrK29tbY8aMUYcOHYr9Rj4AAIDScE3B1B9//CHDMOyh1M8//6y1a9eqWbNmiomJKdUCAQAAKoNx48bpzJkz2rVrl6KiorR27VplZmbq2Wef1auvvnrVx4mNjdXJkyc1depUZWRkKDw8XJs2bbIviJ6WliYnp/9Oes/JydHIkSN1/PhxeXh4KDQ0VG+99ZZiY2PtfdLT0xUfH6/MzEwFBARo4MCBmjJlisN5X3vtNTk5Oalfv37Kzc1VTEyM5s+ff513BQAA4PIsxjWsenn33Xerb9++GjFihM6cOaPQ0FBVqVJFp06d0syZM/XYY4+VRa2lwmazyWq1KisrS97e3maXAwAATFSa44KAgAC9//77at++vby9vfXVV1+pSZMmWr9+vV566SXt3LmzlKquuBhnAQAAqWRjgmtaY2rv3r268847JUnvvfee/Pz89PPPP+vNN9/U7Nmzr+WQAAAAlVpOTo5q164tSapevbpOnjwpSWrZsqX27t1rZmkAAAAV1jUFU2fPnpWXl5ck6aOPPlLfvn3l5OSkW2+9VT///HOpFggAAFAZNG3aVKmpqZKksLAw/eMf/9Avv/yihQsXKiAgwOTqAAAAKqZrCqYaNWqkdevW6dixY9q8ebN94cwTJ04wbRsAANyUxo4dq/T0dEnStGnT9OGHH6pevXqaPXu2nn/+eZOrAwAAqJiuafHzqVOn6uGHH9b48eN111132b9G+KOPPlLr1q1LtUAAAIDK4JFHHrH/3LZtW/388886ePCg6tWrp5o1a5pYGQAAQMV1TcHUfffdpzvuuEPp6ekKCwuzt3fp0kX33ntvqRUHAABQWXl6eqpNmzZmlwEAAFChXVMwJUn+/v7y9/fX8ePHJUl169ZV+/btS60wAACAii4+Pv6q+86cObMMKwEAAKicrimYKigo0LPPPqtXX31V2dnZkiQvLy89/vjjmjRpkpycrmnpKgAAgErl66+/vqp+FouljCsBAAConK4pmJo0aZLeeOMNvfDCC7r99tslSTt37tT06dN17tw5Pffcc6VaJAAAQEW0detWs0sAAACo1K4pmFq2bJn+7//+T/fcc4+9rVWrVqpTp45GjhxJMAUAAAAAAIAruqZg6vTp0woNDS3UHhoaqtOnT193UQAAAJVB3759tXTpUnl7e6tv376X7btmzZpyqgoAAKDyuKbFoMLCwjR37txC7XPnzlWrVq2uuygAAIDKwGq12tePslqtl90AAABQ2DXNmHrppZfUs2dPffzxx+rQoYMkKTk5WceOHdPGjRtLtUAAAICKasmSJUX+DAAAgKtzTTOmOnXqpB9++EH33nuvzpw5ozNnzqhv37767rvvtHz58tKuEQAAAAAAADcgi2EYRmkd7JtvvlGbNm2Un59fWocsdTabTVarVVlZWfL29ja7HAAAYKLSHBf89ttvmjp1qrZu3aoTJ06ooKDA4f2bYR1OxlkAAEAq2Zjgmh7lAwAAgKMBAwboxx9/1JAhQ+Tn52dfewoAAADFI5gCAAAoBTt27NDOnTsVFhZmdikAAACVxjWtMQUAAABHoaGh+uOPP8wuAwAAoFIp0Yypvn37Xvb9M2fOXE8tAAAAldb8+fM1ceJETZ06VS1atFCVKlUc3mfNJQAAgMJKFExZrdYrvj9w4MDrKggAAKAy8vHxkc1m01133eXQbhiGLBZLhf5yGAAAALOUKJhasmRJWdUBAABQqfXv319VqlTRihUrWPwcAADgKrH4OQAAQCn49ttv9fXXX6tp06ZmlwIAAFBpsPg5AABAKYiIiNCxY8fMLgMAAKBSYcYUAABAKRgzZozGjh2rJ554Qi1btiy0+HmrVq1MqgwAAKDiIpgCAAAoBbGxsZKkRx991N5msVhY/BwAAOAyCKYAAABKwZEjR8wuAQAAoNIhmAIAACgF9evXN7sEAACASodgCgAA4BqtX79e3bt3V5UqVbR+/frL9r3nnnvKqSoAAIDKg2AKAADgGvXp00cZGRmqXbu2+vTpU2w/1pgCAAAoGsEUAADANSooKCjyZwAAAFwdJ7MLAAAAqMySk5P1wQcfOLS9+eabatCggWrXrq3hw4crNzfXpOoAAAAqNoIpAACA6zBjxgx999139tf79+/XkCFDFB0drYkTJ+rf//63EhMTTawQAACg4iKYAgAAuA4pKSnq0qWL/fXKlSsVGRmpRYsWKT4+XrNnz9a7775rYoUAAAAVF8EUAADAdfjPf/4jPz8/++tPP/1U3bt3t79u166djh07ZkZpAAAAFR7BFAAAwHXw8/PTkSNHJEl5eXnau3evbr31Vvv7v//+u6pUqWJWeQAAABUawRQAAMB16NGjhyZOnKgdO3YoISFBnp6euvPOO+3v79u3TyEhISZWCAAAUHG5mF0AAABAZfbMM8+ob9++6tSpk6pVq6Zly5bJ1dXV/v7ixYt19913m1ghAABAxUUwBQAAcB1q1qyp7du3KysrS9WqVZOzs7PD+6tXr1a1atVMqg4AAKBiI5gCAAAoBVartch2X1/fcq4EAACg8mCNKQAAgApm3rx5Cg4Olru7uyIjI7V79+5i+65Zs0YRERHy8fFR1apVFR4eruXLlzv0yc7O1ujRo1W3bl15eHioefPmWrhwoUOfqKgoWSwWh23EiBFlcn0AAACXMGMKAACgAlm1apXi4+O1cOFCRUZGatasWYqJiVFqaqpq165dqL+vr68mTZqk0NBQubq66oMPPlBcXJxq166tmJgYSVJ8fLw++eQTvfXWWwoODtZHH32kkSNHKjAwUPfcc4/9WMOGDdOMGTPsrz09Pcv+ggEAwE2NGVMAAAAVyMyZMzVs2DDFxcXZZzZ5enpq8eLFRfaPiorSvffeq2bNmikkJERjx45Vq1attHPnTnufzz//XIMGDVJUVJSCg4M1fPhwhYWFFZqJ5enpKX9/f/vm7e1dptcKAABgajC1fft29erVS4GBgbJYLFq3bt0V99m2bZvatGkjNzc3NWrUSEuXLi3zOgEAAMpDXl6e9uzZo+joaHubk5OToqOjlZycfMX9DcNQUlKSUlNT1bFjR3v7bbfdpvXr1+uXX36RYRjaunWrfvjhh0LfFvj222+rZs2aatGihRISEnT27NnLni83N1c2m81hAwAAKAlTg6mcnByFhYVp3rx5V9X/yJEj6tmzpzp37qyUlBSNGzdOQ4cO1ebNm8u4UgAAgLJ36tQp5efny8/Pz6Hdz89PGRkZxe536RsBXV1d1bNnT82ZM0ddu3a1vz9nzhw1b95cdevWlaurq7p166Z58+Y5hFcPP/yw3nrrLW3dulUJCQlavny5HnnkkcvWm5iYKKvVat+CgoKu8coBAMDNytQ1prp3767u3btfdf+FCxeqQYMGevXVVyVJzZo1086dO/Xaa6/Z11AAAAC42Xh5eSklJUXZ2dlKSkpSfHy8GjZsqKioKEkXg6kvvvhC69evV/369bV9+3aNGjVKgYGB9tlZw4cPtx+vZcuWCggIUJcuXXT48GGFhIQUed6EhATFx8fbX9tsNsIpAABQIpVq8fPk5GSHqe2SFBMTo3HjxhW7T25urnJzc+2vmWIOAAAqqpo1a8rZ2VmZmZkO7ZmZmfL39y92PycnJzVq1EiSFB4ergMHDigxMVFRUVH6448/9Pe//11r165Vz549JUmtWrVSSkqKXnnllUJjq0siIyMlST/++GOxwZSbm5vc3NxKfJ0AAACXVKrFzzMyMoqc2m6z2fTHH38UuQ9TzAEAQGXh6uqqtm3bKikpyd5WUFCgpKQkdejQ4aqPU1BQYP9g7vz58zp//rycnByHfc7OziooKCj2GCkpKZKkgICAElwBAABAyVSqGVPXginmAACgMomPj9egQYMUERGh9u3ba9asWcrJyVFcXJwkaeDAgapTp44SExMlXfwQLiIiQiEhIcrNzdXGjRu1fPlyLViwQJLk7e2tTp066YknnpCHh4fq16+vTz/9VG+++aZmzpwpSTp8+LBWrFihHj16qEaNGtq3b5/Gjx+vjh07qlWrVubcCAAAcFOoVMGUv79/kVPbvb295eHhUeQ+TDEHAACVSWxsrE6ePKmpU6cqIyND4eHh2rRpk33WeFpamsPsp5ycHI0cOVLHjx+Xh4eHQkND9dZbbyk2NtbeZ+XKlUpISFD//v11+vRp1a9fX88995xGjBgh6eJMrY8//tgeggUFBalfv36aPHly+V48AAC46VgMwzDMLkKSLBaL1q5dqz59+hTb56mnntLGjRu1f/9+e9vDDz+s06dPa9OmTVd1HpvNJqvVqqysLHl7e19v2QAAoBJjXFC6uJ8AAEAq2ZjA1DWmsrOzlZKSYl/D4MiRI0pJSVFaWpqki4/hDRw40N5/xIgR+umnn/Tkk0/q4MGDmj9/vt59912NHz/ejPIBAAAAAABwHUwNpr766iu1bt1arVu3lnRxTYXWrVtr6tSpkqT09HR7SCVJDRo00IYNG7RlyxaFhYXp1Vdf1f/93/8pJibGlPoBAAAAAABw7SrMo3zlhSnmAADgEsYFpYv7CQAApEr0KB8AAAAAAABuXgRTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAABUMPPmzVNwcLDc3d0VGRmp3bt3F9t3zZo1ioiIkI+Pj6pWrarw8HAtX77coU92drZGjx6tunXrysPDQ82bN9fChQsd+pw7d06jRo1SjRo1VK1aNfXr10+ZmZllcn0AAACXEEwBAABUIKtWrVJ8fLymTZumvXv3KiwsTDExMTpx4kSR/X19fTVp0iQlJydr3759iouLU1xcnDZv3mzvEx8fr02bNumtt97SgQMHNG7cOI0ePVrr16+39xk/frz+/e9/a/Xq1fr000/166+/qm/fvmV+vQAA4OZmMQzDMLuI8mSz2WS1WpWVlSVvb2+zywEAACaqiOOCyMhItWvXTnPnzpUkFRQUKCgoSGPGjNHEiROv6hht2rRRz5499cwzz0iSWrRoodjYWE2ZMsXep23bturevbueffZZZWVlqVatWlqxYoXuu+8+SdLBgwfVrFkzJScn69Zbb72q81bE+wkAAMpfScYEzJgCAACoIPLy8rRnzx5FR0fb25ycnBQdHa3k5OQr7m8YhpKSkpSamqqOHTva22+77TatX79ev/zyiwzD0NatW/XDDz/o7rvvliTt2bNH58+fdzhvaGio6tWrd9nz5ubmymazOWwAAAAl4WJ2AQAAALjo1KlTys/Pl5+fn0O7n5+fDh48WOx+WVlZqlOnjnJzc+Xs7Kz58+era9eu9vfnzJmj4cOHq27dunJxcZGTk5MWLVpkD68yMjLk6uoqHx+fQufNyMgo9ryJiYl6+umnr+FKAQAALiKYAgAAqOS8vLyUkpKi7OxsJSUlKT4+Xg0bNlRUVJSki8HUF198ofXr16t+/fravn27Ro0apcDAQIdZUiWVkJCg+Ph4+2ubzaagoKDrvRwAAHATIZgCAACoIGrWrClnZ+dC34aXmZkpf3//YvdzcnJSo0aNJEnh4eE6cOCAEhMTFRUVpT/++EN///vftXbtWvXs2VOS1KpVK6WkpOiVV15RdHS0/P39lZeXpzNnzjjMmrrSed3c3OTm5nYdVwwAAG52rDEFAABQQbi6uqpt27ZKSkqytxUUFCgpKUkdOnS46uMUFBQoNzdXknT+/HmdP39eTk6Owz5nZ2cVFBRIurgQepUqVRzOm5qaqrS0tBKdFwAAoKSYMQUAAFCBxMfHa9CgQYqIiFD79u01a9Ys5eTkKC4uTpI0cOBA1alTR4mJiZIurvMUERGhkJAQ5ebmauPGjVq+fLkWLFggSfL29lanTp30xBNPyMPDQ/Xr19enn36qN998UzNnzpQkWa1WDRkyRPHx8fL19ZW3t7fGjBmjDh06XPU38gEAAFwLgikAAIAKJDY2VidPntTUqVOVkZGh8PBwbdq0yb4gelpamsPsp5ycHI0cOVLHjx+Xh4eHQkND9dZbbyk2NtbeZ+XKlUpISFD//v11+vRp1a9fX88995xGjBhh7/Paa6/JyclJ/fr1U25urmJiYjR//vzyu3AAAHBTshiGYZhdRHmy2WyyWq3KysqSt7e32eUAAAATMS4oXdxPAAAglWxMwBpTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTEEwBAAAAAADAFARTAAAAAAAAMAXBFAAAAAAAAExBMAUAAAAAAABTVIhgat68eQoODpa7u7siIyO1e/fuYvueP39eM2bMUEhIiNzd3RUWFqZNmzaVY7UAAAAAAAAoDaYHU6tWrVJ8fLymTZumvXv3KiwsTDExMTpx4kSR/SdPnqx//OMfmjNnjr7//nuNGDFC9957r77++utyrhwAAAAAAADXw2IYhmFmAZGRkWrXrp3mzp0rSSooKFBQUJDGjBmjiRMnFuofGBioSZMmadSoUfa2fv36ycPDQ2+99dYVz2ez2WS1WpWVlSVvb+/SuxAAAFDpMC4oXdxPAAAglWxMYOqMqby8PO3Zs0fR0dH2NicnJ0VHRys5ObnIfXJzc+Xu7u7Q5uHhoZ07dxbb32azOWwAAAAAAAAwn6nB1KlTp5Sfny8/Pz+Hdj8/P2VkZBS5T0xMjGbOnKlDhw6poKBAW7Zs0Zo1a5Senl5k/8TERFmtVvsWFBRU6tcBAAAAAACAkjN9jamSev3119W4cWOFhobK1dVVo0ePVlxcnJycir6UhIQEZWVl2bdjx46Vc8UAAAAAAAAoiqnBVM2aNeXs7KzMzEyH9szMTPn7+xe5T61atbRu3Trl5OTo559/1sGDB1WtWjU1bNiwyP5ubm7y9vZ22AAAAAAAAGA+U4MpV1dXtW3bVklJSfa2goICJSUlqUOHDpfd193dXXXq1NGFCxf0r3/9S7179y7rcgEAAAAAAFCKXMwuID4+XoMGDVJERITat2+vWbNmKScnR3FxcZKkgQMHqk6dOkpMTJQk7dq1S7/88ovCw8P1yy+/aPr06SooKNCTTz5p5mUAAAAAAACghEwPpmJjY3Xy5ElNnTpVGRkZCg8P16ZNm+wLoqelpTmsH3Xu3DlNnjxZP/30k6pVq6YePXpo+fLl8vHxMekKAAAAAAAAcC0shmEYZhdRnmw2m6xWq7KyslhvCgCAmxzjgtLF/QQAAFLJxgSV7lv5AAAAAAAAcGMgmAIAAAAAAIApCKYAAAAAAABgCoIpAAAAAAAAmIJgCgAAAAAAAKYgmAIAAAAAAIApCKYAAAAAAABgCoIpAAAAAAAAmIJgCgAAAAAAAKYgmAIAAAAAAIApCKYAAAAAAABgCoIpAAAAAAAAmIJgCgAAoIKZN2+egoOD5e7ursjISO3evbvYvmvWrFFERIR8fHxUtWpVhYeHa/ny5Q59LBZLkdvLL79s7xMcHFzo/RdeeKHMrhEAAECSXMwuAAAAAP+1atUqxcfHa+HChYqMjNSsWbMUExOj1NRU1a5du1B/X19fTZo0SaGhoXJ1ddUHH3yguLg41a5dWzExMZKk9PR0h30+/PBDDRkyRP369XNonzFjhoYNG2Z/7eXlVQZXCAAA8F8EUwAAABXIzJkzNWzYMMXFxUmSFi5cqA0bNmjx4sWaOHFiof5RUVEOr8eOHatly5Zp586d9mDK39/foc/777+vzp07q2HDhg7tXl5ehfoCAACUJR7lAwAAqCDy8vK0Z88eRUdH29ucnJwUHR2t5OTkK+5vGIaSkpKUmpqqjh07FtknMzNTGzZs0JAhQwq998ILL6hGjRpq3bq1Xn75ZV24cOGy58vNzZXNZnPYAAAASoIZUwAAABXEqVOnlJ+fLz8/P4d2Pz8/HTx4sNj9srKyVKdOHeXm5srZ2Vnz589X165di+y7bNkyeXl5qW/fvg7tf/vb39SmTRv5+vrq888/V0JCgtLT0zVz5sxiz5uYmKinn366BFcIAADgiGAKAACgkvPy8lJKSoqys7OVlJSk+Ph4NWzYsNBjfpK0ePFi9e/fX+7u7g7t8fHx9p9btWolV1dX/fWvf1ViYqLc3NyKPG9CQoLDfjabTUFBQaVzUQAA4KZAMAUAAFBB1KxZU87OzsrMzHRoz8zMvOzaT05OTmrUqJEkKTw8XAcOHFBiYmKhYGrHjh1KTU3VqlWrrlhLZGSkLly4oKNHj6pp06ZF9nFzcys2tAIAALgarDEFAABQQbi6uqpt27ZKSkqytxUUFCgpKUkdOnS46uMUFBQoNze3UPsbb7yhtm3bKiws7IrHSElJkZOTU5HfBAgAAFBamDEFAABQgcTHx2vQoEGKiIhQ+/btNWvWLOXk5Ni/pW/gwIGqU6eOEhMTJV1c5ykiIkIhISHKzc3Vxo0btXz5ci1YsMDhuDabTatXr9arr75a6JzJycnatWuXOnfuLC8vLyUnJ2v8+PF65JFHVL169bK/aAAAcNMimAIAAKhAYmNjdfLkSU2dOlUZGRkKDw/Xpk2b7Auip6Wlycnpv5Pec3JyNHLkSB0/flweHh4KDQ3VW2+9pdjYWIfjrly5UoZh6KGHHip0Tjc3N61cuVLTp09Xbm6uGjRooPHjxzusHwUAAFAWLIZhGGYXUZ5sNpusVquysrLk7e1tdjkAAMBEjAtKF/cTAABIJRsTsMYUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFNUiGBq3rx5Cg4Olru7uyIjI7V79+7L9p81a5aaNm0qDw8PBQUFafz48Tp37lw5VQsAAAAAAIDSYHowtWrVKsXHx2vatGnau3evwsLCFBMToxMnThTZf8WKFZo4caKmTZumAwcO6I033tCqVav097//vZwrBwAAKBsl+dBuzZo1ioiIkI+Pj6pWrarw8HAtX77coY/FYilye/nll+19Tp8+rf79+8vb21s+Pj4aMmSIsrOzy+waAQAApAoQTM2cOVPDhg1TXFycmjdvroULF8rT01OLFy8usv/nn3+u22+/XQ8//LCCg4N1991366GHHrriLCsAAIDKoKQf2vn6+mrSpElKTk7Wvn37FBcXp7i4OG3evNneJz093WFbvHixLBaL+vXrZ+/Tv39/fffdd9qyZYs++OADbd++XcOHDy/z6wUAADc3U4OpvLw87dmzR9HR0fY2JycnRUdHKzk5uch9brvtNu3Zs8ceRP3000/auHGjevToUWT/3Nxc2Ww2hw0AAKCiKumHdlFRUbr33nvVrFkzhYSEaOzYsWrVqpV27txp7+Pv7++wvf/+++rcubMaNmwoSTpw4IA2bdqk//u//1NkZKTuuOMOzZkzRytXrtSvv/5aLtcNAABuTqYGU6dOnVJ+fr78/Pwc2v38/JSRkVHkPg8//LBmzJihO+64Q1WqVFFISIiioqKKfZQvMTFRVqvVvgUFBZX6dQAAAJSGa/nQ7s8Mw1BSUpJSU1PVsWPHIvtkZmZqw4YNGjJkiL0tOTlZPj4+ioiIsLdFR0fLyclJu3btKvZ8fAAIAACul+mP8pXUtm3b9Pzzz2v+/Pnau3ev1qxZow0bNuiZZ54psn9CQoKysrLs27Fjx8q5YgAAgKtzLR/aSVJWVpaqVasmV1dX9ezZU3PmzFHXrl2L7Lts2TJ5eXmpb9++9raMjAzVrl3boZ+Li4t8fX0ve14+AAQAANfLxcyT16xZU87OzsrMzHRoz8zMlL+/f5H7TJkyRQMGDNDQoUMlSS1btlROTo6GDx+uSZMmycnJMWtzc3OTm5tb2VwAAABABeDl5aWUlBRlZ2crKSlJ8fHxatiwoaKiogr1Xbx4sfr37y93d/frPm9CQoLi4+Ptr202G+EUAAAoEVODKVdXV7Vt21ZJSUnq06ePJKmgoEBJSUkaPXp0kfucPXu2UPjk7Ows6eL0dQAAgMrqWj60ky4+7teoUSNJUnh4uA4cOKDExMRCwdSOHTuUmpqqVatWObT7+/sXWlz9woULOn369GXPyweAAADgepn+KF98fLwWLVqkZcuW6cCBA3rssceUk5OjuLg4SdLAgQOVkJBg79+rVy8tWLBAK1eu1JEjR7RlyxZNmTJFvXr1sgdUAAAAldGfP7S75NKHdh06dLjq4xQUFCg3N7dQ+xtvvKG2bdsqLCzMob1Dhw46c+aM9uzZY2/75JNPVFBQoMjIyGu4EgAAgKtj6owpSYqNjdXJkyc1depUZWRkKDw8XJs2bbKvrZCWluYwQ2ry5MmyWCyaPHmyfvnlF9WqVUu9evXSc889Z9YlAAAAlJr4+HgNGjRIERERat++vWbNmlXoQ7s6deooMTFR0sV1niIiIhQSEqLc3Fxt3LhRy5cv14IFCxyOa7PZtHr1ar366quFztmsWTN169ZNw4YN08KFC3X+/HmNHj1aDz74oAIDA8v+ogEAwE3L9GBKkkaPHl3so3vbtm1zeO3i4qJp06Zp2rRp5VAZAABA+Srph3Y5OTkaOXKkjh8/Lg8PD4WGhuqtt95SbGysw3FXrlwpwzD00EMPFXnet99+W6NHj1aXLl3k5OSkfv36afbs2WV3oQAAAJIsxk22MJPNZpPValVWVpa8vb3NLgcAAJiIcUHp4n4CAACpZGMC09eYAgAAAAAAwM2JYAoAAAAAAACmIJgCAAAAAACAKQimAAAAAAAAYAqCKQAAAAAAAJiCYAoAAAAAAACmIJgCAAAAAACAKQimAAAAAAAAYAqCKQAAAAAAAJiCYAoAAAAAAACmIJgCAAAAAACAKQimAAAAAAAAYAqCKQAAAAAAAJiCYAoAAAAAAACmIJgCAAAAAACAKQimAAAAAAAAYAoXswsob4ZhSJJsNpvJlQAAALNdGg9cGh/g+jDOAgAAUsnGWDddMPX7779LkoKCgkyuBAAAVBS///67rFar2WVUeoyzAADAn13NGMti3GQfERYUFOjXX3+Vl5eXLBaL2eVUKDabTUFBQTp27Ji8vb3NLuemwX03B/fdHNx3c3Dfi2cYhn7//XcFBgbKyYkVDq4X46yi8d+gObjv5uC+m4P7bg7ue/FKMsa66WZMOTk5qW7dumaXUaF5e3vzH5UJuO/m4L6bg/tuDu570ZgpVXoYZ10e/w2ag/tuDu67Objv5uC+F+1qx1h8NAgAAAAAAABTEEwBAAAAAADAFARTsHNzc9O0adPk5uZmdik3Fe67Objv5uC+m4P7DpiL/wbNwX03B/fdHNx3c3DfS8dNt/g5AAAAAAAAKgZmTAEAAAAAAMAUBFMAAAAAAAAwBcEUAAAAAAAATEEwBQAAAAAAAFMQTN3A5s2bp+DgYLm7uysyMlK7d+8utu/58+c1Y8YMhYSEyN3dXWFhYdq0aVOhfr/88oseeeQR1ahRQx4eHmrZsqW++uqrsryMSqe073t+fr6mTJmiBg0ayMPDQyEhIXrmmWfE9xb81/bt29WrVy8FBgbKYrFo3bp1V9xn27ZtatOmjdzc3NSoUSMtXbq0UJ+S/C5vRmVx3xMTE9WuXTt5eXmpdu3a6tOnj1JTU8vmAiqpsvp7v+SFF16QxWLRuHHjSq1m4EbEOMscjLPKF2Ms8zDOMgfjLJMYuCGtXLnScHV1NRYvXmx89913xrBhwwwfHx8jMzOzyP5PPvmkERgYaGzYsME4fPiwMX/+fMPd3d3Yu3evvc/p06eN+vXrG4MHDzZ27dpl/PTTT8bmzZuNH3/8sbwuq8Iri/v+3HPPGTVq1DA++OAD48iRI8bq1auNatWqGa+//np5XVaFt3HjRmPSpEnGmjVrDEnG2rVrL9v/p59+Mjw9PY34+Hjj+++/N+bMmWM4OzsbmzZtsvcp6e/yZlQW9z0mJsZYsmSJ8e233xopKSlGjx49jHr16hnZ2dllfDWVR1nc90t2795tBAcHG61atTLGjh1bNhcA3AAYZ5mDcVb5Y4xlHsZZ5mCcZQ6CqRtU+/btjVGjRtlf5+fnG4GBgUZiYmKR/QMCAoy5c+c6tPXt29fo37+//fVTTz1l3HHHHWVT8A2iLO57z549jUcfffSyffBfV/MPyJNPPmnccsstDm2xsbFGTEyM/XVJf5c3u9K67//rxIkThiTj008/LY0ybziled9///13o3HjxsaWLVuMTp06MWACLoNxljkYZ5mLMZZ5GGeZg3FW+eFRvhtQXl6e9uzZo+joaHubk5OToqOjlZycXOQ+ubm5cnd3d2jz8PDQzp077a/Xr1+viIgI3X///apdu7Zat26tRYsWlc1FVEJldd9vu+02JSUl6YcffpAkffPNN9q5c6e6d+9eBldxc0hOTnb4PUlSTEyM/fd0Lb9LXNmV7ntRsrKyJEm+vr5lWtuN7Grv+6hRo9SzZ89CfQE4YpxlDsZZlQNjLPMwzjIH46zSQTB1Azp16pTy8/Pl5+fn0O7n56eMjIwi94mJidHMmTN16NAhFRQUaMuWLVqzZo3S09PtfX766SctWLBAjRs31ubNm/XYY4/pb3/7m5YtW1am11NZlNV9nzhxoh588EGFhoaqSpUqat26tcaNG6f+/fuX6fXcyDIyMor8PdlsNv3xxx/X9LvElV3pvv+vgoICjRs3TrfffrtatGhRXmXecK7mvq9cuVJ79+5VYmKiGSUClQrjLHMwzqocGGOZh3GWORhnlQ6CKUiSXn/9dTVu3FihoaFydXXV6NGjFRcXJyen//6JFBQUqE2bNnr++efVunVrDR8+XMOGDdPChQtNrLxyu5r7/u677+rtt9/WihUrtHfvXi1btkyvvPIKA1Xc8EaNGqVvv/1WK1euNLuUG9qxY8c0duxYvf3224VmFgAoHYyzzME4Cyge46zywTjr6hBM3YBq1qwpZ2dnZWZmOrRnZmbK39+/yH1q1aqldevWKScnRz///LMOHjyoatWqqWHDhvY+AQEBat68ucN+zZo1U1paWulfRCVUVvf9iSeesH+a17JlSw0YMEDjx48ncb8O/v7+Rf6evL295eHhcU2/S1zZle77n40ePVoffPCBtm7dqrp165ZnmTecK933PXv26MSJE2rTpo1cXFzk4uKiTz/9VLNnz5aLi4vy8/NNqhyomBhnmYNxVuXAGMs8jLPMwTirdBBM3YBcXV3Vtm1bJSUl2dsKCgqUlJSkDh06XHZfd3d31alTRxcuXNC//vUv9e7d2/7e7bffXujrRH/44QfVr1+/dC+gkiqr+3727FmHT/YkydnZWQUFBaV7ATeRDh06OPyeJGnLli3239P1/C5RvCvdd0kyDEOjR4/W2rVr9cknn6hBgwblXeYN50r3vUuXLtq/f79SUlLsW0REhPr376+UlBQ5OzubUTZQYTHOMgfjrMqBMZZ5GGeZg3FWKTF79XWUjZUrVxpubm7G0qVLje+//94YPny44ePjY2RkZBiGYRgDBgwwJk6caO//xRdfGP/617+Mw4cPG9u3bzfuuusuo0GDBsZ//vMfe5/du3cbLi4uxnPPPWccOnTIePvttw1PT0/jrbfeKu/Lq7DK4r4PGjTIqFOnjv1rjNesWWPUrFnTePLJJ8v78iqs33//3fj666+Nr7/+2pBkzJw50/j666+Nn3/+2TAMw5g4caIxYMAAe/9LX+v6xBNPGAcOHDDmzZtX5FcZX+53ibK574899phhtVqNbdu2Genp6fbt7Nmz5X59FVVZ3Pf/xbfFAJfHOMscjLPKH2Ms8zDOMgfjLHMQTN3A5syZY9SrV89wdXU12rdvb3zxxRf29zp16mQMGjTI/nrbtm1Gs2bNDDc3N6NGjRrGgAEDjF9++aXQMf/9738bLVq0MNzc3IzQ0FDjn//8Z3lcSqVS2vfdZrMZY8eONerVq2e4u7sbDRs2NCZNmmTk5uaW1yVVeFu3bjUkFdou3etBgwYZnTp1KrRPeHi44erqajRs2NBYsmRJoeNe7neJsrnvRR1PUpG/n5tVWf29/xkDJuDKGGeZg3FW+WKMZR7GWeZgnGUOi2EYRunPwwIAAAAAAAAujzWmAAAAAAAAYAqCKQAAAAAAAJiCYAoAAAAAAACmIJgCAAAAAACAKQimAAAAAAAAYAqCKQAAAAAAAJiCYAoAAAAAAACmIJgCAAAAAACAKQimAKAELBaL1q1bZ3YZAAAANxTGWMDNi2AKQKUxePBgWSyWQlu3bt3MLg0AAKDSYowFwEwuZhcAACXRrVs3LVmyxKHNzc3NpGoAAABuDIyxAJiFGVMAKhU3Nzf5+/s7bNWrV5d0cQr4ggUL1L17d3l4eKhhw4Z67733HPbfv3+/7rrrLnl4eKhGjRoaPny4srOzHfosXrxYt9xyi9zc3BQQEKDRo0c7vH/q1Cnde++98vT0VOPGjbV+/fqyvWgAAIAyxhgLgFkIpgDcUKZMmaJ+/frpm2++Uf/+/fXggw/qwIEDkqScnBzFxMSoevXq+vLLL7V69Wp9/PHHDoOiBQsWaNSoURo+fLj279+v9evXq1GjRg7nePrpp/XAAw9o37596tGjh/r376/Tp0+X63UCAACUJ8ZYAMqMAQCVxKBBgwxnZ2ejatWqDttzzz1nGIZhSDJGjBjhsE9kZKTx2GOPGYZhGP/85z+N6tWrG9nZ2fb3N2zYYDg5ORkZGRmGYRhGYGCgMWnSpGJrkGRMnjzZ/jo7O9uQZHz44Yeldp0AAADliTEWADOxxhSASqVz585asGCBQ5uvr6/95w4dOji816FDB6WkpEiSDhw4oLCwMFWtWtX+/u23366CggKlpqbKYrHo119/VZcuXS5bQ6tWrew/V61aVd7e3jpx4sS1XhIAAIDpGGMBMAvBFIBKpWrVqoWmfZcWDw+Pq+pXpUoVh9cWi0UFBQVlURIAAEC5YIwFwCysMQXghvLFF18Uet2sWTNJUrNmzfTNN98oJyfH/v5nn30mJycnNW3aVF5eXgoODlZSUlK51gwAAFDRMcYCUFaYMQWgUsnNzVVGRoZDm4uLi2rWrClJWr16tSIiInTHHXfo7bff1u7du/XGG29Ikvr3769p06Zp0KBBmj59uk6ePKkxY8ZowIAB8vPzkyRNnz5dI0aMUO3atdW9e3f9/vvv+uyzzzRmzJjyvVAAAIByxBgLgFkIpgBUKps2bVJAQIBDW9OmTXXw4EFJF7/NZeXKlRo5cqQCAgL0zjvvqHnz5pIkT09Pbd68WWPHjlW7du3k6empfv36aebMmfZjDRo0SOfOndNrr72mCRMmqGbNmrrvvvvK7wIBAABMwBgLgFkshmEYZhcBAKXBYrFo7dq16tOnj9mlAAAA3DAYYwEoS6wxBQAAAAAAAFMQTAEAAAAAAMAUPMoHAAAAAAAAUzBjCgAAAAAAAKYgmAIAAAAAAIApCKYAAAAAAABgCoIpAAAAAAAAmIJgCgAAAAAAAKYgmAIAAAAAAIApCKYAAAAAAABgCoIpAAAAAAAAmOL/ARuvLIO+g1OxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:33,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:33,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:32,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:32,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:32,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:30,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:28,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:23,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:21,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:19,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:19,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:16,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:15,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:11,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:10,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:07,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2d57c6ac6a47199458038d29908517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf4780a50fa4d31b43f7d15876cdc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss          : 0.8191\n",
      "Test Semantic Sim  : 0.3918\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 235 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Degenerative changes._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "Degenerative changes. <|endoftext|>\n",
      "Generated Report :\n",
      " 1st MTP joint soft tissue swelling, Rt. ___________________ r/o gout arthritis at Lt 5th MT head and feet sinde last study in both calcaneus medial aspect of right foot -> RA involvement with os trigonum osteophyte --> TMT interlacar bone type II post-opaque lesion on radiographs (open space) tophi erosions state degenerative change hallux valgare pes planis lateral malleolius naviculocuneiform coalition Hallophalangere flat toe vs knee joints erosion condition\n",
      "\n",
      "--- Example 209 ---\n",
      "Raw Report       :\n",
      "[FINDING       ]_x000D_OA, both feet._x000D__x000D_[CONCLUSION    ]_x000D_OA, both feet._x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   :\n",
      "OA, both feet. <|endoftext|>\n",
      "Generated Report :\n",
      "t. 1st MTP joint, OA ____________________ . No bony abnormality suggested otherwise no significant interval change since last study with soft tissue swelling in medial aspect of right lateral malleolus and pes planum both feet -> erosions at Lt 2nd MT head RA involvement - R/O gout arthritis rheumatoid osteopenia degenerative changes os trigonare type III probable calcification or effusion on radiographs --> ossicle deposition vs cuneiform bone fracture hallux valgar joints (egoarthritis) sesamoids\n",
      "\n",
      "--- Example 3 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "degenerative change. <|endoftext|>\n",
      "Generated Report :\n",
      " . soft tissue swelling in medial portion of right 1st MTP joint. \n",
      " Rt calcaneus, both degenerative change suggested otherwise no significant interval changes since last study (egoarthritis). osteopenia with loose bodies and left knee joints - erosions at base distal tip or pes planum -> suspicious erosion on radiographs old fracture site r/pulmonary os trigonar bone type III , Lt 5th MT head --> RA involvement periarticular malleoluse arthritis state : possible accessory navicula tendonitis diffuse ossicle formation around\n",
      "\n",
      "--- Example 43 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "S/P IF for left medial malleolus._x000D_\n",
      "Degenerative changes._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "S/P IF for left medial malleolus. Degenerative changes. <|endoftext|>\n",
      "Generated Report :\n",
      " . soft tissue swelling in both 1st MTP joint  \n",
      " Lt. foot, type II degenerative change of fibular bone with suggested erosions at Rt 2nd MT head and medial aspect -> RA involvement Degenerosity or erosion on radiographs --- OA subluxation state s/p > probable osteopenia --> gout arthritis rheumatoid os trigonum Joint effusion site : left lateral malleolus right knee , otherwise no significant interval changes since last study (egoarthritis) No definite progression to PP side calcification lesion around\n",
      "\n",
      "--- Example 116 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "Rt. knee soft tissue swelling._x000D_\n",
      "both navicular bone, subchondral cyst._x000D_\n",
      "both 1st MTP joint, osteolytic lesion_x000D_\n",
      "  --> R/O gout_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Rt. knee soft tissue swelling._x000D_\n",
      "both navicular bone, subchondral cyst._x000D_\n",
      "both 1st MTP joint, osteolytic lesion_x000D_\n",
      "  --> R/O gout_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "Rt. knee soft tissue swelling. both navicular bone, subchondral cyst. both 1st MTP joint, osteolytic lesion --> R/O gout <|endoftext|>\n",
      "Generated Report :\n",
      "t. 1st MTP joint, erosions --> R/O gout arthritis ___________________\n",
      "Rx 3rd MT head and feet -> RA involvement at Lt 2nd toe or both ankle joints with degenerative change of OA hallux valgus - left calcaneum in medial aspect os trigonis type II: pes planiculare subluxation site osteopenia periarticular bone soft tissue swelling around right 5th metatarsal bones sesamoid bodies old fracture on radiographs suspicious erosion along the lateral side ossicle protrusion dist\n",
      "\n",
      "--- Example 64 ---\n",
      "Raw Report       :\n",
      "[FINDING       ]_x000D_No significant interval change_x000D__x000D_[CONCLUSION    ]_x000D_No significant interval change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   :\n",
      "No significant interval change <|endoftext|>\n",
      "Generated Report :\n",
      " . soft tissue swelling in medial aspect of right 1st MTP joint.   Possible calcification or erosions at both 5th MT head, Lt 2nd IP joints -> R/O gout arthritis progression with multiple os trigonum and osteopenia degenerative change hallux valgus involvement bone type III --> RA lesions on left talons lateral malleolar spur (Rt) feet , probable old fracture site periarticular cystic lesion around OA ankle rheumatoid process suspicious erosion to the posterior portion of fibula radiographs suggestive inflammatory\n",
      "\n",
      "--- Example 220 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "no bony lesion. <|endoftext|>\n",
      "Generated Report :\n",
      " . soft tissue swelling in both 1st MTP joint. _____________________________________________________________________________ OA with erosions at Rt 2nd MT head, Lt 3rd IP joints and medial malleolus formation of left foot --> RA involvement suggested otherwise no significant interval change since last study or clinical findings No definite correlation to old fracture site --- os naviculare type II degenerative changes on radiographs suggestive ossicle deposition around lateral calcaneum right 5th distal phalanx dorsive hallux valgis pes planar aspect osteopenia state suspicious erosion -> r/\n",
      "\n",
      "--- Example 8 ---\n",
      "Raw Report       :\n",
      " 임시판독 결과 입니다 추후에 판독결과가 수정 될수 있으므로 확인 바랍니다._x000D_\n",
      "------------------------------------------------------------------------ _x000D_\n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "Osteophyte of lateral side of right ankle joint._x000D_\n",
      "Os naviculare, both._x000D_\n",
      "Hallux valgus, left._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      ". Osteophyte of lateral side of right ankle joint. Os naviculare, both. Hallux valgus, left. <|endoftext|>\n",
      "Generated Report :\n",
      "t. calcaneus, both ____________________\n",
      " bony erosion in R/O gout joint of right 1st MTP joints with erosions at Lt 5th MT head and left 2nd PIPE - OA involvement Possible accessory navicular bone type III (L5) -> rheumatoid arthritis RA state degenerative change osteophyte os trigonum --> ossicle formation hallux valgare lateral malleolucent lesion on radiographs or suggested sesamoidal fracture Hallucinogenic changes pes planar tendonitis patella\n",
      "\n",
      "--- Example 146 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "No bony abnormality No bony abnormality <|endoftext|>\n",
      "Generated Report :\n",
      " bony abnormality   No significant interval change since last study. os trigonum, both . no prominent fracture at Lt 1st MTP joint or foot with old erosions in medial aspect of calcaneus and left lateral malleolangeal spur on radiographs --> R/O gout arthritis -> RA involvement rt 5th MT head degenerative changes sesamoid bone type III osteophytes pes planar tendonitis soft tissue swelling around right 2nd DIP joints - OA suspicious erosion > subfibulare lesion adjacent to fibular tip\n",
      "\n",
      "--- Example 233 ---\n",
      "Raw Report       :\n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   :\n",
      "No bony abnormality. No bony abnormality. <|endoftext|>\n",
      "Generated Report :\n",
      "t. 1st MTP joint, degenerative change _______________________________________________________________ bony erosion at R/O gout arthritis of both feet and ankles with rheumatoid osteopenia in medial aspect --> RA involvement or state suggested otherwise no significant interval changes since last study No evidence to consider erosions on radiographs --- probable os naviculare type II soft tissue swelling around calcaneus Lt toe -> possible accessory bone lesions Degenerous ossicle formation adjacent fibula sesamoids protrusion space narrowing lateral malleolum head -- likely subchondral cy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        tok = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img':       img,\n",
    "            'patches':        patches_tensor,\n",
    "            'raw_report':     raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids':      tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'):  left  = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left':  left  = lst\n",
    "                            else:               right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left  = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'(?m)^\\s*-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE|re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'^-+\\s*', '', text)\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text += ' ' + self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function (with .long() casts)\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts  = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids,   batch_first=True, padding_value=tokenizer.pad_token_id).long()\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0).long()\n",
    "\n",
    "    return {\n",
    "        'full_imgs':       imgs,\n",
    "        'patches':         patches,\n",
    "        'input_ids':       ids,\n",
    "        'attention_mask':  masks,\n",
    "        'raw_reports':     [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2,3]) if feats.ndim>2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        # encode global image\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        # encode patches\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B*N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "        # cross-attention\n",
    "        cat, _ = self.attn(\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1)\n",
    "        )\n",
    "        cat = self.norm(cat)\n",
    "\n",
    "        # decode\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers (with .long() casts)\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].long().to(device)\n",
    "        msk  = b['attention_mask'].long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].long().to(device)\n",
    "        msk  = b['attention_mask'].long().to(device)\n",
    "\n",
    "        # teacher-forced loss\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "        # prepare pure BOS\n",
    "        B = imgs.size(0)\n",
    "        bos = torch.full((B, 1), tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "        bos_mask = torch.ones_like(bos)\n",
    "\n",
    "        # generate\n",
    "        with torch.no_grad():\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B*N, C, H, W)\n",
    "            pf = (model.patch_encoder.forward_features(p)\n",
    "                  if hasattr(model.patch_encoder, 'forward_features')\n",
    "                  else model.patch_encoder(p))\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "            cat, _ = model.attn(\n",
    "                torch.cat([g, pf], dim=1),\n",
    "                torch.cat([g, pf], dim=1),\n",
    "                torch.cat([g, pf], dim=1)\n",
    "            )\n",
    "            cat = model.norm(cat)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos,\n",
    "                attention_mask=bos_mask,\n",
    "                encoder_hidden_states=cat,\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "        gt_txt  = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "        all_gen.extend(gen_txt)\n",
    "        all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Similarity\"); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN (with early stopping)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # config\n",
    "    class Cfg: pass\n",
    "    cfg = Cfg()\n",
    "    cfg.DATASET = Cfg()\n",
    "    cfg.DATASET.JSON           = 'final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.USE_RAW        = True\n",
    "    cfg.DATASET.USE_PATCH      = True\n",
    "    cfg.DATASET.REPORT         = True\n",
    "    cfg.DATASET.TARGET_CLASSES = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE        = False\n",
    "    cfg.DATASET.AUGMENT        = False\n",
    "\n",
    "    # tokenizer & device\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # prepare dataset\n",
    "    dataset = FinalSamplesDataset(cfg)\n",
    "    dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "    dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "    print(\"\\nDataset class distribution:\")\n",
    "    for cls, cnt in dist.items():\n",
    "        print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "    # train/val/test split\n",
    "    n       = len(dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "\n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "    print(f\"# train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # model, optimizer, scheduler, scaler\n",
    "    model     = MultiModalModel().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    scaler    = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # early stopping params\n",
    "    patience          = 3\n",
    "    best_val_loss     = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # training loop\n",
    "    num_epochs = 1\n",
    "    train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "        val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "        sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        sems.append(sem)\n",
    "\n",
    "        print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "        print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "        print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss     = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"No improvement for {patience} epochs. Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # load best model\n",
    "    #model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
    "\n",
    "    # plot metrics\n",
    "    plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "    # final test eval\n",
    "    test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "    test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "    print(\"\\n========== TEST RESULTS ==========\")\n",
    "    print(f\"Test Loss          : {test_loss:.4f}\")\n",
    "    print(f\"Test Semantic Sim  : {test_sem:.4f}\")\n",
    "\n",
    "    # random examples\n",
    "    print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "    for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "        ex = test_ds[idx]\n",
    "        raw   = ex['raw_report']\n",
    "        clean = ex['cleaned_report']\n",
    "\n",
    "        fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "        pa = ex['patches'].unsqueeze(0).to(device)\n",
    "\n",
    "        # re-encode features\n",
    "        g = model.global_encoder(fi)\n",
    "        g = model.global_proj(g).unsqueeze(1)\n",
    "        B, N, C, H, W = pa.shape\n",
    "        p = pa.view(B*N, C, H, W)\n",
    "        pf = (model.patch_encoder.forward_features(p)\n",
    "              if hasattr(model.patch_encoder, 'forward_features')\n",
    "              else model.patch_encoder(p))\n",
    "        pf = model._pool(pf)\n",
    "        pf = model.patch_proj(pf).view(B, N, 768)\n",
    "\n",
    "        cat, _ = model.attn(\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1),\n",
    "            torch.cat([g, pf], dim=1)\n",
    "        )\n",
    "        cat = model.norm(cat)\n",
    "\n",
    "        # pure BOS token\n",
    "        bos = torch.full((1, 1), tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "        bos_mask = torch.ones_like(bos)\n",
    "\n",
    "        gen_ids = model.decoder.generate(\n",
    "            input_ids=bos,\n",
    "            attention_mask=bos_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            max_length=120,\n",
    "            do_sample=True,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            temperature=0.5,\n",
    "            repetition_penalty=1.3,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\n--- Example {idx} ---\")\n",
    "        print(f\"Raw Report       :\\n{raw}\")\n",
    "        print(f\"Cleaned Report   :\\n{clean}\")\n",
    "        print(f\"Generated Report :\\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f14f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Class distribution: Counter({'normal': 748, 'uncertain': 620, 'oa': 573, 'gout': 267, 'ra': 115, 'ref.prev': 60, 'oa, ra': 8, 'combination of oa, ra': 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29687/3315090905.py:440: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler=torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_29687/3315090905.py:324: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:37,  1.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:37,  1.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:35,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:34,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:03<00:33,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:32,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:32,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:31,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:31,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:06<00:30,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:30,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:29,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:08<00:29,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:28,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:09<00:27,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:27,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:10<00:27,  1.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:11<00:26,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:25,  1.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:12<00:25,  1.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:13<00:24,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:13<00:24,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:14<00:23,  1.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:22,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:15<00:21,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:16<00:21,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:16<00:20,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:17<00:20,  1.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:18<00:19,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:18<00:19,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:19<00:18,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:19<00:17,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:20<00:16,  1.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:21<00:16,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:21<00:13,  1.89it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:22<00:13,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:22<00:13,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:23<00:13,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:24<00:12,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:24<00:12,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:25<00:11,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:25<00:10,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:26<00:10,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:27<00:09,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:27<00:09,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:28<00:08,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:29<00:08,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:29<00:07,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:30<00:07,  1.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:30<00:06,  1.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:31<00:05,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:32<00:04,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:32<00:04,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:33<00:03,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:33<00:02,  1.89it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:34<00:02,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:34<00:01,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:35<00:01,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:36<00:00,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbe06a18acf410697b7435f77ad6e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1dd11bdbc94407950e0db500c108e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3151, Val Loss: 0.8494, Sem: 0.3538\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:19,  2.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:19,  3.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:18,  3.02it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:17,  3.11it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:16,  3.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:17,  3.16it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:17,  3.10it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:17,  3.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:16,  3.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:16,  3.08it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:03<00:15,  3.07it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:15,  3.10it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:14,  3.14it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:04<00:14,  3.09it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:14,  3.08it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:05<00:14,  3.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:05<00:15,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:05<00:14,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:06<00:14,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:06<00:13,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:06<00:13,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:07<00:13,  2.85it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:07<00:12,  2.97it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:07<00:11,  3.02it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:08<00:11,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:08<00:11,  3.01it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:08<00:10,  3.15it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:09<00:10,  2.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:09<00:10,  2.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:10<00:10,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:10<00:09,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:10<00:09,  2.90it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:11<00:09,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:11<00:08,  2.97it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:11<00:08,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:12<00:08,  2.87it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:12<00:08,  2.87it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:13<00:09,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:13<00:08,  2.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:13<00:07,  2.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:14<00:06,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:14<00:06,  2.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:14<00:05,  3.01it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:14<00:05,  3.03it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:15<00:04,  3.06it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:15<00:04,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:16<00:04,  2.92it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:16<00:04,  2.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:16<00:03,  2.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:16<00:03,  3.07it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:17<00:03,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:17<00:02,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:18<00:02,  2.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:18<00:02,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:18<00:01,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:19<00:01,  2.90it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:19<00:01,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:19<00:00,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:20<00:00,  3.05it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acba0ea1d3e406ead06f1b24eb71a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d929e36ef2a54b448274c8331e0d39b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8055, Val Loss: 0.7506, Sem: 0.3083\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:20,  2.89it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:19,  3.03it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:18,  3.14it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:24,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:20,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:18,  2.92it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:18,  2.88it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:18,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:17,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:21,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:19,  2.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:17,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:16,  2.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:20,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:05<00:18,  2.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:05<00:16,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:06<00:16,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:06<00:15,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:07<00:14,  2.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:07<00:13,  2.93it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:07<00:16,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:08<00:15,  2.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:08<00:14,  2.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:08<00:13,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:09<00:12,  2.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:09<00:14,  2.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:10<00:12,  2.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:10<00:12,  2.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:10<00:11,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:11<00:10,  2.87it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:11<00:09,  2.97it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:11<00:09,  3.06it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:12<00:11,  2.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:12<00:10,  2.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:13<00:09,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:13<00:09,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:13<00:08,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:14<00:07,  2.85it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:14<00:08,  2.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:15<00:07,  2.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:15<00:07,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:15<00:06,  2.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:16<00:05,  2.92it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:16<00:05,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:16<00:04,  3.03it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:17<00:04,  2.99it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:17<00:04,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:17<00:04,  2.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:18<00:03,  2.89it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:18<00:03,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:18<00:02,  3.02it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:18<00:02,  3.10it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:19<00:02,  3.07it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:19<00:01,  3.01it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:20<00:02,  2.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:20<00:01,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:20<00:01,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:21<00:00,  2.85it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:21<00:00,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0a8eebcc644e31a8188f752884b779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e1eb3ffab1415084b58fcfa802236d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6831, Val Loss: 0.7015, Sem: 0.2712\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:21,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:20,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:18,  3.01it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:18,  3.08it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:16,  3.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:01<00:17,  3.09it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:17,  3.06it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:16,  3.10it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:02<00:16,  3.07it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:16,  2.97it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:03<00:16,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:03<00:15,  3.02it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:16,  2.92it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:04<00:15,  2.97it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:15,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:05<00:14,  2.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:05<00:15,  2.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:06<00:15,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:06<00:14,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:06<00:14,  2.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:07<00:14,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:07<00:13,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:07<00:13,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:08<00:12,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:08<00:12,  2.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:08<00:11,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:09<00:10,  3.00it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:09<00:11,  2.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:09<00:10,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:10<00:10,  2.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:10<00:10,  2.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:11<00:10,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:11<00:09,  2.85it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:11<00:08,  2.99it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:11<00:08,  3.02it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:12<00:08,  2.92it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:12<00:08,  2.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:13<00:07,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:13<00:07,  2.89it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:13<00:06,  2.90it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:14<00:06,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:14<00:06,  2.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:14<00:05,  3.00it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:15<00:05,  2.99it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:15<00:04,  3.03it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:15<00:04,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:16<00:04,  2.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:16<00:04,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:16<00:03,  2.93it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:17<00:03,  2.91it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:17<00:03,  2.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:17<00:02,  3.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:18<00:02,  2.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:18<00:02,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:18<00:01,  2.99it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:19<00:01,  3.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:19<00:01,  2.99it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:19<00:00,  3.02it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:20<00:00,  3.09it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ff2fec030d45e6854e2e8e4090e812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b401499c0f63410eb474a146113e35dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6087, Val Loss: 0.6831, Sem: 0.2794\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 447\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 447\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     val_loss,gen_txt,gt_txt\u001b[38;5;241m=\u001b[39mevaluate(model,val_loader,device)\n\u001b[1;32m    449\u001b[0m     sem\u001b[38;5;241m=\u001b[39mcompute_semantic_similarity(gen_txt,gt_txt)\n",
      "Cell \u001b[0;32mIn[6], line 317\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, scaler, device)\u001b[0m\n\u001b[1;32m    315\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    316\u001b[0m total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 317\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_imgs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[6], line 160\u001b[0m, in \u001b[0;36mFinalSamplesDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    157\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    158\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transform(img)\n\u001b[0;32m--> 160\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gen_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft_right_file_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeypoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m pt \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_transform(Image\u001b[38;5;241m.\u001b[39mfromarray(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m patches]\n\u001b[1;32m    162\u001b[0m patches_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(pt,\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m pt \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m34\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m112\u001b[39m,\u001b[38;5;241m112\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 199\u001b[0m, in \u001b[0;36mFinalSamplesDataset._gen_patches\u001b[0;34m(self, paths, kps_dict, crop_size, patch_size)\u001b[0m\n\u001b[1;32m    197\u001b[0m p\u001b[38;5;241m=\u001b[39mpaths[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(p): \u001b[38;5;28;01mreturn\u001b[39;00m pad17([])\u001b[38;5;241m+\u001b[39mpad17([])\n\u001b[0;32m--> 199\u001b[0m arr\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mcvtColor(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m,cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mimread(p) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m pad17([])\u001b[38;5;241m+\u001b[39mpad17([])\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kps_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m): left\u001b[38;5;241m=\u001b[39mextract(arr,kps_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if is_binary and set(target_classes) == {'abnormal','normal'}:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(',')\n",
    "        self.is_binary = len(self.target_classes) == 2 and 'abnormal' in self.target_classes and 'normal' in self.target_classes\n",
    "        if self.is_binary:\n",
    "            self.abnormal_mapping = {'ra':'abnormal','oa':'abnormal','gout':'abnormal','normal':'normal'}\n",
    "        else:\n",
    "            self.abnormal_mapping = None\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path','')\n",
    "            fp = item.get('file_paths',[])\n",
    "            if isinstance(fp, str): fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged,fp,item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i,(merged,fp,item) in enumerate(filtered):\n",
    "            cls = item.get('class','unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls,cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis',''),\n",
    "                'keypoints': item.get('keypoints',{})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced,_,_ = prepare_data(self.data, ['abnormal','normal'], cfg, True)\n",
    "            self.data = {i:e for i,e in enumerate(balanced)}\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'],e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt,0) if pt else torch.zeros(34,3,112,112)\n",
    "\n",
    "        raw = e['diagnosis']\n",
    "        clean = self._clean_report(raw)\n",
    "        tok = tokenizer(clean, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': tok['input_ids'].squeeze(0),\n",
    "            'attention_mask': tok['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200,300), patch_size=(112,112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst=[]\n",
    "            pts=side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x,y,s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s>0:\n",
    "                    x0,y0 = max(x-crop_size[0]//2,0), max(y-crop_size[1]//2,0)\n",
    "                    x1,y1 = min(x+crop_size[0]//2,arr.shape[1]), min(y+crop_size[1]//2,arr.shape[0])\n",
    "                    c = arr[y0:y1,x0:x1]\n",
    "                    if c.size: lst.append(cv2.resize(c,patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black=np.zeros((patch_size[1],patch_size[0],3),np.uint8)\n",
    "            while len(lst)<17: lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left,right=[],[]\n",
    "        if len(paths)==1:\n",
    "            p=paths[0]\n",
    "            if not p or not os.path.exists(p): return pad17([])+pad17([])\n",
    "            arr=cv2.cvtColor(cv2.imread(p),cv2.COLOR_BGR2RGB) if cv2.imread(p) is not None else None\n",
    "            if arr is None: return pad17([])+pad17([])\n",
    "            if kps_dict.get('left'): left=extract(arr,kps_dict['left'])\n",
    "            if kps_dict.get('right'): right=extract(arr,kps_dict['right'])\n",
    "        else:\n",
    "            for side,p in zip(['left','right'],paths):\n",
    "                if p and os.path.exists(p):\n",
    "                    img=cv2.imread(p)\n",
    "                    if img is None: continue\n",
    "                    arr=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "                    if kps_dict.get(side):\n",
    "                        lst=extract(arr,kps_dict[side])\n",
    "                        if side=='left': left=lst\n",
    "                        else: right=lst\n",
    "        if left and not right: right=[cv2.flip(p,1) for p in left]\n",
    "        if right and not left: left=[cv2.flip(p,1) for p in right]\n",
    "        if not left and not right: return pad17([])+pad17([])\n",
    "        return pad17(left)+pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text=unicodedata.normalize('NFKC',text or '')\n",
    "        text=text.replace('_x000D_',' ')\n",
    "        text=re.sub(r'(?m)^\\s*-+\\s*$','',text)\n",
    "        text=re.sub(r'[^\\x00-\\x7F]+',' ',text)\n",
    "        text=re.sub(r'([.!?]){2,}',r'\\1',text)\n",
    "        for tag in ['finding','conclusion','diagnosis']:\n",
    "            text=re.sub(rf'\\[\\s*{tag}\\s*\\]',f'[{tag.upper()}]',text,flags=re.IGNORECASE)\n",
    "        text=re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]',text,flags=re.IGNORECASE)[0]\n",
    "        fm=re.search(r'\\[FINDING\\](.*?)(?=\\[|$)',text,flags=re.IGNORECASE|re.DOTALL)\n",
    "        cm=re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)',text,flags=re.IGNORECASE|re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower()==cm.group(1).strip().lower():\n",
    "            text=re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)','',text,flags=re.IGNORECASE|re.DOTALL)\n",
    "        text=re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]','',text,flags=re.IGNORECASE)\n",
    "        text=re.sub(r'\\s+',' ',text).strip()\n",
    "        text=re.sub(r'^-+\\s*','',text)\n",
    "        if text and not text.endswith(self.eos_token):\n",
    "            text+=' '+self.eos_token\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate fn\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs=torch.stack([b['full_img'] for b in batch])\n",
    "    pts=[b['patches'] for b in batch]\n",
    "    max_p=max(p.shape[0] for p in pts)\n",
    "    pads=[]\n",
    "    for p in pts:\n",
    "        if p.shape[0]<max_p:\n",
    "            pad=torch.zeros((max_p-p.shape[0],*p.shape[1:]))\n",
    "            p=torch.cat([p,pad],dim=0)\n",
    "        pads.append(p)\n",
    "    patches=torch.stack(pads,0)\n",
    "\n",
    "    ids=[b['input_ids'] for b in batch]\n",
    "    masks=[b['attention_mask'] for b in batch]\n",
    "    ids=nn.utils.rnn.pad_sequence(ids,batch_first=True,padding_value=tokenizer.pad_token_id).long()\n",
    "    masks=nn.utils.rnn.pad_sequence(masks,batch_first=True,padding_value=0).long()\n",
    "\n",
    "    return {'full_imgs':imgs,\n",
    "            'patches':patches,\n",
    "            'input_ids':ids,\n",
    "            'attention_mask':masks,\n",
    "            'raw_reports':[b['raw_report'] for b in batch],\n",
    "            'cleaned_reports':[b['cleaned_report'] for b in batch]}\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self,gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder=timm.create_model('swin_base_patch4_window7_224',pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj=nn.Linear(self.global_encoder.num_features,768)\n",
    "\n",
    "        self.patch_encoder=timm.create_model('resnet50',pretrained=True)\n",
    "        self.patch_encoder.fc=nn.Identity()\n",
    "        self.patch_proj=nn.Linear(self.patch_encoder.num_features,768)\n",
    "\n",
    "        self.attn=nn.MultiheadAttention(embed_dim=768,num_heads=8,batch_first=True)\n",
    "        self.norm=nn.LayerNorm(768)\n",
    "        self.decoder=GPT2LMHeadModel.from_pretrained(gpt2_model_name,add_cross_attention=True)\n",
    "\n",
    "    def _pool(self,feats):\n",
    "        return feats.mean(dim=[2,3]) if feats.ndim>2 else feats\n",
    "\n",
    "    def forward(self,imgs,patches,input_ids,attention_mask,decoder_labels=None):\n",
    "        g=self.global_encoder(imgs)\n",
    "        g=self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B,N,C,H,W=patches.shape\n",
    "        p=patches.view(B*N,C,H,W)\n",
    "        pf=(self.patch_encoder.forward_features(p)\n",
    "            if hasattr(self.patch_encoder,'forward_features')\n",
    "            else self.patch_encoder(p))\n",
    "        pf=self._pool(pf)\n",
    "        pf=self.patch_proj(pf).view(B,N,768)\n",
    "\n",
    "        cat,_=self.attn(torch.cat([g,pf],dim=1),\n",
    "                       torch.cat([g,pf],dim=1),\n",
    "                       torch.cat([g,pf],dim=1))\n",
    "        cat=self.norm(cat)\n",
    "\n",
    "        return self.decoder(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            encoder_hidden_states=cat,\n",
    "                            labels=decoder_labels)\n",
    "\n",
    "# =============================================================================\n",
    "# Train/Eval\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model,loader,optimizer,scaler,device):\n",
    "    model.train()\n",
    "    total=0.0\n",
    "    for b in tqdm(loader,desc=\"Training\",leave=False):\n",
    "        imgs=b['full_imgs'].to(device)\n",
    "        pts=b['patches'].to(device)\n",
    "        ids=b['input_ids'].to(device).long()\n",
    "        msk=b['attention_mask'].to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out=model(imgs,pts,ids,msk,decoder_labels=ids)\n",
    "            loss=out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total+=loss.item()\n",
    "    return total/len(loader)\n",
    "\n",
    "def evaluate(model,loader,device):\n",
    "    model.eval()\n",
    "    total=0.0\n",
    "    all_gen,all_gt=[],[]\n",
    "    for b in tqdm(loader,desc=\"Evaluating\",leave=False):\n",
    "        imgs=b['full_imgs'].to(device)\n",
    "        pts=b['patches'].to(device)\n",
    "        ids=b['input_ids'].to(device).long()\n",
    "        msk=b['attention_mask'].to(device).long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out=model(imgs,pts,ids,msk,decoder_labels=ids)\n",
    "            total+=out.loss.item()\n",
    "\n",
    "        # --- start generation with BOS ----\n",
    "        B,N,C,H,W=pts.shape\n",
    "        bos=torch.full((B,1),tokenizer.bos_token_id,dtype=torch.long,device=device)\n",
    "        bos_mask=torch.ones_like(bos)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            g=model.global_encoder(imgs)\n",
    "            g=model.global_proj(g).unsqueeze(1)\n",
    "            p=pts.view(B*N,C,H,W)\n",
    "            pf=(model.patch_encoder.forward_features(p)\n",
    "                if hasattr(model.patch_encoder,'forward_features')\n",
    "                else model.patch_encoder(p))\n",
    "            pf=model._pool(pf)\n",
    "            pf=model.patch_proj(pf).view(B,N,768)\n",
    "            cat,_=model.attn(torch.cat([g,pf],dim=1),\n",
    "                             torch.cat([g,pf],dim=1),\n",
    "                             torch.cat([g,pf],dim=1))\n",
    "            cat=model.norm(cat)\n",
    "\n",
    "            gen_ids=model.decoder.generate(\n",
    "                input_ids=bos,\n",
    "                attention_mask=bos_mask,\n",
    "                encoder_hidden_states=cat,\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        gen_txt=[tokenizer.decode(g_,skip_special_tokens=True) for g_ in gen_ids]\n",
    "        gt_txt=[tokenizer.decode(i_,skip_special_tokens=True) for i_ in ids]\n",
    "        all_gen+=gen_txt\n",
    "        all_gt+=gt_txt\n",
    "\n",
    "    return total/len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen,gt):\n",
    "    stm=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1=stm.encode(gen,convert_to_tensor=True)\n",
    "    e2=stm.encode(gt,convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1,e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses,val_losses,sems):\n",
    "    epochs=range(1,len(train_losses)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs,train_losses,label=\"Train Loss\")\n",
    "    plt.plot(epochs,val_losses,label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\");plt.ylabel(\"Loss\");plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs,sems,label=\"Semantic Sim\")\n",
    "    plt.xlabel(\"Epoch\");plt.ylabel(\"Sim\");plt.legend()\n",
    "    plt.tight_layout();plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # config\n",
    "    class Cfg: pass\n",
    "    cfg=Cfg();cfg.DATASET=Cfg()\n",
    "    cfg.DATASET.JSON='final_samples_both_only_v2.json'\n",
    "    cfg.DATASET.USE_RAW=True;cfg.DATASET.USE_PATCH=True\n",
    "    cfg.DATASET.REPORT=True\n",
    "    cfg.DATASET.TARGET_CLASSES=['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "    cfg.DATASET.BALANCE=False;cfg.DATASET.AUGMENT=False\n",
    "\n",
    "    tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "    tokenizer.bos_token=tokenizer.eos_token  # define BOS\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\",device)\n",
    "\n",
    "    dataset=FinalSamplesDataset(cfg)\n",
    "    dataset.eos_token=tokenizer.eos_token\n",
    "    dist=Counter(e['class_label'] for e in dataset.data.values())\n",
    "    print(\"Class distribution:\",dist)\n",
    "\n",
    "    n=len(dataset)\n",
    "    n_train=int(0.8*n);n_val=int(0.1*n);n_test=n-n_train-n_val\n",
    "    train_ds,val_ds,test_ds=random_split(dataset,[n_train,n_val,n_test])\n",
    "    train_loader=DataLoader(train_ds,batch_size=4,shuffle=True,collate_fn=collate_fn)\n",
    "    val_loader=DataLoader(val_ds,batch_size=4,shuffle=False,collate_fn=collate_fn)\n",
    "    test_loader=DataLoader(test_ds,batch_size=4,shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "    model=MultiModalModel().to(device)\n",
    "    optimizer=optim.AdamW(model.parameters(),lr=5e-5)\n",
    "    scheduler=optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=10,T_mult=2)\n",
    "    scaler=torch.cuda.amp.GradScaler()\n",
    "\n",
    "    patience=3;best_val_loss=float('inf');epochs_no_improve=0\n",
    "    num_epochs=10;train_losses=[];val_losses=[];sems=[]\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        train_loss=train_epoch(model,train_loader,optimizer,scaler,device)\n",
    "        val_loss,gen_txt,gt_txt=evaluate(model,val_loader,device)\n",
    "        sem=compute_semantic_similarity(gen_txt,gt_txt)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        sems.append(sem)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Sem: {sem:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        if val_loss<best_val_loss:\n",
    "            best_val_loss=val_loss;epochs_no_improve=0\n",
    "            torch.save(model.state_dict(),'best_model.pt')\n",
    "        else:\n",
    "            epochs_no_improve+=1\n",
    "            if epochs_no_improve>=patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    #model.load_state_dict(torch.load('best_model.pt',map_location=device))\n",
    "    plot_metrics(train_losses,val_losses,sems)\n",
    "\n",
    "    test_loss,test_gen,test_gt=evaluate(model,test_loader,device)\n",
    "    test_sem=compute_semantic_similarity(test_gen,test_gt)\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}, Test Sem: {test_sem:.4f}\")\n",
    "\n",
    "    print(\"\\nRandom Examples:\")\n",
    "    for idx in random.sample(range(len(test_ds)),min(10,len(test_ds))):\n",
    "        ex=test_ds[idx]\n",
    "        raw,clean=ex['raw_report'],ex['cleaned_report']\n",
    "        fi=ex['full_img'].unsqueeze(0).to(device)\n",
    "        pa=ex['patches'].unsqueeze(0).to(device)\n",
    "\n",
    "        # re-encode\n",
    "        g=model.global_encoder(fi);g=model.global_proj(g).unsqueeze(1)\n",
    "        B,N,C,H,W=pa.shape\n",
    "        p=pa.view(B*N,C,H,W)\n",
    "        pf=(model.patch_encoder.forward_features(p)\n",
    "            if hasattr(model.patch_encoder,'forward_features')\n",
    "            else model.patch_encoder(p))\n",
    "        pf=model._pool(pf)\n",
    "        pf=model.patch_proj(pf).view(B,N,768)\n",
    "        cat,_=model.attn(torch.cat([g,pf],dim=1),\n",
    "                         torch.cat([g,pf],dim=1),\n",
    "                         torch.cat([g,pf],dim=1))\n",
    "        cat=model.norm(cat)\n",
    "\n",
    "        bos=torch.full((1,1),tokenizer.bos_token_id,dtype=torch.long,device=device)\n",
    "        bos_mask=torch.ones_like(bos)\n",
    "        gen_ids=model.decoder.generate(\n",
    "            input_ids=bos,\n",
    "            attention_mask=bos_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            max_length=120,\n",
    "            do_sample=True,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            temperature=0.5,\n",
    "            repetition_penalty=1.3,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        gen=tokenizer.decode(gen_ids[0],skip_special_tokens=True)\n",
    "        print(f\"\\n---Example {idx}---\")\n",
    "        print(\"Raw:\",raw)\n",
    "        print(\"Clean:\",clean)\n",
    "        print(\"Gen:\",gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0771b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution: Counter({'normal': 748, 'uncertain': 620, 'oa': 573, 'gout': 267, 'ra': 115, 'ref.prev': 60})\n",
      "Dataset initialized with 2383 samples\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_29687/121159203.py:441: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/477 [00:00<?, ?it/s]/tmp/ipykernel_29687/121159203.py:327: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████| 477/477 [02:23<00:00,  3.31it/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:21,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:21,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:22,  2.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:20,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:19,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:19,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:19,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:18,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:17,  2.89it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:19,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:19,  2.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:18,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:18,  2.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:20,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:05<00:18,  2.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:06<00:17,  2.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:06<00:16,  2.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:06<00:15,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:07<00:18,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:07<00:16,  2.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:08<00:15,  2.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:08<00:14,  2.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:08<00:14,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:09<00:13,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:09<00:13,  2.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:10<00:12,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:10<00:12,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:10<00:12,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:11<00:11,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:11<00:10,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:11<00:10,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:12<00:10,  2.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:12<00:10,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:13<00:11,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:13<00:12,  2.05it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:14<00:11,  2.17it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:14<00:10,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:14<00:09,  2.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:15<00:09,  2.13it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:15<00:08,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:16<00:07,  2.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:16<00:07,  2.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:17<00:06,  2.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:17<00:06,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:17<00:05,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:18<00:05,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:18<00:04,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:18<00:04,  2.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:19<00:04,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:19<00:04,  2.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:20<00:03,  2.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:20<00:03,  2.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:20<00:02,  2.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:21<00:02,  2.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:21<00:02,  2.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:22<00:01,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:22<00:01,  2.00it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:23<00:00,  2.15it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:23<00:00,  1.95it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 60/60 [00:24<00:00,  2.48it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abbfe00efed42c3ae1b604c8210c018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d294cba3024d98a1d3b6efe1efbffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss=0.8801 | ValLoss=0.5529 | ValSem=0.4841\n",
      "Saved test_best_model.pt\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 477/477 [02:24<00:00,  3.31it/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:20,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:29,  1.96it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:25,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:23,  2.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:21,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:19,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:20,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:20,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:20,  2.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:21,  2.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:21,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:20,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:19,  2.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:18,  2.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:17,  2.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:06<00:16,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:06<00:16,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:07<00:15,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:07<00:14,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:07<00:15,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:08<00:14,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:08<00:17,  2.19it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:09<00:15,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:09<00:15,  2.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:10<00:14,  2.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:10<00:14,  2.42it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:10<00:13,  2.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:11<00:13,  2.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:11<00:12,  2.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:12<00:12,  2.46it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:12<00:11,  2.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:12<00:11,  2.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:13<00:10,  2.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:13<00:10,  2.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:14<00:10,  2.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:14<00:10,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:15<00:09,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:15<00:08,  2.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:15<00:08,  2.52it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:16<00:07,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:16<00:08,  2.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:17<00:07,  2.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:17<00:07,  2.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:17<00:06,  2.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:18<00:05,  2.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:18<00:06,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:19<00:05,  2.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:19<00:04,  2.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:20<00:04,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:20<00:04,  2.38it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:20<00:03,  2.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:21<00:03,  2.40it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:21<00:03,  2.13it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:22<00:02,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:22<00:02,  2.39it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:23<00:01,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:23<00:01,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:23<00:00,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:24<00:00,  2.04it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 60/60 [00:24<00:00,  2.42it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5c7b83ef224def95da336a8d337157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819ed91f488d4a06a3b59bb617aab88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss=0.5220 | ValLoss=0.4986 | ValSem=0.4713\n",
      "Saved test_best_model.pt\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 477/477 [02:25<00:00,  3.28it/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:19,  3.03it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:26,  2.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:23,  2.44it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:21,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:24,  2.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:24,  2.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:22,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:21,  2.47it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:23,  2.13it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:23,  2.09it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:22,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:22,  2.17it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:19,  2.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:06<00:18,  2.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:17,  2.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:06<00:16,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:07<00:15,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:07<00:16,  2.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:07<00:15,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:08<00:15,  2.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:08<00:14,  2.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:08<00:14,  2.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:09<00:13,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:09<00:13,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:10<00:12,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:10<00:11,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:10<00:11,  2.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:11<00:12,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:11<00:11,  2.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:11<00:11,  2.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:12<00:10,  2.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:12<00:10,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:13<00:10,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:13<00:09,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:13<00:09,  2.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:14<00:09,  2.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:14<00:08,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:14<00:08,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:15<00:07,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:15<00:07,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:15<00:06,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:16<00:06,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:16<00:06,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:17<00:05,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:17<00:05,  2.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:17<00:04,  2.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:18<00:04,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:18<00:04,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:18<00:04,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:19<00:03,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:19<00:03,  2.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:20<00:03,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:20<00:02,  2.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:20<00:02,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:21<00:01,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:21<00:01,  2.19it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:22<00:01,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:22<00:00,  2.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:22<00:00,  2.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 60/60 [00:23<00:00,  2.59it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c5493441c14d048abdb77b98b79958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffd281238014866b375f2f84f27bddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss=0.4575 | ValLoss=0.4745 | ValSem=0.4737\n",
      "Saved test_best_model.pt\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 477/477 [02:23<00:00,  3.33it/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:20,  2.94it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:21,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:20,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:19,  2.88it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:19,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:19,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:19,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:18,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:18,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:18,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:22,  2.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:20,  2.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:18,  2.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:18,  2.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:05<00:17,  2.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:06<00:16,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:06<00:19,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:07<00:17,  2.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:07<00:15,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:07<00:14,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:08<00:15,  2.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:08<00:14,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:08<00:14,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:09<00:13,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:09<00:12,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:10<00:12,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:10<00:12,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:10<00:11,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:11<00:11,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:11<00:11,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:11<00:11,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:12<00:10,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:12<00:10,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:13<00:09,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:13<00:09,  2.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:13<00:08,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:14<00:08,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:14<00:07,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:14<00:07,  2.88it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:15<00:06,  2.98it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:15<00:06,  2.87it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:16<00:07,  2.43it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:16<00:06,  2.50it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:16<00:06,  2.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:17<00:05,  2.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:17<00:06,  2.19it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:18<00:05,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:18<00:05,  2.37it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:18<00:04,  2.48it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:19<00:03,  2.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:19<00:03,  2.57it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:20<00:03,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:20<00:02,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:20<00:02,  2.53it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:21<00:01,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:21<00:01,  2.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:21<00:01,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:22<00:00,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:22<00:00,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 60/60 [00:22<00:00,  2.63it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef85f1abe6b4e80ab8eeb1e42b443ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85465efda234f749cc303d0b1730681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss=0.4557 | ValLoss=0.4737 | ValSem=0.4145\n",
      "Saved test_best_model.pt\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 477/477 [02:22<00:00,  3.35it/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:21,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:21,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:21,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:20,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:19,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:19,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:19,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:18,  2.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:18,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:18,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:18,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:18,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:17,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:16,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:05<00:16,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:05<00:15,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:06<00:15,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:06<00:14,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:06<00:14,  2.86it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:07<00:13,  2.90it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:07<00:13,  2.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:07<00:13,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:08<00:13,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:08<00:12,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:09<00:12,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:09<00:11,  2.85it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:09<00:11,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:10<00:11,  2.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:10<00:11,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:10<00:10,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:11<00:10,  2.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:11<00:10,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:11<00:09,  2.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:12<00:09,  2.88it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:12<00:08,  2.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:12<00:08,  2.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:13<00:08,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:13<00:07,  2.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:14<00:07,  2.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:14<00:07,  2.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:14<00:07,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:15<00:06,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:15<00:06,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:16<00:06,  2.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:16<00:05,  2.51it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:16<00:05,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:17<00:04,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:17<00:04,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:17<00:04,  2.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:18<00:03,  2.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:18<00:03,  2.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:18<00:02,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:19<00:02,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:19<00:02,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:20<00:01,  2.84it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:20<00:01,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:20<00:01,  2.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:21<00:00,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:21<00:00,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 60/60 [00:21<00:00,  2.76it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2121612e41542fa8bf10979a3205dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28df9ef1941342739269a288d9d089b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainLoss=0.3761 | ValLoss=0.4626 | ValSem=0.4520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29687/121159203.py:462: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"test_best_model.pt\", map_location=device))\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test_best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:21,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:22,  2.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:21,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:21,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:01<00:20,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:19,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:02<00:18,  2.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:02<00:18,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:18,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:18,  2.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:17,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:17,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:17,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:16,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:05<00:16,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:05<00:15,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:06<00:16,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:06<00:15,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:06<00:15,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:07<00:14,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:07<00:14,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:08<00:14,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:08<00:13,  2.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:08<00:13,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:09<00:13,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:09<00:12,  2.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:09<00:11,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:10<00:11,  2.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:10<00:11,  2.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:11<00:11,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:11<00:10,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:11<00:10,  2.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:12<00:10,  2.54it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:12<00:10,  2.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:12<00:09,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:13<00:09,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:13<00:09,  2.49it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:14<00:08,  2.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:14<00:07,  2.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:14<00:07,  2.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:15<00:08,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:15<00:07,  2.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:16<00:07,  2.41it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:16<00:06,  2.55it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:16<00:05,  2.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:17<00:05,  2.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:17<00:04,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:18<00:04,  2.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:18<00:03,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:18<00:03,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:19<00:03,  2.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:19<00:02,  2.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:19<00:02,  2.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:20<00:02,  2.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:20<00:01,  2.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:20<00:01,  2.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:21<00:01,  2.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:21<00:00,  2.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:22<00:00,  2.90it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating: 100%|██████████| 60/60 [00:22<00:00,  2.69it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4347591ad0424c79a27f781324f90b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299b9c622f284585bd95a1a453d2a650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestLoss=0.4482 | TestSem=0.4618\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "class Cfg: pass\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "# Dataset settings\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON            = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.TARGET_CLASSES  = ['ra','oa','gout','normal','uncertain','ref.prev']\n",
    "cfg.DATASET.BINARY          = False    # map ra/oa/gout → abnormal if True\n",
    "cfg.DATASET.BALANCE         = False\n",
    "cfg.DATASET.AUGMENT         = False\n",
    "cfg.DATASET.USE_RAW         = True\n",
    "cfg.DATASET.USE_PATCH       = True\n",
    "\n",
    "# Training settings\n",
    "cfg.TRAIN = Cfg()\n",
    "cfg.TRAIN.BATCH_SIZE        = 4\n",
    "cfg.TRAIN.EPOCHS            = 5\n",
    "cfg.TRAIN.LR                = 5e-5\n",
    "cfg.TRAIN.PATIENCE          = 3\n",
    "cfg.TRAIN.SCHEDULER         = Cfg()\n",
    "cfg.TRAIN.SCHEDULER.TYPE    = 'CosineWarmRestarts'\n",
    "cfg.TRAIN.SCHEDULER.T_0     = 10\n",
    "cfg.TRAIN.SCHEDULER.T_MULT  = 2\n",
    "\n",
    "# Model settings\n",
    "cfg.MODEL = Cfg()\n",
    "cfg.MODEL.VISION_BACKBONE   = 'swin_base_patch4_window7_224'\n",
    "cfg.MODEL.PATCH_BACKBONE    = 'resnet50'\n",
    "cfg.MODEL.TEXT_BACKBONE     = 'gpt2'\n",
    "cfg.MODEL.EMBED_DIM         = 768\n",
    "cfg.MODEL.NUM_HEADS         = 8\n",
    "\n",
    "# =============================================================================\n",
    "# Global tokenizer\n",
    "# =============================================================================\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(cfg.MODEL.TEXT_BACKBONE)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.bos_token = tokenizer.eos_token\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "def count_labels(samples, target_classes):\n",
    "    counts = Counter()\n",
    "    by_class = defaultdict(list)\n",
    "    for s in samples:\n",
    "        lbl = s['class_label']\n",
    "        if lbl in target_classes:\n",
    "            counts[lbl] += 1\n",
    "            by_class[lbl].append(s)\n",
    "    return counts, by_class\n",
    "\n",
    "def prepare_data(samples, target_classes, balance=False, augment=False, seed=42):\n",
    "    random.seed(seed)\n",
    "    counts, by_class = count_labels(samples, target_classes)\n",
    "    print(f\"Original distribution: {counts}\")\n",
    "\n",
    "    if balance:\n",
    "        min_ct = min(counts[c] for c in target_classes)\n",
    "        balanced = []\n",
    "        for c in target_classes:\n",
    "            balanced += random.sample(by_class[c], min_ct)\n",
    "        samples = balanced\n",
    "        counts = Counter({c: min_ct for c in target_classes})\n",
    "        print(f\"Balanced to {min_ct} per class\")\n",
    "    if augment:\n",
    "        samples = samples * 2\n",
    "        print(\"Doubled samples (augmentation)\")\n",
    "    return samples, counts\n",
    "\n",
    "def clean_report(text, eos_token):\n",
    "    txt = text or \"\"\n",
    "    txt = txt.replace('_x000D_', ' ')\n",
    "    txt = ' '.join(txt.split())\n",
    "    txt = ''.join(ch for ch in txt if ord(ch) < 128)\n",
    "    if txt and not txt.endswith(eos_token):\n",
    "        txt += eos_token\n",
    "    return txt\n",
    "\n",
    "def gen_patches(image_paths, keypoints, crop_size=(200,300), patch_size=(112,112)):\n",
    "    def extract(arr, kps_list):\n",
    "        if not kps_list:\n",
    "            return []\n",
    "        out = []\n",
    "        pts = kps_list[0].get('keypoints', [])\n",
    "        for i in range(17):\n",
    "            try:\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "            except (IndexError, TypeError):\n",
    "                break\n",
    "            if s > 0:\n",
    "                x0 = max(x - crop_size[0]//2, 0)\n",
    "                y0 = max(y - crop_size[1]//2, 0)\n",
    "                x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                crop = arr[y0:y1, x0:x1]\n",
    "                if crop.size:\n",
    "                    out.append(cv2.resize(crop, patch_size))\n",
    "        return out\n",
    "\n",
    "    def pad17(lst):\n",
    "        black = np.zeros((*patch_size, 3), dtype=np.uint8)\n",
    "        while len(lst) < 17:\n",
    "            lst.append(black)\n",
    "        return lst[:17]\n",
    "\n",
    "    left, right = [], []\n",
    "    if len(image_paths) == 1:\n",
    "        p = image_paths[0]\n",
    "        if p.exists():\n",
    "            img = cv2.imread(str(p))\n",
    "            if img is not None:\n",
    "                arr = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                left  = extract(arr, keypoints.get('left', []))\n",
    "                right = extract(arr, keypoints.get('right', []))\n",
    "    else:\n",
    "        for side, p in zip(['left','right'], image_paths):\n",
    "            if p.exists():\n",
    "                img = cv2.imread(str(p))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                arr = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                patches = extract(arr, keypoints.get(side, []))\n",
    "                if side == 'left':\n",
    "                    left = patches\n",
    "                else:\n",
    "                    right = patches\n",
    "\n",
    "    if left and not right:\n",
    "        right = [cv2.flip(x, 1) for x in left]\n",
    "    if right and not left:\n",
    "        left = [cv2.flip(x, 1) for x in right]\n",
    "    if not left and not right:\n",
    "        return np.zeros((34, *patch_size, 3), dtype=np.uint8)\n",
    "\n",
    "    all_p = pad17(left) + pad17(right)\n",
    "    return np.stack(all_p, axis=0)\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class RadiologyDataset(Dataset):\n",
    "    def __init__(self, cfg, tokenizer, image_tf, patch_tf):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos = tokenizer.eos_token\n",
    "        self.image_tf = image_tf\n",
    "        self.patch_tf = patch_tf\n",
    "\n",
    "        raw = json.loads(Path(cfg.DATASET.JSON).read_text())\n",
    "        samples = []\n",
    "        for item in raw:\n",
    "            paths = [Path(item.get('merged_image_path',''))] + [\n",
    "                Path(p) for p in (item.get('file_paths') or [])]\n",
    "            paths = [p for p in paths if p.exists()]\n",
    "            if not paths:\n",
    "                continue\n",
    "\n",
    "            cls = item.get('class','').lower()\n",
    "            if cfg.DATASET.BINARY:\n",
    "                cls = 'abnormal' if cls in ('ra','oa','gout') else 'normal'\n",
    "            if cls not in cfg.DATASET.TARGET_CLASSES:\n",
    "                continue\n",
    "\n",
    "            samples.append({\n",
    "                'image_paths': paths,\n",
    "                'class_label': cls,\n",
    "                'diagnosis':   item.get('diagnosis',''),\n",
    "                'keypoints':   item.get('keypoints',{})\n",
    "            })\n",
    "\n",
    "        self.samples, self.class_counts = prepare_data(\n",
    "            samples,\n",
    "            cfg.DATASET.TARGET_CLASSES,\n",
    "            balance=cfg.DATASET.BALANCE,\n",
    "            augment=cfg.DATASET.AUGMENT\n",
    "        )\n",
    "        print(f\"Dataset initialized with {len(self.samples)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        img = Image.open(s['image_paths'][0]).convert('RGB')\n",
    "        img = self.image_tf(img)\n",
    "\n",
    "        raw_patches = gen_patches(s['image_paths'], s['keypoints'])\n",
    "        pt_list = [self.patch_tf(Image.fromarray(p)) for p in raw_patches]\n",
    "        patches = torch.stack(pt_list, 0)\n",
    "\n",
    "        clean = clean_report(s['diagnosis'], self.eos)\n",
    "        tok = self.tokenizer(\n",
    "            clean, truncation=True, max_length=512, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'full_img':      img,\n",
    "            'patches':       patches,\n",
    "            'input_ids':     tok.input_ids.squeeze(0),\n",
    "            'attention_mask':tok.attention_mask.squeeze(0),\n",
    "            'raw':           s['diagnosis'],\n",
    "            'clean':         clean\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts  = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids   = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids   = nn.utils.rnn.pad_sequence(ids, batch_first=True,\n",
    "                                      padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True,\n",
    "                                      padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs':      imgs,\n",
    "        'patches':        patches,\n",
    "        'input_ids':      ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports':    [b['raw']   for b in batch],\n",
    "        'clean_reports':  [b['clean'] for b in batch],\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        D = cfg.MODEL.EMBED_DIM\n",
    "\n",
    "        self.global_encoder = timm.create_model(\n",
    "            cfg.MODEL.VISION_BACKBONE, pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(\n",
    "            self.global_encoder.num_features, D)\n",
    "\n",
    "        self.patch_encoder = timm.create_model(\n",
    "            cfg.MODEL.PATCH_BACKBONE, pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(\n",
    "            self.patch_encoder.num_features, D)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=D, num_heads=cfg.MODEL.NUM_HEADS,\n",
    "            batch_first=True)\n",
    "        self.norm = nn.LayerNorm(D)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            cfg.MODEL.TEXT_BACKBONE, add_cross_attention=True)\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, labels=None):\n",
    "        # Global features\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)  # (B,1,D)\n",
    "\n",
    "        # Patch features\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B*N, C, H, W)\n",
    "        feats = (self.patch_encoder.forward_features(p)\n",
    "                 if hasattr(self.patch_encoder, 'forward_features')\n",
    "                 else self.patch_encoder(p))\n",
    "        feats = feats.mean(dim=[2,3])\n",
    "        p = self.patch_proj(feats).view(B, N, -1)\n",
    "\n",
    "        # Cross-attention\n",
    "        cat, _ = self.attn(torch.cat([g,p],1),\n",
    "                           torch.cat([g,p],1),\n",
    "                           torch.cat([g,p],1))\n",
    "        cat = self.norm(cat)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=cat,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Training & Evaluation\n",
    "# =============================================================================\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for b in tqdm(loader, desc=\"Training\"):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts  = b['patches'].to(device)\n",
    "        ids  = b['input_ids'].to(device)\n",
    "        msk  = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, labels=ids)\n",
    "        scaler.scale(out.loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += out.loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss, gens, gts = 0, [], []\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm(loader, desc=\"Evaluating\"):\n",
    "            imgs = b['full_imgs'].to(device)\n",
    "            pts  = b['patches'].to(device)\n",
    "            ids  = b['input_ids'].to(device)\n",
    "            msk  = b['attention_mask'].to(device)\n",
    "\n",
    "            # Compute loss\n",
    "            out = model(imgs, pts, ids, msk, labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # Prepare encoder states separately for generation\n",
    "            B = imgs.size(0)\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            # Patch features + projection\n",
    "            p = pts.view(B*pts.size(1), *pts.shape[2:])\n",
    "            pf = (model.patch_encoder.forward_features(p)\n",
    "                  if hasattr(model.patch_encoder,'forward_features')\n",
    "                  else model.patch_encoder(p))\n",
    "            pf = pf.mean(dim=[2,3])\n",
    "            pf = model.patch_proj(pf).view(B, pts.size(1), -1)\n",
    "\n",
    "            cat, _ = model.attn(torch.cat([g,pf],1),\n",
    "                                torch.cat([g,pf],1),\n",
    "                                torch.cat([g,pf],1))\n",
    "            cat = model.norm(cat)\n",
    "\n",
    "            # Generate\n",
    "            bos   = torch.full((B,1), tokenizer.bos_token_id, device=device)\n",
    "            bos_m = torch.ones_like(bos)\n",
    "            gen = model.decoder.generate(\n",
    "                input_ids=bos,\n",
    "                attention_mask=bos_m,\n",
    "                encoder_hidden_states=cat,\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "            gens += [tokenizer.decode(x, skip_special_tokens=True) for x in gen]\n",
    "            gts  += [tokenizer.decode(x, skip_special_tokens=True) for x in ids]\n",
    "\n",
    "    e1 = stm.encode(gens, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gts,  convert_to_tensor=True)\n",
    "    sem = nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "    return total_loss / len(loader), sem\n",
    "\n",
    "# =============================================================================\n",
    "# Main\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Transforms\n",
    "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    img_tf = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    pch_tf = transforms.Compose([\n",
    "        transforms.Resize((112,112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # Dataset & loaders\n",
    "    ds = RadiologyDataset(cfg, tokenizer, img_tf, pch_tf)\n",
    "    n = len(ds)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        ds, [n_train, n_val, n_test],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "        shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "        shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "        shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, scheduler, scaler\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = MultiModalModel(cfg).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.TRAIN.LR)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=cfg.TRAIN.SCHEDULER.T_0,\n",
    "        T_mult=cfg.TRAIN.SCHEDULER.T_MULT\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_val, no_imp = float('inf'), 0\n",
    "    for epoch in range(1, cfg.TRAIN.EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{cfg.TRAIN.EPOCHS}\")\n",
    "        tr_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "        val_loss, val_sem = evaluate(model, val_loader, device)\n",
    "        print(f\"TrainLoss={tr_loss:.4f} | ValLoss={val_loss:.4f} | ValSem={val_sem:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        if val_loss < best_val:\n",
    "            best_val, no_imp = val_loss, 0\n",
    "            torch.save(model.state_dict(), \"test_best_model.pt\")\n",
    "            print(\"Saved test_best_model.pt\")\n",
    "        else:\n",
    "            no_imp += 1\n",
    "            if no_imp >= cfg.TRAIN.PATIENCE:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Final test\n",
    "    model.load_state_dict(torch.load(\"test_best_model.pt\", map_location=device))\n",
    "    test_loss, test_sem = evaluate(model, test_loader, device)\n",
    "    print(f\"TestLoss={test_loss:.4f} | TestSem={test_sem:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea5a5e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 163 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] degenerative change. [ Conclusion ] degenerative change. [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      " Finding ] No bony abnormality [ Diagnosis ] No bony abnormality [ Recommend ]\n",
      "\n",
      "--- Example 28 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "diffuse osteopenia _x000D_\n",
      "mild degenerative change_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "diffuse osteopenia _x000D_\n",
      "mild degenerative change_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] diffuse osteopenia mild degenerative change [ Conclusion ] diffuse osteopenia mild degenerative change [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      " Finding ] no significant bony lesion on radiographs. [ Conclusion ] no significant bony lesion on radiographs. [ Recommend ]\n",
      "\n",
      "--- Example 6 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both wrist, R/O RA_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both wrist, R/O RA_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] both wrist, R/O RA [ Conclusion ] both wrist, R/O RA [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      "FINDING ] - [CONCLUSION ] degenerative change [RECOMMENDATION] -\n",
      "\n",
      "--- Example 189 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_No significant interval change_x000D__x000D_[CONCLUSION    ]_x000D_No significant interval change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "[FINDING ] No significant interval change [CONCLUSION ] No significant interval change [RECOMMENDATION] -<|endoftext|>\n",
      "Generated Report : \n",
      "FINDING ] - [CONCLUSION ] No significant interval change [RECOMMENDATION] -\n",
      "\n",
      "--- Example 70 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "soft tissue swelling, Lt ankle and lower leg _x000D_\n",
      "_x000D_\n",
      "os subfibulare, Rt _x000D_\n",
      "posterior calcaneal spur, both _x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "soft tissue swelling, Lt ankle and lower leg _x000D_\n",
      "_x000D_\n",
      "os subfibulare, Rt _x000D_\n",
      "posterior calcaneal spur, both _x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] soft tissue swelling, Lt ankle and lower leg os subfibulare, Rt posterior calcaneal spur, both [ Conclusion ] soft tissue swelling, Lt ankle and lower leg os subfibulare, Rt posterior calcaneal spur, both [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      " Finding ] [ Diagnosis ] soft tissue swelling, Lt ankle joint [ Recommend ]\n",
      "\n",
      "--- Example 62 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "osteopenia._x000D_\n",
      "degenerative change_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "osteopenia._x000D_\n",
      "degenerative change_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] osteopenia. degenerative change [ Conclusion ] osteopenia. degenerative change [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      "FINDING ] - [CONCLUSION ] No significant interval change [RECOMMENDATION] -\n",
      "\n",
      "--- Example 57 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both 1st MTP joint hallux valgus and joint space narrowing._x000D_\n",
      "  --> R/O OA_x000D_\n",
      "boht ankle OA_x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both 1st MTP joint hallux valgus and joint space narrowing._x000D_\n",
      "  --> R/O OA_x000D_\n",
      "boht ankle OA_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] both 1st MTP joint hallux valgus and joint space narrowing. --> R/O OA boht ankle OA [ Conclusion ] both 1st MTP joint hallux valgus and joint space narrowing. --> R/O OA boht ankle OA [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      " Finding ] [ Diagnosis ] Osteoarthritis in Rt 1st MTP joint. [ Recommend ]\n",
      "\n",
      "--- Example 35 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no significant bony lesion on radiographs._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no significant bony lesion on radiographs._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] no significant bony lesion on radiographs. [ Conclusion ] no significant bony lesion on radiographs. [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      " Finding ] [ Diagnosis ] Soft tissue swelling, left 1st MTP joint. [ Recommend ]\n",
      "\n",
      "--- Example 188 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "extensive bony erosion with deformity, especially in 5th MTP, feet_x000D_\n",
      "left distal fibula, S/P ORIF_x000D_\n",
      "both radiocarpal OA_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] [ Conclusion ] extensive bony erosion with deformity, especially in 5th MTP, feet left distal fibula, S/P ORIF both radiocarpal OA [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      " Finding ] No bony abnormality [ Diagnosis ] No bony abnormality [ Recommend ]\n",
      "\n",
      "--- Example 26 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "Possible osteophyte in left talus head._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "[ Finding ] [ Diagnosis ] Possible osteophyte in left talus head. [ Recommend ]<|endoftext|>\n",
      "Generated Report : \n",
      " Finding ] both ankle OA. [ Conclusion ] both ankle OA. [ Recommend ]\n"
     ]
    }
   ],
   "source": [
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw   = ex['raw']\n",
    "    clean = ex['clean']\n",
    "\n",
    "    img = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pts = ex['patches'].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        g = model.global_encoder(img)\n",
    "        g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = pts.shape\n",
    "        p = pts.view(B*N, C, H, W)\n",
    "        pf = (model.patch_encoder.forward_features(p)\n",
    "                if hasattr(model.patch_encoder,'forward_features')\n",
    "                else model.patch_encoder(p))\n",
    "        pf = pf.mean(dim=[2,3])\n",
    "        pf = model.patch_proj(pf).view(B, N, -1)\n",
    "\n",
    "        cat, _ = model.attn(torch.cat([g,pf],1),\n",
    "                            torch.cat([g,pf],1),\n",
    "                            torch.cat([g,pf],1))\n",
    "        cat = model.norm(cat)\n",
    "\n",
    "        bos   = torch.full((B,1), tokenizer.bos_token_id, device=device)\n",
    "        bos_m = torch.ones_like(bos)\n",
    "        gen_ids = model.decoder.generate(\n",
    "            input_ids=bos,\n",
    "            attention_mask=bos_m,\n",
    "            encoder_hidden_states=cat,\n",
    "            max_length=100,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ceb46",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8043ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56903/3693856814.py:489: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_56903/3693856814.py:358: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:33,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:30,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:29,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:28,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:28,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:27,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:26,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:07<00:26,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:24,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:23,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:11<00:22,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:15<00:18,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:15,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:19<00:14,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:20<00:12,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:11,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:10,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:23<00:10,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:24<00:09,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:07,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:28<00:05,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:03,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:32<00:01,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2a9958c34943208a600ae0fdaf02df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7ff363e90d41dda20e66b5cd6f0dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.4140\n",
      "  Validation Loss     : 0.8932\n",
      "  Semantic Similarity : 0.4220\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:33,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:31,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:30,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:28,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:27,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:27,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:26,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:07<00:25,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:08<00:24,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:23,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:11<00:22,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:12<00:20,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:15<00:17,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:16<00:16,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:17<00:15,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:19<00:13,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:20<00:12,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:21<00:11,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:22<00:10,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:23<00:09,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:24<00:08,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:25<00:07,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:26<00:06,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:27<00:05,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:28<00:05,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:29<00:03,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:30<00:02,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:31<00:01,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:32<00:01,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f78c915daa42108c5c1485145aaf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36dcfcce7d1c4c9fb5e778e62016c7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss              : 0.9283\n",
      "Test Semantic Sim     : 0.4356\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 173 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Osteoporosis in both foot bones._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Osteoporosis in both foot bones.\n",
      "Generated Report : \n",
      "no bony lesion. no abnormality, interval change or fracture with bone erosions at Rt 1st MTP joint -> r/o gout arthritis degenerative changes --> RA involvement otherwise subchondral cystic formation of both calcaneus and phalanx in left lateral malleolar area (mild type II) old os trigonum diffuse osteopenia mild erosion on radiographs suggested to be proximal metatarsal head rather than talocalcarpetoid coalition Lt 5th MT base suspicious periarticular ossification likely hallux val\n",
      "\n",
      "--- Example 189 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "gout, both feet and ankles, probable both knees._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "gout, both feet and ankles, probable both knees._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "gout, both feet and ankles, probable both knees.\n",
      "Generated Report : \n",
      "No bony abnormality. No significant interval change since last study of Rt 1st MTP joint, suspicious erosion and degenerative changes at both lateral malleolar area with osteopenia or otherwise mild inflammatory arthritis type 2 OA periarticular bone fragment head involvement --> RA insertion site on radiographs --- probable gouty old fracture -> r/o fibular coalition erosions in left talonavus base -- possible phalanx formation tophi deposition around calcaneal spur > PIP joints os trigoneum posterior aspect ossification rather than lesion progression\n",
      "\n",
      "--- Example 228 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no bony lesion.\n",
      "Generated Report : \n",
      "no significant interval change. no bony lesion on radiographs or otherwise suggestive of degenerative changes, both ankle joints and left calcaneus with os trigonum type II (ego-possible osteopenia) gout arthritis in right 1st MTP joint -> R/O RA involvement possible OA correlation suggested mild inflammatory state --> rheumatoid cystic fibrosis old fracture space narrowing to Lt 2nd toe soft tissue swelling at proximal phalanx lateral malleolar area > subchondralbone abnormality - small bone erosion periarticular bul\n",
      "\n",
      "--- Example 139 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "suspicious periarticular tophi, left big toe._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "suspicious periarticular tophi, left big toe._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "suspicious periarticular tophi, left big toe.\n",
      "Generated Report : \n",
      "no bony lesion. no significant interval change since last study of pes planus degenerative changes, both feet and head suspicious erosion with increased density at Rt 1st MTP joint -> gout arthritis or otherwise old fracture site mild OA probable ossification in left lateral malleolar area periarticular osteopenia type II diffuse inflammatory lesions --> r/o RA involvement severe os trigonum rec> patella bone erosions - hallux valgare sesamoid coalition suggested to be subchondral cystic formation > Lt 5th met\n",
      "\n",
      "--- Example 22 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Multiple joint space narrowing, both ankle, foot, knee joints with subchondral cystic change and degenerative change_x000D_\n",
      "Os naviculare, type1, Rt._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Multiple joint space narrowing, both ankle, foot, knee joints with subchondral cystic change and degenerative change_x000D_\n",
      "Os naviculare, type1, Rt._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Multiple joint space narrowing, both ankle, foot, knee joints with subchondral cystic change and degenerative change Os naviculare, type1, Rt.\n",
      "Generated Report : \n",
      "no bony lesion. no significant interval change since last study, both feet and wrists degenerative changes with RA involvement or otherwise mild osteopenia Rt 1st MTP joint space narrowing to right 5th metatarsal bone base -> gout arthritis in left calcaneus of old fracture site at r/o spondylosis subchronic inflammatory syndrome probable ossification rather than periarticular cystic erosions on radiographs --> PIP joints --- Osteophyte os trigonum ------------------------------------------------------------------------ No other abnormality suspicious erosion around talocalcoid head\n",
      "\n",
      "--- Example 151 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "C.I.: knee pain (Lt>Rt, 3YA), wrist(5mo), wrist(2yr), gout, UA 8.6_x000D_\n",
      "_x000D_\n",
      "erosion and tophi, the both patella _x000D_\n",
      "Soft tissue swelling of both wrist (right > left), both ankle (right > left), both 1st MTP joints with suspicious tophi in the right_x000D_\n",
      " -- Bony erosion in both 1st MTP joints_x000D_\n",
      " --> Suggestive of gout arthritis_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "C.I.: knee pain (Lt>Rt, 3YA), wrist(5mo), wrist(2yr), gout, UA 8.6_x000D_\n",
      "_x000D_\n",
      "erosion and tophi, the both patella _x000D_\n",
      "Soft tissue swelling of both wrist (right > left), both ankle (right > left), both 1st MTP joints with suspicious tophi in the right_x000D_\n",
      " -- Bony erosion in both 1st MTP joints_x000D_\n",
      " --> Suggestive of gout arthritis_x000D_\n",
      " --> Suggestive of gout arthritis_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "C.I.: knee pain (Lt>Rt, 3YA), wrist(5mo), wrist(2yr), gout, UA 8.6 erosion and tophi, the both patella Soft tissue swelling of both wrist (right > left), both ankle (right > left), both 1st MTP joints with suspicious tophi in the right -- Bony erosion in both 1st MTP joints --> Suggestive of gout arthritis C.I.: knee pain (Lt>Rt, 3YA), wrist(5mo), wrist(2yr), gout, UA 8.6 erosion and tophi, the both patella Soft tissue swelling of both wrist (right > left), both ankle (right > left), both 1st MTP joints with suspicious tophi in the right -- Bony erosion in both 1st MTP joints --> Suggestive of gout arthritis --> Suggestive of gout arthritis\n",
      "Generated Report : \n",
      "No bony abnormality No significant interval change. Rt 1st MTP joint, degenerative changes and erosion of both feet with suspicious soft tissue swelling around right 5th metatarsal head base -> r/o gout arthritis or old fracture in left calcaneus Lt 2nd toe --> RA involvement PIP joints erosions at medial malleolar area Osteophyte formation on radiographs -- mild inflammatory osteopenia - diffuse bone deformity ------------------------------------------------------------------------ no definite correlation between talonavicular type IIa) ossification --- possible small subchondral cystic\n",
      "\n",
      "--- Example 108 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Rt.1st PP base subchondral cyst._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Rt.1st PP base subchondral cyst._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Rt.1st PP base subchondral cyst.\n",
      "Generated Report : \n",
      "No bony abnormality. No significant interval change since last study of gout arthritis, both feet and ankles ------------------------------------------------------------------------ no BIP lesion on radiographs or otherwise suspicious old fracture site at Lt 1st MTP joint type II OA --- probable degenerative changes Rt 5th metatarsal bone erosion --> RA involvement with possible osteopenia -> r/o inflammatory disease in mild to moderate malleolar os trigonum (degeneration) - Possible accessory navicular bones coalition Degenerous cystic formation around left lateral aspect head space narrowing periartis tendonitis\n",
      "\n",
      "--- Example 8 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_degenerative change_x000D__x000D_[CONCLUSION    ]_x000D_degenerative change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "degenerative change\n",
      "Generated Report : \n",
      "no bony lesion. no significant interval change since last study of Rt 1st MTP joint, Lt 2nd MT head erosion --> r/o gout arthritis with degenerative changes or both hands and feet possible osteopenia old fracture at left distal phalanx type II bone density abnormality suspicious subchondral cysts in lateral aspect - interleukoid coalition -> RA involvement suggested otherwise mild inflammatory state probable ossification more than 5th metatarsomal area diffuse OA on radiographs rather large os trigonum periarticular malformation soft tissue\n",
      "\n",
      "--- Example 7 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No significant interval change since last study._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No significant interval change since last study.\n",
      "Generated Report : \n",
      "no bony lesion. no significant interval change of gout arthritis, Lt 1st MTP joint and both 2nd MT head -> R/O OA or old fracture site with degenerative changes --> rt calcaneous bone erosion in lateral aspect (both feet) diffuse osteopenia mild to moderate inflammatory state severe ossification process at proximal phalanx type II large soft tissue swelling around left 5th metatarsal base area suspicious erosions on radiographs - hallux valgus os trigonum small periarticular malleolar coalition syndrome probable sub\n",
      "\n",
      "--- Example 23 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "Osteoarthritis in right 1st MTP joint. _x000D_\n",
      "small enthesophytes of both calcaneus, plantar area._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Osteoarthritis in right 1st MTP joint. small enthesophytes of both calcaneus, plantar area.\n",
      "Generated Report : \n",
      "no bony lesion. no significant interval change since last study, both feet and ankles with mild degenerative changes of left 1st MTP joint -> R/O gout arthritis or old fracture . No signficant abnormality at Lt 2nd PIP site --- probable subchondral cystic bone erosion --> rt calcification type II - diffuse osteopenia , suspicious os trigonum (R)\n",
      "No large vascular deformity in lateral malleolar area : inflammatory involvement -- possible plantar aspect ossicles erosions on radiographs > radiopaque\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, tokenizer, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        # Set EOS/BOS tokens before any cleaning happens\n",
    "        self.bos_token_id = tokenizer.eos_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.eos_token = tokenizer.eos_token  # string\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        # If binary, balance here\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        # TOKENIZATION: no leaking of tokens, build full seq with BOS/EOS\n",
    "        token_ids = self.tokenizer.encode(\n",
    "            clean,\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length - 2,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        full_ids = [self.bos_token_id] + token_ids + [self.eos_token_id]\n",
    "        input_ids = torch.tensor(full_ids, dtype=torch.long)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text  # EOS will be added in tokenization\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B, N, 768)\n",
    "\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # START ONLY FROM BOS\n",
    "            B = ids.size(0)\n",
    "            prompt = torch.full((B, 1), tokenizer.eos_token_id,\n",
    "                                dtype=ids.dtype, device=device)\n",
    "\n",
    "            # build encoder_hidden_states\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf)\n",
    "            pf = pf.view(B, N, 768)\n",
    "\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=prompt,\n",
    "                encoder_hidden_states=comb,\n",
    "                attention_mask=torch.ones_like(prompt),\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg, tokenizer)\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss              : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim     : {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    prompt = torch.full((1, 1), tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf)\n",
    "    pf = pf.view(B, N, 768)\n",
    "\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=120,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66473380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56903/554444045.py:489: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_56903/554444045.py:357: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:33,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:31,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:29,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:16,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:15,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:10,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:07,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803f6936630c475fb187965c624fa301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a8b6ef99d6424fae79a30d2eafc633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.4043\n",
      "  Validation Loss     : 0.9345\n",
      "  Semantic Similarity : 0.4250\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:30,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:29,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:07<00:25,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:24,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:11<00:22,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:20,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:19,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:15<00:17,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:16<00:16,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:15,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:19<00:14,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:20<00:13,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:11,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:10,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:23<00:10,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:24<00:09,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:07,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:28<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:03,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:32<00:01,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dbf074133d4797a868671fe16daa5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24d0648de564441bfe49ce9a15c72d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.9084\n",
      "  Validation Loss     : 0.8253\n",
      "  Semantic Similarity : 0.4250\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:30,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:29,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:07<00:26,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:11<00:22,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:15<00:17,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:16,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:15,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:19<00:14,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:20<00:13,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:10,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:23<00:10,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:24<00:09,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:07,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:28<00:05,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:32<00:01,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c563bfb02645339a992ed7abf8e90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485fdc70d00d46a7bd44376d45b7122c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7952\n",
      "  Validation Loss     : 0.7916\n",
      "  Semantic Similarity : 0.4360\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:30,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:29,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:07<00:25,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:11<00:22,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:20,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:15<00:17,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:16<00:16,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:15,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:19<00:14,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:20<00:13,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:10,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:23<00:10,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:24<00:09,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:07,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:28<00:05,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:32<00:01,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da74c4fbb1e4d5ab3609f6999e74aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c819cf21a2d144afb345babbf16828e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7141\n",
      "  Validation Loss     : 0.7831\n",
      "  Semantic Similarity : 0.4310\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:30,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:29,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:29,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:28,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:27,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:07<00:25,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:25,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:23,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:11<00:21,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:21,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:20,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:20,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:19,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:19,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:18,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:15<00:17,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:16<00:16,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:15,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:14,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:19<00:14,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:20<00:13,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:10,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:23<00:10,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:24<00:09,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:07,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:27<00:06,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:28<00:05,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:28<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:03,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:31<00:02,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:32<00:01,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:32<00:01,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:33<00:00,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a8d4ca0a0c4f13b07a21d93e205b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bdc1d4c0394222b2434b289f728a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.6607\n",
      "  Validation Loss     : 0.7613\n",
      "  Semantic Similarity : 0.4321\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:32,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:31,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:30,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:29,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:29,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:28,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:04<00:28,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:27,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:27,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:26,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:26,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:07<00:25,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:24,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:08<00:24,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:23,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:23,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:10<00:22,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:22,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:11<00:21,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:20,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:12<00:20,  1.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:13<00:19,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:13<00:19,  1.83it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:14<00:18,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:14<00:18,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:15<00:17,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:17,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:16<00:16,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:17<00:16,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:17<00:15,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:18<00:15,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:18<00:14,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:19<00:14,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:20<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:20<00:13,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:21<00:12,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:21<00:11,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:22<00:11,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:22<00:10,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:23<00:10,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:24<00:09,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:24<00:08,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:25<00:08,  1.80it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:25<00:07,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:26<00:07,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:26<00:06,  1.82it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:27<00:06,  1.81it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:27<00:05,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:28<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:29<00:04,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:29<00:03,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:30<00:03,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:30<00:02,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:31<00:02,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:31<00:01,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:32<00:01,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:32<00:00,  1.79it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196ad890a98e461d9619ac875a3fb51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3c72d3d91b4a51bb2e76ca3910f313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss              : 0.7589\n",
      "Test Semantic Sim     : 0.4253\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 139 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no significant bony abnormality_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no significant bony abnormality_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no significant bony abnormality\n",
      "Generated Report : \n",
      " no significant bony lesion on radiographs. both ankle OA with suspicious erosion at Rt 1st MTP joint -> RA involvement, more likely mild degenerative change of right foot and left talonavicular bone head area in medial portion or posterior aspect rather than lateral malleolus tendon insertion site --> gout arthritis suggested (R/O tophi) rec> clinical correlation vs probable old fracture(sue osteopenia). otherwise minimal ossification deposition over righ 5th MT base island adjacent territory subchondral cysts (+/-4rd MC shaft),\n",
      "\n",
      "--- Example 107 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "hallus valgus deformity, left._x000D_\n",
      "no visible tophis on radiograph_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "correlation with US findings.\n",
      "Cleaned Report   : \n",
      "hallus valgus deformity, left. no visible tophis on radiograph\n",
      "Generated Report : \n",
      " no bony lesion. No joint space narrowing with sclerotic change of both feet, Lt ankle and knee joints since last study on soft tissue swelling in lateral portion of left 1st MTP area(right > right). --> R/O gout arthritis or osteopenia rather than old fracture (Rt calcaneal spur) rec> clinical correlation for radiographs periarticular tophi vs bone erosion at distus medial side -> probable RA involvement degenerative changes state suggested otherwise cannot be excluded as an additional possibility suggestive of possible infectious disease such more likely mild radiolu\n",
      "\n",
      "--- Example 56 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change.\n",
      "Generated Report : \n",
      " No significant interval change since last study. Suspicious erosion in both 1st MTP joint area and left 2nd MT head, lateral malleolus formation --> R/O gout involvement of Lt calcaneocuboidal coalition with loose bodies on radiographs (Lt CT) rather than bone fragment deposition or osteopenia fracture (+). --- suspicious erosions at right 5th IP joints -> RA vs mild degenerative changes rec> clinical correlation no definite evidence that rheumatoid arthritis cannot be excluded as an additional possibility moderate OA involving knee soft tissue swelling around medial side dist\n",
      "\n",
      "--- Example 114 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "Lt. MTP joint, 1st, soft tissue swelling with suspicious bony erosion._x000D_\n",
      "_x000D_\n",
      "Lt. tarsal bone level, suspicious calcification or ossification (arrow)_x000D_\n",
      "_x000D_\n",
      "Lt. 5th MT neck, bony protrusion._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Lt. MTP joint, 1st, soft tissue swelling with suspicious bony erosion. Lt. tarsal bone level, suspicious calcification or ossification (arrow) Lt. 5th MT neck, bony protrusion.\n",
      "Generated Report : \n",
      " No bony abnormality. Mild soft tissue swelling in medial portion of both ankle joint with subtle erosion at Lt 1st IP and 5th MT head --- probable gouty arthritis involvement or old fracture, right fibula (right distal bone). Small osteophyte on radiographs rather than clinical correlation (+) suggested mild degenerative change since last study -- R/O RA patient vs non-respiratory lesion --> rheumatoid process rec> consider Fx.: Osteoarthritis involving left talonavicular joints Both accessory naviculocuneiform coalition De\n",
      "\n",
      "--- Example 150 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "spur change of both calcaneous _x000D_\n",
      "_x000D_\n",
      "Rt. 1st MTP joint, gout arthritis with tophi._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "spur change of both calcaneous _x000D_\n",
      "_x000D_\n",
      "Rt. 1st MTP joint, gout arthritis with tophi._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "spur change of both calcaneous Rt. 1st MTP joint, gout arthritis with tophi.\n",
      "Generated Report : \n",
      " both feet, OA. Lt 1st MTP joint bony erosion with soft tissue swelling and mild degenerative change of lateral malleolus tendon base --> R/O gout arthritis involvement or old fracture in right 3rd MT head -> RA vs non-hear stress fractures rather than bone erosions since 2010s rec> clinical correlation no significant interval changes sinde last study on radiographs suggestive tophi deposition at left distal phalanx posterior calcaneocuboidal line (Rt.) Subluxation around the 5th metatarsofibular tip De\n",
      "\n",
      "--- Example 71 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both hands, feet, and knees, FBs._x000D_\n",
      "_x000D_\n",
      "both 3rd MT bone, erosive change._x000D_\n",
      "both hands, erosions._x000D_\n",
      "   --> RA, suggested._x000D_\n",
      "_x000D_\n",
      "Rt. knee RA involvement._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both hands, feet, and knees, FBs._x000D_\n",
      "_x000D_\n",
      "both 3rd MT bone, erosive change._x000D_\n",
      "both hands, erosions._x000D_\n",
      "   --> RA, suggested._x000D_\n",
      "_x000D_\n",
      "Rt. knee RA involvement._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both hands, feet, and knees, FBs. both 3rd MT bone, erosive change. both hands, erosions. --> RA, suggested. Rt. knee RA involvement.\n",
      "Generated Report : \n",
      " no bony lesion. No abnormality of both feet, Lt ankle and talonavicular joint with bone erosion in lateral portion of right 1st MTP joints -> r/o inflammatory arthritis involvement or old fracture rather than gouty periarticleitis syndrome --> RA vs rec> clinical correlation study cannot be excluded as likely erosive change since last visit to Rt medial side area such an increased density around left 2nd MT head (Rx) suggested otherwise soft tissue swelling over the naviculocuneiform coalition on radiographs : probable sclerotic changes along\n",
      "\n",
      "--- Example 207 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_diffuse osteopenia\n",
      "degenerative change\n",
      "severe OA, both knee joints\n",
      "Lt. knee joint effusion._x000D__x000D_[CONCLUSION    ]_x000D_diffuse osteopenia\n",
      "degenerative change\n",
      "severe OA, both knee joints\n",
      "Lt. knee joint effusion._x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "diffuse osteopenia degenerative change severe OA, both knee joints Lt. knee joint effusion.\n",
      "Generated Report : \n",
      " degenerative change. osteopenia, both feet and wrist with loose bodies in the left 1st MTP joint area(right > left). Rt. 2nd MT head erosion --> RA involvement (R/O gout) rec> clinical correlation of old fracture or os trigonum vs bone fragment -> rheumatoid arthritis rather than inflammatory reaction such as multiple sclerosis progression flat foot OA Lt. knee soft tissue swelling (+/- milder type II diffuse vascular calcification subluxation), suggestive of erosions at right 5th metatarsal shaft --> possible accessory o\n",
      "\n",
      "--- Example 222 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change.\n",
      "Generated Report : \n",
      " No bony abnormality. Mild hallux valgus, both hands and feet with soft tissue swelling in medial portion of right ankle joint (right > left). Tiny calcification adjacent to Lt 1st IP bone base --> R/O gout involvement rather than old fracture or osteopenia such as an os trigonum insertion site vs lateral malleolar tip -> rheumatoid arthritis rec> clinical correlation otherwise no significant interval change since last study --- suggested inflammatory reaction Osteoarthritis at MTP joints Both accessory navicular bones including tarsal neck Subluxation (+\n",
      "\n",
      "--- Example 1 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No significant interval change_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "No significant interval change_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No significant interval change\n",
      "Generated Report : \n",
      " no significant bony lesion on radiographs. diffuse osteopenia degenerative change both ankle, soft tissue swelling and loose bodies suggestive of old fracture or gout involvement with bone erosions at Rt 1st MT head -> r/o inflammatory arthritis such as RA mild joint space narrowing in Lt 5th MTP area moderate OA around 2nd TMT base rec> clinical correlation to consider FxI vs LT knee joints periarticular changes --> postop state rather than probable subluxation flat foot disease progression since last study hallux valgus deformity (+)\n",
      "\n",
      "--- Example 194 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_degenerative change_x000D__x000D_[CONCLUSION    ]_x000D_degenerative change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "degenerative change\n",
      "Generated Report : \n",
      " degenerative change. osteopenia, both hands and feet with joint space narrowing in Rt 1st MTP joints -> r/o inflammatory arthritis involvement than old fracture or erosion at Lt 5th MT head bone fragment --> RA vs gouty mild radiopaque reaction of right ankle dorsum distal phalanx (R>Lt) neck island progression progressed state since last study hallux valgus deformity on radiographs flat foot swelling (+/- minimal OA). diffuse vascular calcifications adjacent to left wrist base area soft tissue bulging syndrome such as erosions rather subtle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        \n",
    "        # Add BOS token to the beginning of the report\n",
    "        input_text = f\"{self.tokenizer.bos_token} {clean}\"\n",
    "        \n",
    "        # Tokenize with truncation and ensuring EOS token at end\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=511, return_tensors='pt')\n",
    "        \n",
    "        # Add EOS token if not already present\n",
    "        if tok['input_ids'][0][-1] != self.tokenizer.eos_token_id:\n",
    "            input_ids = torch.cat([tok['input_ids'], torch.tensor([[self.tokenizer.eos_token_id]])], dim=1)\n",
    "            attention_mask = torch.cat([tok['attention_mask'], torch.tensor([[1]])], dim=1)\n",
    "        else:\n",
    "            input_ids = tok['input_ids']\n",
    "            attention_mask = tok['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B, N, 768)\n",
    "\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate loss using full ground truth\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # For generation, use only BOS token as input\n",
    "            bos_tokens = torch.tensor([[tokenizer.bos_token_id]] * imgs.size(0), device=device)\n",
    "\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf)\n",
    "            pf = pf.view(B, N, 768)\n",
    "\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos_tokens,\n",
    "                encoder_hidden_states=comb,\n",
    "                attention_mask=torch.ones_like(bos_tokens),\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\", color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.tokenizer = tokenizer\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss              : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim     : {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Use only BOS token for generation\n",
    "    prompt = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf)\n",
    "    pf = pf.view(B, N, 768)\n",
    "\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=120,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c232c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_56903/3352693259.py:491: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]/tmp/ipykernel_56903/3352693259.py:357: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:27,  2.14it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:25,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:25,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:25,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:25,  2.16it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:24,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:23,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:22,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:22,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:22,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:21,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:21,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:20,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:06<00:20,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:20,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:07<00:19,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:07<00:18,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:07<00:18,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:08<00:17,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:08<00:17,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:09<00:16,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:09<00:16,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:10<00:16,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:10<00:15,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:11<00:15,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:11<00:15,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:11<00:14,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:12<00:14,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:12<00:13,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:13<00:12,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:13<00:12,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:14<00:12,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:14<00:12,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:15<00:11,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:15<00:10,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:15<00:10,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:16<00:10,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:16<00:09,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:17<00:09,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:17<00:08,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:18<00:08,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:18<00:07,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:18<00:07,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:19<00:06,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:19<00:06,  2.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:20<00:06,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:20<00:05,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:21<00:05,  2.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:21<00:04,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:21<00:04,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:22<00:03,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:22<00:03,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:23<00:02,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:23<00:02,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:24<00:02,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:24<00:01,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:24<00:01,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:25<00:00,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:25<00:00,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092a82d888c04b069027317330f8bc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42e92b2b31948bdb8a4d167f80633b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.3992\n",
      "  Validation Loss     : 0.8406\n",
      "  Semantic Similarity : 0.4078\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:27,  2.14it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:25,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:25,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:25,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:25,  2.17it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:24,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:23,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:22,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:22,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:22,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:21,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:21,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:20,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:06<00:20,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:19,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:07<00:19,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:07<00:18,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:07<00:18,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:08<00:18,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:08<00:17,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:09<00:17,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:09<00:16,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:10<00:16,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:10<00:15,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:11<00:15,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:11<00:15,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:11<00:14,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:12<00:13,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:12<00:13,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:13<00:12,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:13<00:12,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:14<00:12,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:14<00:12,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:15<00:11,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:15<00:10,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:15<00:10,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:16<00:10,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:16<00:09,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:17<00:09,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:17<00:08,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:18<00:08,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:18<00:07,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:18<00:07,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:19<00:06,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:19<00:06,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:20<00:06,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:20<00:05,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:21<00:05,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:21<00:04,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:21<00:04,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:22<00:03,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:22<00:03,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:23<00:03,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:23<00:02,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:24<00:02,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:24<00:01,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:25<00:01,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:25<00:00,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:25<00:00,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e9a79830c34a62ab42e4ed7ec7bc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efc4aa56b9746989afab244602a1613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.9141\n",
      "  Validation Loss     : 0.7609\n",
      "  Semantic Similarity : 0.4083\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:27,  2.14it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:26,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:25,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:25,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:25,  2.16it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:24,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:23,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:22,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:04<00:22,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:22,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:21,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:21,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:20,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:06<00:20,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:20,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:07<00:19,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:07<00:18,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:08<00:18,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:08<00:18,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:08<00:17,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:09<00:17,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:09<00:16,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:10<00:16,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:10<00:15,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:11<00:15,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:11<00:14,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:11<00:14,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:12<00:13,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:12<00:13,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:13<00:12,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:13<00:12,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:14<00:12,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:14<00:11,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:15<00:11,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:15<00:10,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:15<00:10,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:16<00:10,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:16<00:09,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:17<00:09,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:17<00:08,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:18<00:08,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:18<00:07,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:18<00:07,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:19<00:06,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:19<00:06,  2.34it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:20<00:06,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:20<00:05,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:21<00:05,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:21<00:04,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:21<00:04,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:22<00:03,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:22<00:03,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:23<00:03,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:23<00:02,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:24<00:02,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:24<00:01,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:25<00:01,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:25<00:00,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:25<00:00,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111f47394f9d4e32b1e153c857af3a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a7532f0ea14f8e884cc8dff3c82b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7884\n",
      "  Validation Loss     : 0.7287\n",
      "  Semantic Similarity : 0.4097\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:27,  2.14it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:25,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:25,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:25,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:25,  2.17it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:24,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:23,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:22,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:22,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:22,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:21,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:21,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:20,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:06<00:20,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:20,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:07<00:20,  2.19it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:07<00:19,  2.16it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:08<00:19,  2.13it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:08<00:18,  2.18it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:09<00:18,  2.14it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:09<00:17,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:09<00:16,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:10<00:16,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:10<00:15,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:11<00:15,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:11<00:15,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:12<00:14,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:12<00:13,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:12<00:13,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:13<00:12,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:13<00:12,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:14<00:12,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:14<00:12,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:15<00:11,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:15<00:10,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:16<00:10,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:16<00:10,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:16<00:09,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:17<00:09,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:17<00:08,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:18<00:08,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:18<00:07,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:19<00:07,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:19<00:06,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:19<00:06,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:20<00:06,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:20<00:05,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:21<00:05,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:21<00:04,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:22<00:04,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:22<00:03,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:22<00:03,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:23<00:03,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:23<00:02,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:24<00:02,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:24<00:01,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:25<00:01,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:25<00:00,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:26<00:00,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427298f5d7c1447bbeba084bca76b714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e086bc4688843fa802f5fe01659e1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7031\n",
      "  Validation Loss     : 0.7187\n",
      "  Semantic Similarity : 0.4011\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:27,  2.16it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:25,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:25,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:24,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:25,  2.19it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:23,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:23,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:22,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:22,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:22,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:21,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:21,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:20,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:06<00:20,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:19,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:07<00:19,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:07<00:19,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:07<00:19,  2.20it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:08<00:18,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:08<00:18,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:09<00:17,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:09<00:16,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:10<00:16,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:10<00:15,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:11<00:15,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:11<00:15,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:11<00:14,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:12<00:13,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:12<00:13,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:13<00:12,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:13<00:12,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:14<00:12,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:14<00:12,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:15<00:11,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:15<00:10,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:15<00:10,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:16<00:10,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:16<00:09,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:17<00:09,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:17<00:08,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:18<00:08,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:18<00:07,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:18<00:07,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:19<00:06,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:19<00:06,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:20<00:06,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:20<00:05,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:21<00:05,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:21<00:04,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:21<00:04,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:22<00:03,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:22<00:03,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:23<00:03,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:23<00:02,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:24<00:02,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:24<00:01,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:25<00:01,  2.22it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:25<00:00,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:25<00:00,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4539a10938b6480688f31489fd1ca1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ef6139dd5e4f2b8b43c0f7b41b318b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.6491\n",
      "  Validation Loss     : 0.7134\n",
      "  Semantic Similarity : 0.4020\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:25,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:00<00:25,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:24,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:01<00:23,  2.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:23,  2.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:02<00:23,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:03<00:23,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:03<00:22,  2.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:03<00:21,  2.36it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:21,  2.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:20,  2.35it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:20,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:05<00:20,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:06<00:19,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:06<00:19,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:06<00:18,  2.33it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:07<00:18,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:07<00:18,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:08<00:17,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:08<00:17,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:09<00:16,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:09<00:16,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:09<00:15,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:10<00:15,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:10<00:15,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:11<00:15,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:11<00:14,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:12<00:14,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:12<00:14,  2.21it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:13<00:13,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:13<00:12,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:13<00:12,  2.25it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:14<00:11,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:14<00:11,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:15<00:10,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:15<00:10,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:16<00:10,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:16<00:09,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:17<00:09,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:17<00:08,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:17<00:08,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:18<00:07,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:18<00:07,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:19<00:07,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:19<00:06,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:20<00:06,  2.30it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:20<00:05,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:20<00:05,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:21<00:04,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:21<00:04,  2.32it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:22<00:03,  2.31it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:22<00:03,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:23<00:03,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:23<00:02,  2.27it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:24<00:02,  2.29it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:24<00:01,  2.24it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:24<00:01,  2.23it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:25<00:00,  2.26it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:25<00:00,  2.28it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8dda5c9e914823a9eca5b8d1e57f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b1fd217c614ae39672f55d0ea11f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss              : 0.7482\n",
      "Test Semantic Sim     : 0.4262\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 212 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "1. Possible S/P fusion operation of left ankle and subtalar joint._x000D_\n",
      "2. Joint space narrowing in both 1st MTP, right 1st-5th TMT and both midfoot._x000D_\n",
      " - With deformity of right navicular bone, right medial and intermediate cuneiform._x000D_\n",
      "3. Diffuse osteopenia._x000D_\n",
      "4. Both flat foot._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "1. Possible S/P fusion operation of left ankle and subtalar joint. 2. Joint space narrowing in both 1st MTP, right 1st-5th TMT and both midfoot. - With deformity of right navicular bone, right medial and intermediate cuneiform. 3. Diffuse osteopenia. 4. Both flat foot.\n",
      "Generated Report : \n",
      " Rt. ankle joint effusion Lt. knee, gout arthritis both 1st MTP joints and medial hallux valgus degenerative change with suspicious erosions on right talonavicular bone --> GIP vs soft tissue swelling of left 2nd PP base -> r/o\n",
      "\n",
      "--- Example 160 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no bony lesion.\n",
      "Generated Report : \n",
      " No bony abnormality. Small enthesophyte in plantar aspect of right calcaneus, both -- with subtle erosions at 1st and 5th metatarsal heads Degenerative changes on left disticulo1cmillary joint Soft tissue swelling around medial side Subch\n",
      "\n",
      "--- Example 158 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_loose body, Lt ankle posterior aspect  \n",
      "degenerative change, Lt ankle joint _x000D__x000D_[CONCLUSION    ]_x000D_loose body, Lt ankle posterior aspect  \n",
      "degenerative change, Lt ankle joint _x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "loose body, Lt ankle posterior aspect degenerative change, Lt ankle joint\n",
      "Generated Report : \n",
      " No significant interval change since last study. - suspicious gout involvement, right 1st MTP joint and both wrist joints Mild hallux valgus with OA Rt 3rd PP base -> r/o inflammatory arthritis rec> consider F-spines rather than IP or CMC replacement\n",
      "\n",
      "--- Example 226 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both 1st toe and 5th toe, gout arthritis._x000D_\n",
      "_x000D_\n",
      "Rt. distal tibiofibular joint, gout involvement._x000D_\n",
      "both ankle, gout involvement._x000D_\n",
      "_x000D_\n",
      "Lt. knee, gout._x000D_\n",
      "_x000D_\n",
      "both wrist and hands gout._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both 1st toe and 5th toe, gout arthritis._x000D_\n",
      "_x000D_\n",
      "Rt. distal tibiofibular joint, gout involvement._x000D_\n",
      "both ankle, gout involvement._x000D_\n",
      "_x000D_\n",
      "Lt. knee, gout._x000D_\n",
      "_x000D_\n",
      "both wrist and hands gout._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both 1st toe and 5th toe, gout arthritis. Rt. distal tibiofibular joint, gout involvement. both ankle, gout involvement. Lt. knee, gout. both wrist and hands gout.\n",
      "Generated Report : \n",
      " degenerative change. os trigonum, both (right). Rt ankle joint effusion --> gout arthritis cannot be excluded and suggested otherwise no significant bony lesion on radiographs or in the study of soft tissue swelling around 1st MTP joints with old fracture at Lt 3rd\n",
      "\n",
      "--- Example 220 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_Lt. preatellar soft tissue swelling.\n",
      "Lt. patella, suspicious erosion\n",
      "   --> R/O enthesitis.\n",
      "Rt. elbow, olecranon spur (+)\n",
      "   --> R/O triceps tendinitis._x000D__x000D_[CONCLUSION    ]_x000D_Lt. preatellar soft tissue swelling.\n",
      "Lt. patella, suspicious erosion\n",
      "   --> R/O enthesitis.\n",
      "Rt. elbow, olecranon spur (+)\n",
      "   --> R/O triceps tendinitis._x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "Lt. preatellar soft tissue swelling. Lt. patella, suspicious erosion --> R/O enthesitis. Rt. elbow, olecranon spur (+) --> R/O triceps tendinitis.\n",
      "Generated Report : \n",
      " - mild degenerative change soft tissue swelling, Lt ankle medial side knee dorsum suspicious erosion at Rt 1st MTP joint -> r/o gouty arthritis involvement diffuse osteopenia old fracture with sclerotic changes hallux valgus (right foot) probable erosions\n",
      "\n",
      "--- Example 92 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no significant interval change since last study._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no significant interval change since last study._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no significant interval change since last study.\n",
      "Generated Report : \n",
      " no significant interval change since last study. --> R/O gout, right 1st MTP joint and both 4th PIP joints (right 3rd) diffuse osteopenia degenerative changes Lt ankle soft tissue swelling (+). old fracture with callus formation at LT lateral malleolar\n",
      "\n",
      "--- Example 147 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Degenerative change _x000D_\n",
      "Rt. ankle soft tissue swelling._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Degenerative change _x000D_\n",
      "Rt. ankle soft tissue swelling._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Degenerative change Rt. ankle soft tissue swelling.\n",
      "Generated Report : \n",
      " - mild degenerative change hallux valgus, right type 2 Hallocarpal joint OA pes planis both Degenerous soft tissue swelling in medial aspect of left 1st MTP joints -> gouty arthritis possible old fracture with bone erosions at Rt 5th\n",
      "\n",
      "--- Example 49 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "Rt. accessory navicular bone, type II_x000D_\n",
      "_x000D_\n",
      "both 5th and Lt. 3-4 MT head, suspicious marginal erosion._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Rt. accessory navicular bone, type II both 5th and Lt. 3-4 MT head, suspicious marginal erosion.\n",
      "Generated Report : \n",
      " no bony lesion. No interval change since last study : possible gout arthritis, left 2nd proximal phalanx or pes planus both knee joint effusion --> R/O inflammatory reaction Lt ankle soft tissue swelling with erosion of medial malleolius -> rheumatoid\n",
      "\n",
      "--- Example 180 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no significant bony abnormality_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no significant bony abnormality_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no significant bony abnormality\n",
      "Generated Report : \n",
      " degenerative change. osteopenia, both feet and wrist joints no significant interval increase in size since last study: RA involvement D/Dx.) probable old fracture with multiple erosions --> Rt fibular thickening Lt ankle OA mild diffuse joint space narrowing vs.: gout arthritis\n",
      "\n",
      "--- Example 17 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "diffuse degenerative change_x000D_\n",
      "Calcaneal spur, Lt._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "diffuse degenerative change_x000D_\n",
      "Calcaneal spur, Lt._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "diffuse degenerative change Calcaneal spur, Lt.\n",
      "Generated Report : \n",
      " no significant bony lesion on radiographs. osteophytes in both 1st MTP joint and talar head, right --> R/O gout involvement mild degenerative change of rt 2nd proximal phalanx with bone erosions -> RA vs Lt TB> ul\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        \n",
    "        # Add BOS token to the beginning of the report\n",
    "        input_text = f\"{self.tokenizer.bos_token} {clean}\"\n",
    "        \n",
    "        # Tokenize with truncation and ensuring EOS token at end\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=511, return_tensors='pt')\n",
    "        \n",
    "        # Add EOS token if not already present\n",
    "        if tok['input_ids'][0][-1] != self.tokenizer.eos_token_id:\n",
    "            input_ids = torch.cat([tok['input_ids'], torch.tensor([[self.tokenizer.eos_token_id]])], dim=1)\n",
    "            attention_mask = torch.cat([tok['attention_mask'], torch.tensor([[1]])], dim=1)\n",
    "        else:\n",
    "            input_ids = tok['input_ids']\n",
    "            attention_mask = tok['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B, N, 768)\n",
    "\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate loss using full ground truth\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # For generation, use only BOS token as input\n",
    "            bos_tokens = torch.tensor([[tokenizer.bos_token_id]] * imgs.size(0), device=device)\n",
    "\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf)\n",
    "            pf = pf.view(B, N, 768)\n",
    "\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos_tokens,\n",
    "                encoder_hidden_states=comb,\n",
    "                early_stopping = True,\n",
    "                attention_mask=torch.ones_like(bos_tokens),\n",
    "                max_length=60,\n",
    "                do_sample=True,\n",
    "                top_k=20,\n",
    "                top_p=0.85,\n",
    "                temperature=0.9,\n",
    "                repetition_penalty=1.8,\n",
    "                no_repeat_ngram_size=3,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\", color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.tokenizer = tokenizer\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss              : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Sim     : {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Use only BOS token for generation\n",
    "    prompt = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf)\n",
    "    pf = pf.view(B, N, 768)\n",
    "\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        early_stopping = True,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=60,  # Reduce from 120\n",
    "        do_sample=True,\n",
    "        top_k=20,  # Reduce from 40\n",
    "        top_p=0.85,  # More restrictive than 0.95\n",
    "        temperature=0.9,  # Increase randomness slightly\n",
    "        repetition_penalty=1.8,  # Increase from 1.3\n",
    "        no_repeat_ngram_size=3,  # Add this parameter\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe804f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:33,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:30,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:30,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:28,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:23,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:22,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:21,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:20,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:19,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:18,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:17,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:15,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:15,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:14,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:11,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:25<00:10,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:29<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:32<00:02,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e00721742495c9deb7a6dba292fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3e1de0ea0247edbf5292d7595794f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.4182\n",
      "  Validation Loss     : 0.8784\n",
      "  Semantic Similarity : 0.4250\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:33,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:32,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:32,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:30,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:29,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:28,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:25,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:24,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:23,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:22,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:21,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:21,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:16<00:20,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:19,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:17<00:19,  1.61it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:18,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:17,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:19<00:16,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:16,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:20<00:15,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:14,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:22<00:13,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:23<00:12,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:24<00:11,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:25<00:10,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:26<00:09,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:27<00:08,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:28<00:07,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:29<00:06,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:29<00:05,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:30<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:31<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:32<00:02,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:33<00:02,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:34<00:01,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6049ed35607048808be50bbdb2b3eaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b758867766244d47a344789dfa5d800b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.9029\n",
      "  Validation Loss     : 0.8048\n",
      "  Semantic Similarity : 0.4204\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:33,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:32,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:32,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:31,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:29,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:28,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:24,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:23,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:22,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:21,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:20,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:19,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:17<00:18,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:16,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:15,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:15,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:14,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:24<00:11,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:25<00:09,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:29<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:32<00:02,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdddb833a9a043e99cff0ba8c26f9a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3b913f0d0247579a053831e2bbb450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7882\n",
      "  Validation Loss     : 0.7625\n",
      "  Semantic Similarity : 0.4198\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:33,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.78it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:31,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:30,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:06<00:28,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:24,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:23,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:21,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:19,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:19,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:16<00:18,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:16,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:15,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:15,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:14,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:25<00:10,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:28<00:06,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:29<00:05,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:32<00:02,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e1c5893edf48cea2ac5eed7e2de9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65521efe52ea4cbabba1f9f3ac8ba23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.7266\n",
      "  Validation Loss     : 0.7722\n",
      "  Semantic Similarity : 0.4251\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:33,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:32,  1.77it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:32,  1.76it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:32,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:32,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:31,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:29,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:29,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:28,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:25,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:24,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:23,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:21,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:20,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:19,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:17<00:18,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:16,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:15,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:15,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:14,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:24<00:11,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:25<00:09,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:29<00:05,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.75it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:32<00:02,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c494865245e3485299220400e5beb069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e16359c2324e158f059f50754a29c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 0.6542\n",
      "  Validation Loss     : 0.7714\n",
      "  Semantic Similarity : 0.4265\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:34,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:34,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:33,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:32,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:32,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:31,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:31,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:30,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:30,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:05<00:29,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:28,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:27,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:27,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:26,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:08<00:26,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:25,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:09<00:25,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:24,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:24,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:11<00:23,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:22,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:12<00:22,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:13<00:21,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:21,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:14<00:20,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:19,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:15<00:19,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:16<00:18,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:17<00:18,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:17<00:17,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:16,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:18<00:16,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:19<00:15,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:19<00:15,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:20<00:14,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:21<00:13,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:22<00:12,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:22<00:12,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:23<00:11,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:23<00:11,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:24<00:10,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:25<00:09,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:25<00:09,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:26<00:08,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:26<00:08,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:27<00:07,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:28<00:07,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:28<00:06,  1.70it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:29<00:05,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:29<00:05,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:30<00:04,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:30<00:04,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:31<00:03,  1.72it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:32<00:02,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:32<00:02,  1.71it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:33<00:01,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:33<00:01,  1.73it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:34<00:00,  1.74it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631bee6705084eea8d17c0b60e035103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba70a0f0501b475c8d5cebdacfbe85a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss              : 0.7630\n",
      "Test Semantic Similarity: 0.4386\n",
      "\n",
      "===== RANDOM TEST EXAMPLES =====\n",
      "\n",
      "--- Example 97 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both ankle OA._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both ankle OA._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both ankle OA.\n",
      "Generated Report : \n",
      " no bony lesion. Lt., 1st MTP joint, suspicious erosion --> R/O gout arthritis cannot be excluded as suggested in both foot bones of right talus neck (right>left) and proximal phalanx head(left>.1). possible old fracture with bone fragment on left ankle dorsum or tophi at tibial plafond vs. calcaneous ligament -> rheumatoid osteopenia degenerative change syndrome mild OA pes planu\n",
      "\n",
      "--- Example 71 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "Suspicious subchondral cyst of both 1st proximal phalangeal bone base._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Suspicious subchondral cyst of both 1st proximal phalangeal bone base.\n",
      "Generated Report : \n",
      " No bony abnormality. Slightly soft tissue swelling in lateral portion of right ankle joint area and left 1st MTP joints with suspicious erosions --> R/O gout involvement, suggested. Otherwise no significant interval change since last study (p> tarsal bone correlation). Suspicious talocalcaneocuboidal coalition between both calcaneous & posterior aspect adjacent to dorsal malleolus or os trigonum(right) -> rheumatoid arthritis as possible mild degener\n",
      "\n",
      "--- Example 116 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no bony lesion.\n",
      "Generated Report : \n",
      " No bony abnormality. Suspicious soft tissue swelling in left 1st MTP joint area of right ankle and foot Both hallux valgus, type 3 Right talocalcaneal coalition with erosions at both 2nd MT head level --- possible gouty arthritis --> R/O osteochondral lesion no significant interval change since last study . Probable old fracture or bipartite bone fragment on radiograph rather than visible crystal deposition around the lateral malleolae\n",
      "\n",
      "--- Example 162 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "marginal erosion, 1st TMT joint and medial naviculocuneiform joint._x000D_\n",
      " -> possibility of RA involvement._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "marginal erosion, 1st TMT joint and medial naviculocuneiform joint. -> possibility of RA involvement.\n",
      "Generated Report : \n",
      " no bony lesion. No Boral abnormality, both feet and wrists with soft tissue swelling --> gout arthritis cannot be excluded as a possible intertarsal joint or accessory ossicle type III osteopenia degenerative change of left foot suggestive of sclerotic os trigonum Both hallux valgus (right) Lt tibia tendon insertion site at right 1st proximal phalanx base R/O stress fracture -> rheumatoid inflammatory syndrome diffuse vascular\n",
      "\n",
      "--- Example 213 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "R/O Fracture and healing at right distal fibular._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "R/O Fracture and healing at right distal fibular.\n",
      "Generated Report : \n",
      " no significant bony lesion on radiographs. sinde last study of both feet, suggested otherwise soft tissue swelling around the left 1st MTP joint area with bone erosion --> R/O gout arthritis cannot be excluded as possible mild degenerative change in right 2nd metatarsal head and tarsometrium base -> rheumatoid osteopenia severe OA (right ankle) at talonavicular joints vs medial malleolus subluxation syndrome rather than\n",
      "\n",
      "--- Example 93 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality\n",
      "Generated Report : \n",
      " No bony abnormality. Possible soft tissue swelling in lateral portion of right ankle and left talonavicular joint, medial malleolus both Rt. 1st MTP Joint - Suspicious gouty arthritis with bone erosions at multiple joints on radiographs --> Goutsky disease or old fracture cannot be excluded as periartisansia suspicious calcifications around the distal phalanx (right) -> r/o inflammatory condition degenerative change osteopenial instability\n",
      "\n",
      "--- Example 41 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_degenerative change_x000D__x000D_[CONCLUSION    ]_x000D_degenerative change_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "degenerative change\n",
      "Generated Report : \n",
      " degenerative change diffuse osteopenia Lt. 1st MTP joint OA Rt 5th MT head, erosion --> gout arthritis involvement no significant interval progression of RA or inflammatory changes - mild old fracture with erosions at both 3rd and 4nd metatarsal shafts -> r/o severe ossification os trigonum flat foot (RMT) probable tophi deposition on radiocarpinal coalition type 2 soft tissue swelling in lateral aspect subluxation hallux\n",
      "\n",
      "--- Example 94 ---\n",
      "Raw Report       : \n",
      "[FINDING       ]_x000D_degenerative change\n",
      "Lt. 1st MTP joint, R/O gout_x000D__x000D_[CONCLUSION    ]_x000D_degenerative change\n",
      "Lt. 1st MTP joint, R/O gout_x000D__x000D_[RECOMMENDATION]_x000D_-\n",
      "Cleaned Report   : \n",
      "degenerative change Lt. 1st MTP joint, R/O gout\n",
      "Generated Report : \n",
      " degenerative change hallux valgus, both mild OA. os trigonum pes planis Rt., suspicious erosion -> r/o gouty arthritis diffuse osteopenia bilateral calcaneal spur progression and erosions at Lt 1st IP joint subluxation state DIP joints soft tissue swelling with loose bodies in TMT areas or tophi around talocalcatella rather than old fracture --> RA involvement no significant interval increase flat foot - otherwise severe ossicle\n",
      "\n",
      "--- Example 90 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "degenerative change_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "degenerative change_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "degenerative change\n",
      "Generated Report : \n",
      " No bony abnormality. Slightly soft tissue swelling in medial portion of left ankle joint area and adjacent to right 1st MTP joints, both (right > Left). Soft bone erosion with sclerotic lesion --> R/O gout arthritis cannot be excluded as possible degenerative change rather than old fracture or osteopenia at Lt talocalcaneal spur -> rheumatoid ligament involvement no significant interval increase since last study --- probable inflammatory state on radiographs vs rec>\n",
      "\n",
      "--- Example 53 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No significant interval change._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No significant interval change._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No significant interval change. No significant interval change.\n",
      "Generated Report : \n",
      " No bony abnormality. Small osteophyte in left 1st MTP joint and distal phalanx, right 2nd MT head area. Suspicious calcaneocuboid bone protrusion adjacent talus neck with suspicious gouty arthritis state of both feet (right 3rd metatarsia). Soft tissue swelling over medial malleolae at lateral edge of hip joints --> R/O stress fracture or bipartite sesial cystic lesion cannot be excluded since\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import unicodedata\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        \n",
    "        # Add BOS token to the beginning of the report\n",
    "        input_text = f\"{self.tokenizer.bos_token} {clean}\"\n",
    "        \n",
    "        # Tokenize with truncation and ensuring EOS token at end\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=511, return_tensors='pt')\n",
    "        \n",
    "        # Add EOS token if not already present\n",
    "        if tok['input_ids'][0][-1] != self.tokenizer.eos_token_id:\n",
    "            input_ids = torch.cat([tok['input_ids'], torch.tensor([[self.tokenizer.eos_token_id]])], dim=1)\n",
    "            attention_mask = torch.cat([tok['attention_mask'], torch.tensor([[1]])], dim=1)\n",
    "        else:\n",
    "            input_ids = tok['input_ids']\n",
    "            attention_mask = tok['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0)\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        g = self.global_encoder(imgs)\n",
    "        g = self.global_proj(g).unsqueeze(1)\n",
    "\n",
    "        B, N, C, H, W = patches.shape\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf = (self.patch_encoder.forward_features(p)\n",
    "              if hasattr(self.patch_encoder, 'forward_features')\n",
    "              else self.patch_encoder(p))\n",
    "        pf = self._pool(pf)\n",
    "        pf = self.patch_proj(pf)\n",
    "        pf = pf.view(B, N, 768)\n",
    "\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "\n",
    "        return self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate loss using full ground truth\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            # For generation, use only BOS token as input\n",
    "            bos_tokens = torch.tensor([[tokenizer.bos_token_id]] * imgs.size(0), device=device)\n",
    "\n",
    "            g = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g).unsqueeze(1)\n",
    "\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf = model.patch_encoder(p)\n",
    "            pf = model._pool(pf)\n",
    "            pf = model.patch_proj(pf)\n",
    "            pf = pf.view(B, N, 768)\n",
    "\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos_tokens,\n",
    "                encoder_hidden_states=comb,\n",
    "                early_stopping=True,\n",
    "                attention_mask=torch.ones_like(bos_tokens),\n",
    "                max_length=100,  # Increased from 60\n",
    "                do_sample=True,\n",
    "                top_k=40,  # Adjusted from 20\n",
    "                top_p=0.9,  # Slightly increased from 0.85\n",
    "                temperature=0.7,  # Reduced from 0.9 for less randomness\n",
    "                repetition_penalty=1.3,  # Decreased from 1.8\n",
    "                no_repeat_ngram_size=2,  # Reduced from 3\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\", color='green')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "# Initialize and configure tokenizer - FIX: set left padding for decoder model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = 'left'  # Critical fix for GPT-2 decoder model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.tokenizer = tokenizer\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "# Create data splits\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize model, optimizer and training utilities\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.amp.GradScaler()  # Fixed: removed device_type parameter\n",
    "\n",
    "# Training configuration - increased from 5 to 10 epochs\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Plot training metrics\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss              : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Similarity: {test_sem:.4f}\")\n",
    "\n",
    "# Display random test examples\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Use only BOS token for generation\n",
    "    prompt = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "\n",
    "    g = model.global_encoder(fi)\n",
    "    g = model.global_proj(g).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf = model.patch_encoder(p)\n",
    "    pf = model._pool(pf)\n",
    "    pf = model.patch_proj(pf)\n",
    "    pf = pf.view(B, N, 768)\n",
    "\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    # Improved generation parameters for better medical reports\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        early_stopping=True,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=100,         # Increased from 60 for complete reports\n",
    "        do_sample=True,\n",
    "        top_k=40,               # Increased from 20 for more vocabulary options\n",
    "        top_p=0.9,              # Adjusted from 0.85 for better diversity\n",
    "        temperature=0.7,        # Reduced from 0.9 for more focused generation\n",
    "        repetition_penalty=1.3, # Reduced from 1.8 to allow necessary medical term repetition\n",
    "        no_repeat_ngram_size=2, # Reduced from 3 for more natural medical phrasing\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d3d43",
   "metadata": {},
   "source": [
    "## Printing Intermediate Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eec149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset class distribution:\n",
      "  oa: 573\n",
      "  ra: 115\n",
      "  uncertain: 620\n",
      "  oa, ra: 8\n",
      "  normal: 748\n",
      "  gout: 267\n",
      "  combination of oa, ra: 3\n",
      "  ref.prev: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/479 [00:00<03:46,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/479 [00:00<02:40,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 13]), padded attention_mask shape: torch.Size([4, 13])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 13]), mask torch.Size([4, 13])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 13, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/479 [00:00<02:19,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 21]), padded attention_mask shape: torch.Size([4, 21])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 21]), mask torch.Size([4, 21])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 21, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 4/479 [00:01<02:09,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 6/479 [00:01<01:57,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 10]), padded attention_mask shape: torch.Size([4, 10])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 10]), mask torch.Size([4, 10])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 10, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 8/479 [00:02<01:53,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([3]), attention_mask shape: torch.Size([3])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 15]), padded attention_mask shape: torch.Size([4, 15])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 15]), mask torch.Size([4, 15])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 15, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([38]), attention_mask shape: torch.Size([38])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 38]), padded attention_mask shape: torch.Size([4, 38])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 38]), mask torch.Size([4, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 9/479 [00:02<02:03,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 38, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 10/479 [00:02<02:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 37]), padded attention_mask shape: torch.Size([4, 37])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 37]), mask torch.Size([4, 37])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 37, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([42]), attention_mask shape: torch.Size([42])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/479 [00:03<02:07,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 42]), padded attention_mask shape: torch.Size([4, 42])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 42]), mask torch.Size([4, 42])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 42, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 33]), padded attention_mask shape: torch.Size([4, 33])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 33]), mask torch.Size([4, 33])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 33, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 12/479 [00:03<02:07,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 28]), padded attention_mask shape: torch.Size([4, 28])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 28]), mask torch.Size([4, 28])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 28, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 13/479 [00:03<02:08,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 22]), padded attention_mask shape: torch.Size([4, 22])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 22]), mask torch.Size([4, 22])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 22, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 14/479 [00:03<02:03,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 15/479 [00:04<01:59,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 16/479 [00:04<01:57,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([35]), attention_mask shape: torch.Size([35])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([55]), attention_mask shape: torch.Size([55])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 55]), padded attention_mask shape: torch.Size([4, 55])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 55]), mask torch.Size([4, 55])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 55, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▎         | 17/479 [00:04<01:56,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 18/479 [00:04<02:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 19/479 [00:05<02:08,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([51]), attention_mask shape: torch.Size([51])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 51]), padded attention_mask shape: torch.Size([4, 51])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 51]), mask torch.Size([4, 51])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 51, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 15]), padded attention_mask shape: torch.Size([4, 15])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 15]), mask torch.Size([4, 15])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 15, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 20/479 [00:05<02:05,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([36]), attention_mask shape: torch.Size([36])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 36]), padded attention_mask shape: torch.Size([4, 36])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 36]), mask torch.Size([4, 36])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 36, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 21/479 [00:05<02:04,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([52]), attention_mask shape: torch.Size([52])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 52]), padded attention_mask shape: torch.Size([4, 52])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 52]), mask torch.Size([4, 52])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 52, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 22/479 [00:05<01:59,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([46]), attention_mask shape: torch.Size([46])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 46]), padded attention_mask shape: torch.Size([4, 46])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 46]), mask torch.Size([4, 46])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 46, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 23/479 [00:06<01:56,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 18]), padded attention_mask shape: torch.Size([4, 18])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 18]), mask torch.Size([4, 18])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 18, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 24/479 [00:06<01:57,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 16]), padded attention_mask shape: torch.Size([4, 16])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 16]), mask torch.Size([4, 16])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 16, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 25/479 [00:06<01:59,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 17]), padded attention_mask shape: torch.Size([4, 17])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 17]), mask torch.Size([4, 17])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 17, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 26/479 [00:06<02:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 22]), padded attention_mask shape: torch.Size([4, 22])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 22]), mask torch.Size([4, 22])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 22, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 27/479 [00:07<01:55,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([61]), attention_mask shape: torch.Size([61])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 61]), padded attention_mask shape: torch.Size([4, 61])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 61]), mask torch.Size([4, 61])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 61, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 28/479 [00:07<01:56,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([58]), attention_mask shape: torch.Size([58])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([25]), attention_mask shape: torch.Size([25])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 58]), padded attention_mask shape: torch.Size([4, 58])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 58]), mask torch.Size([4, 58])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 58, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 29/479 [00:07<01:55,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([49]), attention_mask shape: torch.Size([49])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 49]), padded attention_mask shape: torch.Size([4, 49])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 49]), mask torch.Size([4, 49])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 49, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 30/479 [00:07<01:54,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 31]), padded attention_mask shape: torch.Size([4, 31])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 31]), mask torch.Size([4, 31])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 31, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 31/479 [00:08<01:56,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 31]), padded attention_mask shape: torch.Size([4, 31])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 31]), mask torch.Size([4, 31])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 31, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 32/479 [00:08<01:58,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 33/479 [00:08<01:57,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 33]), padded attention_mask shape: torch.Size([4, 33])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 33]), mask torch.Size([4, 33])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 33, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 34/479 [00:09<01:56,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([76]), attention_mask shape: torch.Size([76])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 76]), padded attention_mask shape: torch.Size([4, 76])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 76]), mask torch.Size([4, 76])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 76, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 35/479 [00:09<01:52,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 36/479 [00:09<01:52,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 10]), padded attention_mask shape: torch.Size([4, 10])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 10]), mask torch.Size([4, 10])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 10, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 37/479 [00:09<01:55,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 38/479 [00:10<01:57,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([38]), attention_mask shape: torch.Size([38])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 38]), padded attention_mask shape: torch.Size([4, 38])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 38]), mask torch.Size([4, 38])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 38, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 39/479 [00:10<01:57,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([32]), attention_mask shape: torch.Size([32])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 32]), padded attention_mask shape: torch.Size([4, 32])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 32]), mask torch.Size([4, 32])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 32, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 40/479 [00:10<01:53,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 41/479 [00:10<01:58,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([84]), attention_mask shape: torch.Size([84])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 84]), padded attention_mask shape: torch.Size([4, 84])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 84]), mask torch.Size([4, 84])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 84, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 42/479 [00:11<01:53,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 28]), padded attention_mask shape: torch.Size([4, 28])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 28]), mask torch.Size([4, 28])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 28, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 43/479 [00:11<01:52,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 44/479 [00:11<01:48,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([42]), attention_mask shape: torch.Size([42])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 42]), padded attention_mask shape: torch.Size([4, 42])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 42]), mask torch.Size([4, 42])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 42, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 45/479 [00:11<01:52,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([36]), attention_mask shape: torch.Size([36])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 36]), padded attention_mask shape: torch.Size([4, 36])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 36]), mask torch.Size([4, 36])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 36, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 46/479 [00:12<01:52,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([68]), attention_mask shape: torch.Size([68])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 68]), padded attention_mask shape: torch.Size([4, 68])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 68]), mask torch.Size([4, 68])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 68, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 47/479 [00:12<01:53,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 13]), padded attention_mask shape: torch.Size([4, 13])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 13]), mask torch.Size([4, 13])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 13, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 48/479 [00:12<01:49,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([57]), attention_mask shape: torch.Size([57])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 57]), padded attention_mask shape: torch.Size([4, 57])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 57]), mask torch.Size([4, 57])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 57, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 49/479 [00:12<01:51,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 31]), padded attention_mask shape: torch.Size([4, 31])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 31]), mask torch.Size([4, 31])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 31, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 50/479 [00:13<01:51,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([41]), attention_mask shape: torch.Size([41])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([19]), attention_mask shape: torch.Size([19])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 41]), padded attention_mask shape: torch.Size([4, 41])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 41]), mask torch.Size([4, 41])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 51/479 [00:13<01:55,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 41, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([32]), attention_mask shape: torch.Size([32])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 52/479 [00:13<01:52,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 53/479 [00:13<01:50,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([47]), attention_mask shape: torch.Size([47])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 47]), padded attention_mask shape: torch.Size([4, 47])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 47]), mask torch.Size([4, 47])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 47, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 54/479 [00:14<01:49,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 29]), padded attention_mask shape: torch.Size([4, 29])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 29]), mask torch.Size([4, 29])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 29, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 17]), padded attention_mask shape: torch.Size([4, 17])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 17]), mask torch.Size([4, 17])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 17, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█▏        | 55/479 [00:14<01:51,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([36]), attention_mask shape: torch.Size([36])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 36]), padded attention_mask shape: torch.Size([4, 36])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 36]), mask torch.Size([4, 36])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 36, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 56/479 [00:14<01:51,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 18]), padded attention_mask shape: torch.Size([4, 18])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 18]), mask torch.Size([4, 18])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 18, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 57/479 [00:14<01:48,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 58/479 [00:15<01:50,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([19]), attention_mask shape: torch.Size([19])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 31]), padded attention_mask shape: torch.Size([4, 31])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 31]), mask torch.Size([4, 31])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 31, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 59/479 [00:15<01:51,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 20]), padded attention_mask shape: torch.Size([4, 20])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 20]), mask torch.Size([4, 20])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 20, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 60/479 [00:15<01:47,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 28]), padded attention_mask shape: torch.Size([4, 28])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 28]), mask torch.Size([4, 28])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 28, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 61/479 [00:16<01:45,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([50]), attention_mask shape: torch.Size([50])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 50]), padded attention_mask shape: torch.Size([4, 50])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 50]), mask torch.Size([4, 50])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 50, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 62/479 [00:16<01:47,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 22]), padded attention_mask shape: torch.Size([4, 22])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 22]), mask torch.Size([4, 22])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 22, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 63/479 [00:16<01:50,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 26]), padded attention_mask shape: torch.Size([4, 26])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 26]), mask torch.Size([4, 26])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 26, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 64/479 [00:16<01:52,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 17]), padded attention_mask shape: torch.Size([4, 17])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 17]), mask torch.Size([4, 17])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 17, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▎        | 65/479 [00:17<01:47,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([41]), attention_mask shape: torch.Size([41])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 41]), padded attention_mask shape: torch.Size([4, 41])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 41]), mask torch.Size([4, 41])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 41, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 66/479 [00:17<01:47,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([45]), attention_mask shape: torch.Size([45])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 45]), padded attention_mask shape: torch.Size([4, 45])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 45]), mask torch.Size([4, 45])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 45, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 67/479 [00:17<01:45,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 68/479 [00:17<01:45,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 21]), padded attention_mask shape: torch.Size([4, 21])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 21]), mask torch.Size([4, 21])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 21, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 69/479 [00:18<01:48,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([76]), attention_mask shape: torch.Size([76])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 76]), padded attention_mask shape: torch.Size([4, 76])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 76]), mask torch.Size([4, 76])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 76, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 70/479 [00:18<01:46,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([25]), attention_mask shape: torch.Size([25])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([19]), attention_mask shape: torch.Size([19])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 25]), padded attention_mask shape: torch.Size([4, 25])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 25]), mask torch.Size([4, 25])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 25, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 71/479 [00:18<01:49,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 72/479 [00:18<01:50,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 15]), padded attention_mask shape: torch.Size([4, 15])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 15]), mask torch.Size([4, 15])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 15, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 73/479 [00:19<01:51,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 74/479 [00:19<01:46,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([43]), attention_mask shape: torch.Size([43])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([64]), attention_mask shape: torch.Size([64])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 64]), padded attention_mask shape: torch.Size([4, 64])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 64]), mask torch.Size([4, 64])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 75/479 [00:19<01:50,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 64, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 76/479 [00:20<01:51,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([32]), attention_mask shape: torch.Size([32])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 32]), padded attention_mask shape: torch.Size([4, 32])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 32]), mask torch.Size([4, 32])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 32, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 77/479 [00:20<01:51,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▋        | 78/479 [00:20<01:50,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([25]), attention_mask shape: torch.Size([25])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 25]), padded attention_mask shape: torch.Size([4, 25])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 25]), mask torch.Size([4, 25])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 25, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▋        | 79/479 [00:20<01:50,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 80/479 [00:21<01:47,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 33]), padded attention_mask shape: torch.Size([4, 33])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 33]), mask torch.Size([4, 33])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 33, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 81/479 [00:21<01:47,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 21]), padded attention_mask shape: torch.Size([4, 21])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 21]), mask torch.Size([4, 21])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 21, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 82/479 [00:21<01:44,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([77]), attention_mask shape: torch.Size([77])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 77]), padded attention_mask shape: torch.Size([4, 77])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 77]), mask torch.Size([4, 77])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 77, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 83/479 [00:21<01:43,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([46]), attention_mask shape: torch.Size([46])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 46]), padded attention_mask shape: torch.Size([4, 46])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 46]), mask torch.Size([4, 46])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 46, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 84/479 [00:22<01:42,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 85/479 [00:22<01:41,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 16]), padded attention_mask shape: torch.Size([4, 16])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 16]), mask torch.Size([4, 16])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 16, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 86/479 [00:22<01:37,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([32]), attention_mask shape: torch.Size([32])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 33]), padded attention_mask shape: torch.Size([4, 33])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 33]), mask torch.Size([4, 33])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 33, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 87/479 [00:22<01:41,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([71]), attention_mask shape: torch.Size([71])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 71]), padded attention_mask shape: torch.Size([4, 71])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 71]), mask torch.Size([4, 71])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 88/479 [00:23<01:44,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 71, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([76]), attention_mask shape: torch.Size([76])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▊        | 89/479 [00:23<01:44,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([47]), attention_mask shape: torch.Size([47])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 76]), padded attention_mask shape: torch.Size([4, 76])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 76]), mask torch.Size([4, 76])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 76, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 90/479 [00:23<01:43,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([42]), attention_mask shape: torch.Size([42])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 42]), padded attention_mask shape: torch.Size([4, 42])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 42]), mask torch.Size([4, 42])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 42, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 91/479 [00:23<01:40,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 18]), padded attention_mask shape: torch.Size([4, 18])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 18]), mask torch.Size([4, 18])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 18, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 92/479 [00:24<01:41,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([19]), attention_mask shape: torch.Size([19])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 93/479 [00:24<01:43,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 94/479 [00:24<01:41,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 20]), padded attention_mask shape: torch.Size([4, 20])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 20]), mask torch.Size([4, 20])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 95/479 [00:25<01:45,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 20, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 96/479 [00:25<01:41,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 97/479 [00:25<01:41,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 30]), padded attention_mask shape: torch.Size([4, 30])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 30]), mask torch.Size([4, 30])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 30, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 15]), padded attention_mask shape: torch.Size([4, 15])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 15]), mask torch.Size([4, 15])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 15, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 98/479 [00:25<01:42,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 37]), padded attention_mask shape: torch.Size([4, 37])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 37]), mask torch.Size([4, 37])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 99/479 [00:26<01:45,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 37, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([32]), attention_mask shape: torch.Size([32])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([41]), attention_mask shape: torch.Size([41])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 100/479 [00:26<01:45,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([54]), attention_mask shape: torch.Size([54])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 54]), padded attention_mask shape: torch.Size([4, 54])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 54]), mask torch.Size([4, 54])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 54, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 37]), padded attention_mask shape: torch.Size([4, 37])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 37]), mask torch.Size([4, 37])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 37, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 101/479 [00:26<01:47,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 17]), padded attention_mask shape: torch.Size([4, 17])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 17]), mask torch.Size([4, 17])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 17, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 102/479 [00:27<01:45,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 103/479 [00:27<01:42,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 104/479 [00:27<01:41,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([74]), attention_mask shape: torch.Size([74])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 74]), padded attention_mask shape: torch.Size([4, 74])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 74]), mask torch.Size([4, 74])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 74, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 105/479 [00:27<01:39,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 106/479 [00:28<01:36,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 107/479 [00:28<01:36,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([76]), attention_mask shape: torch.Size([76])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 76]), padded attention_mask shape: torch.Size([4, 76])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 76]), mask torch.Size([4, 76])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 108/479 [00:28<01:40,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 76, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 109/479 [00:28<01:44,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 9]), padded attention_mask shape: torch.Size([4, 9])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 9]), mask torch.Size([4, 9])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 9, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([38]), attention_mask shape: torch.Size([38])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 38]), padded attention_mask shape: torch.Size([4, 38])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 38]), mask torch.Size([4, 38])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 38, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 110/479 [00:29<01:44,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 13]), padded attention_mask shape: torch.Size([4, 13])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 13]), mask torch.Size([4, 13])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 13, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 111/479 [00:29<01:40,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([54]), attention_mask shape: torch.Size([54])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([50]), attention_mask shape: torch.Size([50])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 54]), padded attention_mask shape: torch.Size([4, 54])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 54]), mask torch.Size([4, 54])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 112/479 [00:29<01:43,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 54, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 113/479 [00:30<01:43,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 20]), padded attention_mask shape: torch.Size([4, 20])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 20]), mask torch.Size([4, 20])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 20, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([36]), attention_mask shape: torch.Size([36])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 36]), padded attention_mask shape: torch.Size([4, 36])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 36]), mask torch.Size([4, 36])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 36, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 114/479 [00:30<01:43,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([19]), attention_mask shape: torch.Size([19])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 37]), padded attention_mask shape: torch.Size([4, 37])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 37]), mask torch.Size([4, 37])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 115/479 [00:30<01:46,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 37, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 116/479 [00:30<01:44,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([25]), attention_mask shape: torch.Size([25])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 25]), padded attention_mask shape: torch.Size([4, 25])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 25]), mask torch.Size([4, 25])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 25, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 117/479 [00:31<01:39,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 118/479 [00:31<01:35,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 18]), padded attention_mask shape: torch.Size([4, 18])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 18]), mask torch.Size([4, 18])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 18, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 29]), padded attention_mask shape: torch.Size([4, 29])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 29]), mask torch.Size([4, 29])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 29, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 119/479 [00:31<01:37,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([47]), attention_mask shape: torch.Size([47])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 47]), padded attention_mask shape: torch.Size([4, 47])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 47]), mask torch.Size([4, 47])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 47, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 120/479 [00:31<01:36,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([41]), attention_mask shape: torch.Size([41])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 41]), padded attention_mask shape: torch.Size([4, 41])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 41]), mask torch.Size([4, 41])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 41, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 121/479 [00:32<01:34,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 21]), padded attention_mask shape: torch.Size([4, 21])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 21]), mask torch.Size([4, 21])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 122/479 [00:32<01:37,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 21, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 123/479 [00:32<01:36,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([48]), attention_mask shape: torch.Size([48])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 124/479 [00:33<01:37,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 48]), padded attention_mask shape: torch.Size([4, 48])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 48]), mask torch.Size([4, 48])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 48, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([68]), attention_mask shape: torch.Size([68])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 68]), padded attention_mask shape: torch.Size([4, 68])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 68]), mask torch.Size([4, 68])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 68, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 125/479 [00:33<01:38,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▋       | 126/479 [00:33<01:36,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([55]), attention_mask shape: torch.Size([55])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 55]), padded attention_mask shape: torch.Size([4, 55])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 55]), mask torch.Size([4, 55])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 55, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 127/479 [00:33<01:33,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 128/479 [00:34<01:35,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 129/479 [00:34<01:34,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([49]), attention_mask shape: torch.Size([49])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 49]), padded attention_mask shape: torch.Size([4, 49])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 49]), mask torch.Size([4, 49])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 49, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 130/479 [00:34<01:31,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 30]), padded attention_mask shape: torch.Size([4, 30])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 30]), mask torch.Size([4, 30])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 30, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 131/479 [00:34<01:31,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 132/479 [00:35<01:32,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([67]), attention_mask shape: torch.Size([67])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 67]), padded attention_mask shape: torch.Size([4, 67])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 67]), mask torch.Size([4, 67])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 67, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 133/479 [00:35<01:33,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([49]), attention_mask shape: torch.Size([49])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 49]), padded attention_mask shape: torch.Size([4, 49])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 49]), mask torch.Size([4, 49])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 49, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 134/479 [00:35<01:29,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([25]), attention_mask shape: torch.Size([25])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 28]), padded attention_mask shape: torch.Size([4, 28])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 28]), mask torch.Size([4, 28])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 28, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 135/479 [00:35<01:30,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 136/479 [00:36<01:28,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([41]), attention_mask shape: torch.Size([41])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 41]), padded attention_mask shape: torch.Size([4, 41])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 41]), mask torch.Size([4, 41])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 41, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 137/479 [00:36<01:27,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 138/479 [00:36<01:29,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 139/479 [00:37<01:33,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([105]), attention_mask shape: torch.Size([105])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 105]), padded attention_mask shape: torch.Size([4, 105])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 105]), mask torch.Size([4, 105])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 105, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 140/479 [00:37<01:30,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 26]), padded attention_mask shape: torch.Size([4, 26])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 26]), mask torch.Size([4, 26])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 26, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 141/479 [00:37<01:30,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([39]), attention_mask shape: torch.Size([39])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 39]), padded attention_mask shape: torch.Size([4, 39])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 39]), mask torch.Size([4, 39])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 39, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 142/479 [00:37<01:30,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([35]), attention_mask shape: torch.Size([35])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 37]), padded attention_mask shape: torch.Size([4, 37])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 37]), mask torch.Size([4, 37])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 37, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 143/479 [00:38<01:27,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 144/479 [00:38<01:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 145/479 [00:38<01:26,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 146/479 [00:38<01:25,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 17]), padded attention_mask shape: torch.Size([4, 17])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 17]), mask torch.Size([4, 17])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 17, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([38]), attention_mask shape: torch.Size([38])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 147/479 [00:39<01:26,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([42]), attention_mask shape: torch.Size([42])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 42]), padded attention_mask shape: torch.Size([4, 42])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 42]), mask torch.Size([4, 42])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 42, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([19]), attention_mask shape: torch.Size([19])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 19]), padded attention_mask shape: torch.Size([4, 19])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 19]), mask torch.Size([4, 19])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 19, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 148/479 [00:39<01:29,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([73]), attention_mask shape: torch.Size([73])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([49]), attention_mask shape: torch.Size([49])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 149/479 [00:39<01:34,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 73]), padded attention_mask shape: torch.Size([4, 73])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 73]), mask torch.Size([4, 73])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 73, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███▏      | 150/479 [00:39<01:31,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 15]), padded attention_mask shape: torch.Size([4, 15])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 15]), mask torch.Size([4, 15])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 15, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 151/479 [00:40<01:29,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([35]), attention_mask shape: torch.Size([35])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 35]), padded attention_mask shape: torch.Size([4, 35])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 35]), mask torch.Size([4, 35])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 35, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 152/479 [00:40<01:31,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 153/479 [00:40<01:29,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 154/479 [00:41<01:28,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 155/479 [00:41<01:25,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([39]), attention_mask shape: torch.Size([39])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 39]), padded attention_mask shape: torch.Size([4, 39])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 39]), mask torch.Size([4, 39])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 39, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 156/479 [00:41<01:22,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([44]), attention_mask shape: torch.Size([44])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 44]), padded attention_mask shape: torch.Size([4, 44])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 44]), mask torch.Size([4, 44])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 44, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 157/479 [00:41<01:25,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 31]), padded attention_mask shape: torch.Size([4, 31])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 31]), mask torch.Size([4, 31])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 158/479 [00:42<01:27,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 31, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 159/479 [00:42<01:28,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 160/479 [00:42<01:31,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▎      | 161/479 [00:42<01:26,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([42]), attention_mask shape: torch.Size([42])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 42]), padded attention_mask shape: torch.Size([4, 42])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 42]), mask torch.Size([4, 42])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 42, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 162/479 [00:43<01:26,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 29]), padded attention_mask shape: torch.Size([4, 29])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 29]), mask torch.Size([4, 29])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 163/479 [00:43<01:28,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 29, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 164/479 [00:43<01:27,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 16]), padded attention_mask shape: torch.Size([4, 16])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 16]), mask torch.Size([4, 16])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 16, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 165/479 [00:44<01:27,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([104]), attention_mask shape: torch.Size([104])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 104]), padded attention_mask shape: torch.Size([4, 104])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 104]), mask torch.Size([4, 104])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 104, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 166/479 [00:44<01:22,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([43]), attention_mask shape: torch.Size([43])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 43]), padded attention_mask shape: torch.Size([4, 43])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 43]), mask torch.Size([4, 43])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 43, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 167/479 [00:44<01:22,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([50]), attention_mask shape: torch.Size([50])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 50]), padded attention_mask shape: torch.Size([4, 50])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 50]), mask torch.Size([4, 50])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 50, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 168/479 [00:44<01:20,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 21]), padded attention_mask shape: torch.Size([4, 21])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 21]), mask torch.Size([4, 21])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 21, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 169/479 [00:45<01:21,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 20]), padded attention_mask shape: torch.Size([4, 20])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 20]), mask torch.Size([4, 20])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 20, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 170/479 [00:45<01:21,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 171/479 [00:45<01:22,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 172/479 [00:45<01:22,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([46]), attention_mask shape: torch.Size([46])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 46]), padded attention_mask shape: torch.Size([4, 46])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 46]), mask torch.Size([4, 46])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 46, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 173/479 [00:46<01:22,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▋      | 174/479 [00:46<01:22,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 28]), padded attention_mask shape: torch.Size([4, 28])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 28]), mask torch.Size([4, 28])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 175/479 [00:46<01:24,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 28, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 176/479 [00:47<01:21,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([47]), attention_mask shape: torch.Size([47])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 47]), padded attention_mask shape: torch.Size([4, 47])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 47]), mask torch.Size([4, 47])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 47, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 177/479 [00:47<01:21,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 14]), padded attention_mask shape: torch.Size([4, 14])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 14]), mask torch.Size([4, 14])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 14, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([38]), attention_mask shape: torch.Size([38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 178/479 [00:47<01:18,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 38]), padded attention_mask shape: torch.Size([4, 38])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 38]), mask torch.Size([4, 38])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 38, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 179/479 [00:47<01:21,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([60]), attention_mask shape: torch.Size([60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 180/479 [00:48<01:20,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([19]), attention_mask shape: torch.Size([19])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 60]), padded attention_mask shape: torch.Size([4, 60])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 60]), mask torch.Size([4, 60])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 60, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 181/479 [00:48<01:16,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 28]), padded attention_mask shape: torch.Size([4, 28])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 28]), mask torch.Size([4, 28])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 28, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 182/479 [00:48<01:15,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([66]), attention_mask shape: torch.Size([66])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 66]), padded attention_mask shape: torch.Size([4, 66])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 66]), mask torch.Size([4, 66])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 66, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 183/479 [00:48<01:15,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([59]), attention_mask shape: torch.Size([59])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 59]), padded attention_mask shape: torch.Size([4, 59])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 59]), mask torch.Size([4, 59])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 59, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 184/479 [00:49<01:15,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 33]), padded attention_mask shape: torch.Size([4, 33])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 33]), mask torch.Size([4, 33])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 33, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▊      | 185/479 [00:49<01:15,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 186/479 [00:49<01:17,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([52]), attention_mask shape: torch.Size([52])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 52]), padded attention_mask shape: torch.Size([4, 52])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 52]), mask torch.Size([4, 52])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 187/479 [00:49<01:20,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 52, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([45]), attention_mask shape: torch.Size([45])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([39]), attention_mask shape: torch.Size([39])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 188/479 [00:50<01:20,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([32]), attention_mask shape: torch.Size([32])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 45]), padded attention_mask shape: torch.Size([4, 45])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 45]), mask torch.Size([4, 45])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 45, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([53]), attention_mask shape: torch.Size([53])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 53]), padded attention_mask shape: torch.Size([4, 53])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 53]), mask torch.Size([4, 53])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 189/479 [00:50<01:25,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 53, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([50]), attention_mask shape: torch.Size([50])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 190/479 [00:50<01:20,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 50]), padded attention_mask shape: torch.Size([4, 50])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 50]), mask torch.Size([4, 50])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 50, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 191/479 [00:51<01:24,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([38]), attention_mask shape: torch.Size([38])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 38]), padded attention_mask shape: torch.Size([4, 38])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 38]), mask torch.Size([4, 38])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 38, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 192/479 [00:51<01:21,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 16]), padded attention_mask shape: torch.Size([4, 16])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 16]), mask torch.Size([4, 16])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 16, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 193/479 [00:51<01:19,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 30]), padded attention_mask shape: torch.Size([4, 30])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 30]), mask torch.Size([4, 30])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 30, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 194/479 [00:51<01:16,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([18]), attention_mask shape: torch.Size([18])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 30]), padded attention_mask shape: torch.Size([4, 30])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 30]), mask torch.Size([4, 30])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 30, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 195/479 [00:52<01:18,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 31]), padded attention_mask shape: torch.Size([4, 31])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 31]), mask torch.Size([4, 31])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 31, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 196/479 [00:52<01:15,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 197/479 [00:52<01:13,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 15]), padded attention_mask shape: torch.Size([4, 15])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 15]), mask torch.Size([4, 15])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 15, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████▏     | 198/479 [00:52<01:13,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([30]), attention_mask shape: torch.Size([30])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 30]), padded attention_mask shape: torch.Size([4, 30])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 30]), mask torch.Size([4, 30])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 199/479 [00:53<01:15,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 30, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 200/479 [00:53<01:14,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 201/479 [00:53<01:11,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([72]), attention_mask shape: torch.Size([72])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 72]), padded attention_mask shape: torch.Size([4, 72])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 72]), mask torch.Size([4, 72])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 72, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 202/479 [00:53<01:12,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 34]), padded attention_mask shape: torch.Size([4, 34])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 34]), mask torch.Size([4, 34])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 34, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 203/479 [00:54<01:11,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([90]), attention_mask shape: torch.Size([90])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 90]), padded attention_mask shape: torch.Size([4, 90])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 90]), mask torch.Size([4, 90])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 90, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 204/479 [00:54<01:09,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 205/479 [00:54<01:08,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 37]), padded attention_mask shape: torch.Size([4, 37])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 37]), mask torch.Size([4, 37])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 37, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 206/479 [00:54<01:07,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 13]), padded attention_mask shape: torch.Size([4, 13])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 13]), mask torch.Size([4, 13])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 13, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 207/479 [00:55<01:10,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 16]), padded attention_mask shape: torch.Size([4, 16])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 16]), mask torch.Size([4, 16])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 16, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 208/479 [00:55<01:11,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 16]), padded attention_mask shape: torch.Size([4, 16])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 16]), mask torch.Size([4, 16])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▎     | 209/479 [00:55<01:13,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 16, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([35]), attention_mask shape: torch.Size([35])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 210/479 [00:56<01:14,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 35]), padded attention_mask shape: torch.Size([4, 35])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 35]), mask torch.Size([4, 35])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 35, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 211/479 [00:56<01:14,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 37]), padded attention_mask shape: torch.Size([4, 37])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 37]), mask torch.Size([4, 37])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 37, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([46]), attention_mask shape: torch.Size([46])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 46]), padded attention_mask shape: torch.Size([4, 46])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 46]), mask torch.Size([4, 46])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 46, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 212/479 [00:56<01:13,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([74]), attention_mask shape: torch.Size([74])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 74]), padded attention_mask shape: torch.Size([4, 74])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 74]), mask torch.Size([4, 74])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 74, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 213/479 [00:56<01:11,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([22]), attention_mask shape: torch.Size([22])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 22]), padded attention_mask shape: torch.Size([4, 22])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 22]), mask torch.Size([4, 22])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 22, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 214/479 [00:57<01:09,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 12]), padded attention_mask shape: torch.Size([4, 12])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 12]), mask torch.Size([4, 12])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 12, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 215/479 [00:57<01:09,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([25]), attention_mask shape: torch.Size([25])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([49]), attention_mask shape: torch.Size([49])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 49]), padded attention_mask shape: torch.Size([4, 49])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 49]), mask torch.Size([4, 49])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 49, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 216/479 [00:57<01:10,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 21]), padded attention_mask shape: torch.Size([4, 21])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 21]), mask torch.Size([4, 21])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 21, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 217/479 [00:57<01:12,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([29]), attention_mask shape: torch.Size([29])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 29]), padded attention_mask shape: torch.Size([4, 29])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 29]), mask torch.Size([4, 29])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 29, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 218/479 [00:58<01:09,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([23]), attention_mask shape: torch.Size([23])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 23]), padded attention_mask shape: torch.Size([4, 23])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 23]), mask torch.Size([4, 23])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 23, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 219/479 [00:58<01:07,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([31]), attention_mask shape: torch.Size([31])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([15]), attention_mask shape: torch.Size([15])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([21]), attention_mask shape: torch.Size([21])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 31]), padded attention_mask shape: torch.Size([4, 31])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 31]), mask torch.Size([4, 31])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 31, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 220/479 [00:58<01:07,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([27]), attention_mask shape: torch.Size([27])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 221/479 [00:59<01:13,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 27]), padded attention_mask shape: torch.Size([4, 27])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 27]), mask torch.Size([4, 27])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 27, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([68]), attention_mask shape: torch.Size([68])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▋     | 222/479 [00:59<01:11,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 68]), padded attention_mask shape: torch.Size([4, 68])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 68]), mask torch.Size([4, 68])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 68, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([14]), attention_mask shape: torch.Size([14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 223/479 [00:59<01:09,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([11]), attention_mask shape: torch.Size([11])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 17]), padded attention_mask shape: torch.Size([4, 17])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 17]), mask torch.Size([4, 17])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 17, 50257])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([111]), attention_mask shape: torch.Size([111])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([38]), attention_mask shape: torch.Size([38])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([47]), attention_mask shape: torch.Size([47])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 111]), padded attention_mask shape: torch.Size([4, 111])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 111]), mask torch.Size([4, 111])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 111, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 224/479 [00:59<01:09,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([28]), attention_mask shape: torch.Size([28])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 33]), padded attention_mask shape: torch.Size([4, 33])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 33]), mask torch.Size([4, 33])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 33, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 225/479 [01:00<01:08,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([48]), attention_mask shape: torch.Size([48])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([37]), attention_mask shape: torch.Size([37])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([64]), attention_mask shape: torch.Size([64])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 64]), padded attention_mask shape: torch.Size([4, 64])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 64]), mask torch.Size([4, 64])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 64, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 226/479 [01:00<01:06,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([95]), attention_mask shape: torch.Size([95])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([34]), attention_mask shape: torch.Size([34])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([8]), attention_mask shape: torch.Size([8])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 95]), padded attention_mask shape: torch.Size([4, 95])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 95]), mask torch.Size([4, 95])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 95, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 227/479 [01:00<01:06,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([20]), attention_mask shape: torch.Size([20])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([7]), attention_mask shape: torch.Size([7])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 20]), padded attention_mask shape: torch.Size([4, 20])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 20]), mask torch.Size([4, 20])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 20, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 228/479 [01:00<01:06,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([26]), attention_mask shape: torch.Size([26])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([16]), attention_mask shape: torch.Size([16])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([13]), attention_mask shape: torch.Size([13])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 26]), padded attention_mask shape: torch.Size([4, 26])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 26]), mask torch.Size([4, 26])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 26, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 229/479 [01:01<01:06,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([49]), attention_mask shape: torch.Size([49])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 49]), padded attention_mask shape: torch.Size([4, 49])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 49]), mask torch.Size([4, 49])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 49, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 230/479 [01:01<01:08,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([24]), attention_mask shape: torch.Size([24])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([12]), attention_mask shape: torch.Size([12])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 24]), padded attention_mask shape: torch.Size([4, 24])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 24]), mask torch.Size([4, 24])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 24, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 231/479 [01:01<01:09,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([36]), attention_mask shape: torch.Size([36])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([5]), attention_mask shape: torch.Size([5])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([33]), attention_mask shape: torch.Size([33])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([9]), attention_mask shape: torch.Size([9])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 36]), padded attention_mask shape: torch.Size([4, 36])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 36]), mask torch.Size([4, 36])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n",
      "[Model] decoder logits shape: torch.Size([4, 36, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 232/479 [01:02<01:09,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([17]), attention_mask shape: torch.Size([17])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([6]), attention_mask shape: torch.Size([6])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([40]), attention_mask shape: torch.Size([40])\n",
      "[Dataset] full_img shape: torch.Size([3, 224, 224])\n",
      "[Dataset] patches_tensor shape: torch.Size([34, 3, 112, 112])\n",
      "[Dataset] input_ids shape: torch.Size([10]), attention_mask shape: torch.Size([10])\n",
      "[Collate] stacked full_imgs shape: torch.Size([4, 3, 224, 224])\n",
      "[Collate] stacked patches shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Collate] padded input_ids shape: torch.Size([4, 40]), padded attention_mask shape: torch.Size([4, 40])\n",
      "[Train] batch full_imgs torch.Size([4, 3, 224, 224]), patches torch.Size([4, 34, 3, 112, 112]), input_ids torch.Size([4, 40]), mask torch.Size([4, 40])\n",
      "[Model] global_encoder output shape: torch.Size([4, 1024])\n",
      "[Model] global_proj + unsqueeze shape: torch.Size([4, 1, 768])\n",
      "[Model] patches input shape: torch.Size([4, 34, 3, 112, 112])\n",
      "[Model] patch_encoder output shape: torch.Size([136, 2048, 4, 4])\n",
      "[Model] patch_proj + reshape shape: torch.Size([4, 34, 768])\n",
      "[Model] concatenated features shape: torch.Size([4, 35, 768])\n",
      "[Model] after attention & norm shape: torch.Size([4, 35, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] decoder logits shape: torch.Size([4, 40, 50257])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 522\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 522\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     val_loss, gen_txt, gt_txt \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n\u001b[1;32m    524\u001b[0m     sem \u001b[38;5;241m=\u001b[39m compute_semantic_similarity(gen_txt, gt_txt)\n",
      "Cell \u001b[0;32mIn[2], line 379\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, scaler, device)\u001b[0m\n\u001b[1;32m    377\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(imgs, pts, ids, msk, decoder_labels\u001b[38;5;241m=\u001b[39mids)\n\u001b[1;32m    378\u001b[0m     loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m--> 379\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m    381\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rsna/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import unicodedata \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        print(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    print(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    print(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        print(f\"[Dataset] full_img shape: {img.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        print(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "        \n",
    "        input_text = f\"{self.tokenizer.bos_token} {clean}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=511, return_tensors='pt')\n",
    "        \n",
    "        if tok['input_ids'][0][-1] != self.tokenizer.eos_token_id:\n",
    "            input_ids = torch.cat([tok['input_ids'], torch.tensor([[self.tokenizer.eos_token_id]])], dim=1)\n",
    "            attention_mask = torch.cat([tok['attention_mask'], torch.tensor([[1]])], dim=1)\n",
    "        else:\n",
    "            input_ids = tok['input_ids']\n",
    "            attention_mask = tok['attention_mask']\n",
    "\n",
    "        input_ids = input_ids.squeeze(0)\n",
    "        attention_mask = attention_mask.squeeze(0)\n",
    "        print(f\"[Dataset] input_ids shape: {input_ids.shape}, attention_mask shape: {attention_mask.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    print(f\"[Collate] stacked full_imgs shape: {imgs.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "    print(f\"[Collate] stacked patches shape: {patches.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "    print(f\"[Collate] padded input_ids shape: {ids.shape}, padded attention_mask shape: {masks.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        # Global image\n",
    "        g_feats = self.global_encoder(imgs)\n",
    "        print(f\"[Model] global_encoder output shape: {g_feats.shape}\")  # ‼️ SHAPE\n",
    "        g = self.global_proj(g_feats).unsqueeze(1)\n",
    "        print(f\"[Model] global_proj + unsqueeze shape: {g.shape}\")    # ‼️ SHAPE\n",
    "\n",
    "        # Patch images\n",
    "        B, N, C, H, W = patches.shape\n",
    "        print(f\"[Model] patches input shape: {patches.shape}\")         # ‼️ SHAPE\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf_feats = (self.patch_encoder.forward_features(p)\n",
    "                    if hasattr(self.patch_encoder, 'forward_features')\n",
    "                    else self.patch_encoder(p))\n",
    "        print(f\"[Model] patch_encoder output shape: {pf_feats.shape}\") # ‼️ SHAPE\n",
    "        pf_pooled = self._pool(pf_feats)\n",
    "        pf = self.patch_proj(pf_pooled).view(B, N, 768)\n",
    "        print(f\"[Model] patch_proj + reshape shape: {pf.shape}\")       # ‼️ SHAPE\n",
    "\n",
    "        # Combine\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        print(f\"[Model] concatenated features shape: {cat.shape}\")     # ‼️ SHAPE\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "        print(f\"[Model] after attention & norm shape: {comb.shape}\")   # ‼️ SHAPE\n",
    "\n",
    "        # Decoder\n",
    "        out = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "        print(f\"[Model] decoder logits shape: {out.logits.shape}\")     # ‼️ SHAPE\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        print(f\"[Train] batch full_imgs {imgs.shape}, patches {pts.shape}, input_ids {ids.shape}, mask {msk.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        print(f\"[Eval] batch full_imgs {imgs.shape}, patches {pts.shape}, input_ids {ids.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            bos_tokens = torch.tensor([[tokenizer.bos_token_id]] * imgs.size(0), device=device)\n",
    "            print(f\"[Eval] bos_tokens shape: {bos_tokens.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "            # Feature reuse\n",
    "            g_feats = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g_feats).unsqueeze(1)\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf_feats = model.patch_encoder(p)\n",
    "            pf_pooled = model._pool(pf_feats)\n",
    "            pf = model.patch_proj(pf_pooled).view(B, N, 768)\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos_tokens,\n",
    "                encoder_hidden_states=comb,\n",
    "                early_stopping=True,\n",
    "                attention_mask=torch.ones_like(bos_tokens),\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=40,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.3,\n",
    "                no_repeat_ngram_size=2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            print(f\"[Eval] gen_ids shape: {gen_ids.shape}\")  # ‼️ SHAPE\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "# Initialize and configure tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = 'left'  \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.tokenizer = tokenizer\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "print(\"\\nDataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "# Split\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model, optimizer, scheduler, scaler\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# Training\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss               : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Similarity: {test_sem:.4f}\")\n",
    "\n",
    "# Random test examples\n",
    "print(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(20, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    \n",
    "    prompt = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "    g_feats = model.global_encoder(fi)\n",
    "    g = model.global_proj(g_feats).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf_feats = model.patch_encoder(p)\n",
    "    pf_pooled = model._pool(pf_feats)\n",
    "    pf = model.patch_proj(pf_pooled).view(B, N, 768)\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        early_stopping=True,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=2,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7808ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]         /home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:35,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:35,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:35,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:34,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:03<00:33,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:33,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:32,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:31,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:31,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:06<00:30,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:29,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:29,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:28,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:27,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:09<00:26,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:26,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:10<00:25,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:10<00:25,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:24,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:12<00:24,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:23,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:13<00:23,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:14<00:23,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:23,  1.56it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:15<00:22,  1.58it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:21,  1.60it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:16<00:20,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:17<00:19,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:17<00:19,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:18<00:18,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:17,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:19<00:16,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:20<00:16,  1.69it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:20<00:15,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:21<00:14,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:14,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:22<00:13,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:23<00:13,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:23<00:12,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:24<00:12,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:24<00:11,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:25<00:10,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:26<00:10,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:26<00:09,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:27<00:09,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:27<00:08,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:28<00:07,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:29<00:07,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:29<00:06,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:30<00:06,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:31<00:05,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:31<00:04,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:32<00:04,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:32<00:03,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:33<00:03,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:34<00:02,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:34<00:01,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:35<00:01,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:35<00:00,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "                                                           \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f6e6d4377b4d059625e7cdfe8b7818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73822329883d45e6b5695265de9ca377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss          : 1.3899\n",
      "  Validation Loss     : 1.0378\n",
      "  Semantic Similarity : 0.4170\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:37,  1.59it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   3%|▎         | 2/60 [00:01<00:34,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   5%|▌         | 3/60 [00:01<00:34,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   7%|▋         | 4/60 [00:02<00:33,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:   8%|▊         | 5/60 [00:03<00:33,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  10%|█         | 6/60 [00:03<00:32,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  12%|█▏        | 7/60 [00:04<00:31,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  13%|█▎        | 8/60 [00:04<00:31,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  15%|█▌        | 9/60 [00:05<00:31,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  17%|█▋        | 10/60 [00:06<00:30,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  18%|█▊        | 11/60 [00:06<00:30,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  20%|██        | 12/60 [00:07<00:29,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  22%|██▏       | 13/60 [00:07<00:29,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  23%|██▎       | 14/60 [00:08<00:28,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  25%|██▌       | 15/60 [00:09<00:27,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:27,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  28%|██▊       | 17/60 [00:10<00:26,  1.62it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  30%|███       | 18/60 [00:11<00:25,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  32%|███▏      | 19/60 [00:11<00:25,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:12<00:24,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  35%|███▌      | 21/60 [00:12<00:23,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  37%|███▋      | 22/60 [00:13<00:23,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  38%|███▊      | 23/60 [00:14<00:22,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  40%|████      | 24/60 [00:14<00:22,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  42%|████▏     | 25/60 [00:15<00:21,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  43%|████▎     | 26/60 [00:15<00:20,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  45%|████▌     | 27/60 [00:16<00:20,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  47%|████▋     | 28/60 [00:17<00:19,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  48%|████▊     | 29/60 [00:17<00:18,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  50%|█████     | 30/60 [00:18<00:18,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  52%|█████▏    | 31/60 [00:18<00:17,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  53%|█████▎    | 32/60 [00:19<00:17,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:20<00:16,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  57%|█████▋    | 34/60 [00:20<00:15,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  58%|█████▊    | 35/60 [00:21<00:15,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  60%|██████    | 36/60 [00:21<00:14,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  62%|██████▏   | 37/60 [00:22<00:13,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  63%|██████▎   | 38/60 [00:23<00:13,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  65%|██████▌   | 39/60 [00:23<00:12,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:24<00:12,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  68%|██████▊   | 41/60 [00:25<00:11,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  70%|███████   | 42/60 [00:25<00:11,  1.63it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:26<00:10,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  73%|███████▎  | 44/60 [00:26<00:09,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  75%|███████▌  | 45/60 [00:27<00:09,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  77%|███████▋  | 46/60 [00:28<00:08,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  78%|███████▊  | 47/60 [00:28<00:07,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  80%|████████  | 48/60 [00:29<00:07,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  82%|████████▏ | 49/60 [00:29<00:06,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  83%|████████▎ | 50/60 [00:30<00:05,  1.68it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  85%|████████▌ | 51/60 [00:30<00:05,  1.67it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  87%|████████▋ | 52/60 [00:31<00:04,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  88%|████████▊ | 53/60 [00:32<00:04,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  90%|█████████ | 54/60 [00:32<00:03,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:33<00:03,  1.66it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:34<00:02,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:34<00:01,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:35<00:01,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Evaluating:  98%|█████████▊| 59/60 [00:35<00:00,  1.65it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "                                                           \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814b48da5ef745649f24e56824023cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e92315fe5664b86ba01de1074b78885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULTS ==========\n",
      "Test Loss               : 0.9449\n",
      "Test Semantic Similarity: 0.4267\n",
      "\n",
      "--- Example 59 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "Lt. calcaneous, fracture._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "Lt. calcaneous, fracture._x000D_\n",
      "_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Lt. calcaneous, fracture.\n",
      "Generated Report : \n",
      " no bony lesion. both feet, OA with subluxation of Rt 1st MTP joint -> gouty arthritis --> RA involvement in left calcaneal spur type II and foot soft tissue swelling around rheumatoid bone base area on radiographs since last study -- possible osteopenia degenerative change tophi suspicious erosion at Lt 5th MT head/foot tip > PIP site adjacent tendonitis receding plantar aspect fracture periarticular vascular bulging\n",
      "\n",
      "--- Example 106 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "no bony lesion._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no bony lesion. no bony lesion.\n",
      "Generated Report : \n",
      " no significant bony lesion on radiographs. --> R/O gout arthritis, both knee joints with degenerative change since last study (tibia fracture) No signficant interval variance in radiolucent soft tissue swelling at Lt 1st MTP joint vs talus bone type II involvement or suspicious erosion of calcaneal spur rather than erosions ------------------------------------------------------------------------ RA findings and probable sclerotic changes periarticular osteopenia tophi tendonitis - old bones\n",
      "\n",
      "--- Example 159 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both foot, gout._x000D_\n",
      "Lt. 2-3rd intermetatarsal osteophytes._x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both foot, gout._x000D_\n",
      "Lt. 2-3rd intermetatarsal osteophytes._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both foot, gout. Lt. 2-3rd intermetatarsal osteophytes.\n",
      "Generated Report : \n",
      " degenerative change. no bony lesion on radiographs and gouty arthritis, both 1st MTP joint -> R/O RA involvement of Lt 5th IP joints --> suspicious erosion in lateral portion with periarticular bone type III erosions at base ossicle head diffuse osteopenia tendon inflammatory naviculare subchondral cysts vs ankle soft tissue swelling around accessory malleolar area tuberosity mild fracture progression to state level calcaneal spur LT\n",
      "\n",
      "--- Example 21 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "suspicious marginal erosion, both MCP and MTP joints_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "suspicious marginal erosion, both MCP and MTP joints_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "suspicious marginal erosion, both MCP and MTP joints\n",
      "Generated Report : \n",
      " No bony abnormality. Rt 1st MTP joint soft tissue swelling, both --> OA with erosions at Lt 5th metatarsal head and talus tuberosity in left lateral aspect of right calcaneum area around 2nd-4rd MT base tip -> RA involvement ------------------------------------------------------------------------ mild degenerative change on radiographs or gouty arthritis progression - no significant interval difference since last study --- possible old fracture type III/D2 vs 3x5+1)\n",
      "\n",
      "--- Example 61 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "Enthesophytes or calcifications in both calcaneous at Achilles tendon insertion site._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "Enthesophytes or calcifications in both calcaneous at Achilles tendon insertion site.\n",
      "Generated Report : \n",
      " No bony abnormality. Small periarticular bone, both 1st MTP joints with soft tissue swelling in right 2nd MT head and posterior malleolar area of Rt foot base Lt talus tendonitis --> r/o OA RA involvement on radiographs or otherwise diffuse osteopenia degenerative change suspicious erosion at left 5th medial subchondral cyramoid joint -> gout arthritis ------------------------------------------------------------------------ loose bodies type III vs IIx5a-4fv\n",
      "\n",
      "--- Example 182 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "both 1st MTP joint, gout._x000D_\n",
      "_x000D_\n",
      "Rt. large os trigonum_x000D_\n",
      "both ankle OA_x000D_\n",
      "_x000D_\n",
      "Rt. knee joint effusion_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "both 1st MTP joint, gout._x000D_\n",
      "_x000D_\n",
      "Rt. large os trigonum_x000D_\n",
      "both ankle OA_x000D_\n",
      "_x000D_\n",
      "Rt. knee joint effusion_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both 1st MTP joint, gout. Rt. large os trigonum both ankle OA Rt. knee joint effusion\n",
      "Generated Report : \n",
      " degenerative change. both ankle soft tissue swelling, type II with possible Rt 2nd MTP joint erosions at Lt 5th metatarsal head -> RA involvement --> otherwise no significant bony abnormality on radiographs or MRI findings in r/o gout arthritis progression hallux valgus deformity subchondral cyst and osteopenia severe OA diffuse inflammatory bowel syndrome mild to moderate navicular bone erosion periartinal tuberosity lateral malleolar area\n",
      "\n",
      "--- Example 15 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "lumbar spondylosis._x000D_\n",
      "diffuse osteopenia._x000D_\n",
      "L2/3 disc space narrowing._x000D_\n",
      "_x000D_\n",
      "Lt. 3rd MCP joint bony erosion_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "lumbar spondylosis._x000D_\n",
      "diffuse osteopenia._x000D_\n",
      "L2/3 disc space narrowing._x000D_\n",
      "_x000D_\n",
      "Lt. 3rd MCP joint bony erosion_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "lumbar spondylosis. diffuse osteopenia. L2/3 disc space narrowing. Lt. 3rd MCP joint bony erosion\n",
      "Generated Report : \n",
      " degenerative change. both knee joints, no significant bony lesion on radiographs since last study and possible involvement of left ankle spondylosis in Rt 1st MTP joint -> gout arthritis diffuse osteopenia mild OA with soft tissue swelling around base fracture bone erosions at Lt 4th IP head suspicious erosion --> RA/HgR interfibulare type II subchoroidal coalition periarticular spur lateral malleolar area rheumat\n",
      "\n",
      "--- Example 162 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "no significant bony lesion on radiographs._x000D_\n",
      "_x000D_\n",
      "[ Conclusion ]_x000D_\n",
      "no definite bony lesion_x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "no significant bony lesion on radiographs. no definite bony lesion\n",
      "Generated Report : \n",
      " no bony lesion. mild degenerative change, both knee joints and rt 1st MTP joint erosions in medial aspect of right foot with R/O gout arthritis --> RA involvement at Lt 5th MT head -> OA-related fracture on radiographs suggested otherwise possible ossicle deformity or os trigonum subluxation type 2 LT calcaneal tuberosity diffuse osteopenia inflammatory correlation - old bone erosion vs hallux valgus suspicious soft tissue\n",
      "\n",
      "--- Example 112 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "No bony abnormality._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "No bony abnormality. No bony abnormality.\n",
      "Generated Report : \n",
      " No bony abnormality. - no significant interval change since last study, both knee joints with soft tissue swelling in medial aspect of Rt 1st MTP joint and foot bones (right talus tendon). --> RA involvement otherwise suspicious erosion at right base-left lateral tip bone head space narrowing hallux valgare type III -> gout arthritis degenerative changes Lt subchondral cysts vs r/o osteopenia diffuse OA mild to moderate malleolar area on radi\n",
      "\n",
      "--- Example 228 ---\n",
      "Raw Report       : \n",
      "[ Finding ]_x000D_\n",
      "_x000D_\n",
      "[ Diagnosis ]_x000D_\n",
      "both 1st MTP joint, bony erosion._x000D_\n",
      "  --> gout, suggested._x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      "degenerative change of both feet._x000D_\n",
      "_x000D_\n",
      "both elbow OA esp> Rt. side._x000D_\n",
      "[ Recommend ]_x000D_\n",
      "\n",
      "Cleaned Report   : \n",
      "both 1st MTP joint, bony erosion. --> gout, suggested. degenerative change of both feet. both elbow OA esp> Rt. side.\n",
      "Generated Report : \n",
      " No bony abnormality. - no significant interval change since last study with old fracture, both hands and feet suspicious erosion on radiographs Lt Rt 1st MT head -> RA involvement of left calcaneal tuberosity at L1 to 5th MTP joints erosions in lateral aspect os trigonum bone type III-4/5rd metatarsus joint subfibulare degenerative changes diffuse osteopenia TMT foot soft tissue swelling --> gout arthritis recurrence\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import random\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging configuration: write INFO+ logs only to training.log (no console output)\n",
    "# =============================================================================\n",
    "# 1) Remove any existing handlers (e.g. from prior imports or notebook runs)\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "\n",
    "# 2) Configure file-only logging\n",
    "logging.basicConfig(\n",
    "    filename='training.log',      # will be created in your current working directory\n",
    "    filemode='w',                 # overwrite on each run\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Utility functions\n",
    "# =============================================================================\n",
    "\n",
    "def count_labels(data, target_classes, cfg):\n",
    "    class_counts = defaultdict(int)\n",
    "    data_by_class = defaultdict(list)\n",
    "    for entry in tqdm(data.values(), desc=\"Counting dataset\"):\n",
    "        lbl = entry.get('class_label', '').lower()\n",
    "        if lbl in target_classes and os.path.exists(entry['file_path']):\n",
    "            class_counts[lbl] += 1\n",
    "            data_by_class[lbl].append(entry)\n",
    "    return class_counts, data_by_class\n",
    "\n",
    "def prepare_abnormal_normal_data(data, cfg):\n",
    "    random.seed(42)\n",
    "    abnormal = ['ra', 'oa', 'gout']\n",
    "    normal = ['normal']\n",
    "    class_counts, data_by_class = count_labels(data, abnormal + normal, cfg)\n",
    "    combined = {\n",
    "        'abnormal': sum((data_by_class[c] for c in abnormal), []),\n",
    "        'normal': data_by_class['normal']\n",
    "    }\n",
    "    combined_counts = {\n",
    "        'abnormal': sum(class_counts[c] for c in abnormal),\n",
    "        'normal': class_counts['normal']\n",
    "    }\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(combined.values(), []), combined_counts, combined_counts\n",
    "    min_count = min(combined_counts.values())\n",
    "    balanced = []\n",
    "    final_counts = {}\n",
    "    for lbl, items in combined.items():\n",
    "        sampled = random.sample(items, min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, combined_counts, final_counts\n",
    "\n",
    "def prepare_data(data, target_classes, cfg, is_binary=False):\n",
    "    random.seed(42)\n",
    "    if len(target_classes) == 2 and 'abnormal' in target_classes and 'normal' in target_classes:\n",
    "        logging.info(\"Using abnormal-vs-normal logic\")\n",
    "        return prepare_abnormal_normal_data(data, cfg)\n",
    "    class_counts, data_by_class = count_labels(data, target_classes, cfg)\n",
    "    logging.info(f\"Original class distribution: {class_counts}\")\n",
    "    if not cfg.DATASET.BALANCE and not cfg.DATASET.AUGMENT:\n",
    "        return sum(data_by_class.values(), []), class_counts, class_counts\n",
    "    min_count = min(class_counts.values())\n",
    "    balanced, final_counts = [], {}\n",
    "    for lbl in target_classes:\n",
    "        sampled = random.sample(data_by_class[lbl], min_count)\n",
    "        balanced.extend(sampled)\n",
    "        final_counts[lbl] = min_count\n",
    "    logging.info(f\"Balanced class distribution: {final_counts}\")\n",
    "    if cfg.DATASET.AUGMENT:\n",
    "        balanced *= 2\n",
    "    return balanced, class_counts, final_counts\n",
    "\n",
    "# =============================================================================\n",
    "# Transforms\n",
    "# =============================================================================\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "patch_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "\n",
    "class FinalSamplesDataset(Dataset):\n",
    "    def __init__(self, cfg, image_transform=train_transform, patch_transform=patch_transform):\n",
    "        self.cfg = cfg\n",
    "        self.image_transform = image_transform\n",
    "        self.patch_transform = patch_transform\n",
    "\n",
    "        self.target_classes = cfg.DATASET.TARGET_CLASSES\n",
    "        if isinstance(self.target_classes, str):\n",
    "            self.target_classes = self.target_classes.split(\",\")\n",
    "\n",
    "        self.is_binary = len(self.target_classes) == 2\n",
    "        self.abnormal_classify = self.is_binary and 'abnormal' in self.target_classes\n",
    "        self.abnormal_mapping = (\n",
    "            {'ra': 'abnormal', 'oa': 'abnormal', 'gout': 'abnormal', 'normal': 'normal'}\n",
    "            if self.abnormal_classify else None\n",
    "        )\n",
    "\n",
    "        with open(cfg.DATASET.JSON, 'r') as f:\n",
    "            raw_list = json.load(f)\n",
    "\n",
    "        filtered = []\n",
    "        for item in raw_list:\n",
    "            merged = item.get('merged_image_path', '')\n",
    "            fp = item.get('file_paths', [])\n",
    "            if isinstance(fp, str):\n",
    "                fp = [fp]\n",
    "            paths = [merged] + fp\n",
    "            if any(os.path.exists(p) for p in paths):\n",
    "                filtered.append((merged, fp, item))\n",
    "\n",
    "        self.data = {}\n",
    "        for i, (merged, fp, item) in enumerate(filtered):\n",
    "            cls = item.get('class', 'unknown').lower()\n",
    "            if self.abnormal_mapping:\n",
    "                cls = self.abnormal_mapping.get(cls, cls)\n",
    "            self.data[i] = {\n",
    "                'file_path': merged,\n",
    "                'left_right_file_path': fp,\n",
    "                'class_label': cls,\n",
    "                'diagnosis': item.get('diagnosis', ''),\n",
    "                'keypoints': item.get('keypoints', {})\n",
    "            }\n",
    "\n",
    "        if self.is_binary:\n",
    "            balanced, _, _ = prepare_data(self.data, self.target_classes, cfg, True)\n",
    "            self.data = {i: e for i, e in enumerate(balanced)}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.eos_token = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        e = self.data[idx]\n",
    "        img = Image.open(e['file_path']).convert('RGB')\n",
    "        img = self.image_transform(img)\n",
    "        logging.info(f\"[Dataset] full_img shape: {img.shape}\")\n",
    "\n",
    "        patches = self._gen_patches(e['left_right_file_path'], e['keypoints'])\n",
    "        pt = [self.patch_transform(Image.fromarray(p)) for p in patches]\n",
    "        patches_tensor = torch.stack(pt, 0) if pt else torch.zeros(34, 3, 112, 112)\n",
    "        logging.info(f\"[Dataset] patches_tensor shape: {patches_tensor.shape}\")\n",
    "\n",
    "        raw = e.get('diagnosis', '')\n",
    "        clean = self._clean_report(raw)\n",
    "\n",
    "        input_text = f\"{self.tokenizer.bos_token} {clean}\"\n",
    "        tok = self.tokenizer(input_text, truncation=True, max_length=511, return_tensors='pt')\n",
    "\n",
    "        if tok['input_ids'][0][-1] != self.tokenizer.eos_token_id:\n",
    "            input_ids = torch.cat([tok['input_ids'], torch.tensor([[self.tokenizer.eos_token_id]])], dim=1)\n",
    "            attention_mask = torch.cat([tok['attention_mask'], torch.tensor([[1]])], dim=1)\n",
    "        else:\n",
    "            input_ids = tok['input_ids']\n",
    "            attention_mask = tok['attention_mask']\n",
    "\n",
    "        input_ids = input_ids.squeeze(0)\n",
    "        attention_mask = attention_mask.squeeze(0)\n",
    "        logging.info(f\"[Dataset] input_ids shape: {input_ids.shape}, attention_mask shape: {attention_mask.shape}\")\n",
    "\n",
    "        return {\n",
    "            'full_img': img,\n",
    "            'patches': patches_tensor,\n",
    "            'raw_report': raw,\n",
    "            'cleaned_report': clean,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "    def _gen_patches(self, paths, kps_dict, crop_size=(200, 300), patch_size=(112, 112)):\n",
    "        def extract(arr, side_kps):\n",
    "            lst = []\n",
    "            pts = side_kps[0]['keypoints']\n",
    "            for i in range(17):\n",
    "                x, y, s = int(pts[3*i]), int(pts[3*i+1]), pts[3*i+2]\n",
    "                if s > 0:\n",
    "                    x0 = max(x - crop_size[0]//2, 0)\n",
    "                    y0 = max(y - crop_size[1]//2, 0)\n",
    "                    x1 = min(x + crop_size[0]//2, arr.shape[1])\n",
    "                    y1 = min(y + crop_size[1]//2, arr.shape[0])\n",
    "                    c = arr[y0:y1, x0:x1]\n",
    "                    if c.size:\n",
    "                        lst.append(cv2.resize(c, patch_size))\n",
    "            return lst\n",
    "\n",
    "        def pad17(lst):\n",
    "            black = np.zeros((patch_size[1], patch_size[0], 3), np.uint8)\n",
    "            while len(lst) < 17:\n",
    "                lst.append(black)\n",
    "            return lst[:17]\n",
    "\n",
    "        left, right = [], []\n",
    "        if len(paths) == 1:\n",
    "            pth = paths[0]\n",
    "            if not pth or not os.path.exists(pth):\n",
    "                return pad17([]) + pad17([])\n",
    "            img_arr = cv2.imread(pth)\n",
    "            if img_arr is None:\n",
    "                return pad17([]) + pad17([])\n",
    "            arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "            if kps_dict.get('left'): left = extract(arr, kps_dict['left'])\n",
    "            if kps_dict.get('right'): right = extract(arr, kps_dict['right'])\n",
    "        else:\n",
    "            for side, pth in zip(['left', 'right'], paths):\n",
    "                if pth and os.path.exists(pth):\n",
    "                    img_arr = cv2.imread(pth)\n",
    "                    if img_arr is not None:\n",
    "                        arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "                        if kps_dict.get(side):\n",
    "                            lst = extract(arr, kps_dict[side])\n",
    "                            if side == 'left': left = lst\n",
    "                            else: right = lst\n",
    "\n",
    "        if left and not right:\n",
    "            right = [cv2.flip(p, 1) for p in left]\n",
    "        if right and not left:\n",
    "            left = [cv2.flip(p, 1) for p in right]\n",
    "        if not left and not right:\n",
    "            return pad17([]) + pad17([])\n",
    "\n",
    "        return pad17(left) + pad17(right)\n",
    "\n",
    "    def _clean_report(self, text):\n",
    "        text = unicodedata.normalize('NFKC', text or '')\n",
    "        text = re.sub(r'(?m)^-+\\s*$', '', text)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        text = re.sub(r'\\[\\s*finding\\s*\\]', '[FINDING]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*conclusion\\s*\\]', '[CONCLUSION]', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\s*diagnosis\\s*\\]', '[DIAGNOSIS]', text, flags=re.IGNORECASE)\n",
    "        parts = re.split(r'\\[\\s*recommend(?:ation)?\\s*\\]', text, flags=re.IGNORECASE)\n",
    "        text = parts[0]\n",
    "        fm = re.search(r'\\[FINDING\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        cm = re.search(r'\\[CONCLUSION\\](.*?)(?=\\[|$)', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if fm and cm and fm.group(1).strip().lower() == cm.group(1).strip().lower():\n",
    "            text = re.sub(r'\\[CONCLUSION\\].*?(?=\\[|$)', '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'\\[\\s*(FINDING|CONCLUSION|DIAGNOSIS)\\s*\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = text.replace('_x000D_', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "# =============================================================================\n",
    "# Collate function\n",
    "# =============================================================================\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([b['full_img'] for b in batch])\n",
    "    logging.info(f\"[Collate] stacked full_imgs shape: {imgs.shape}\")\n",
    "\n",
    "    pts = [b['patches'] for b in batch]\n",
    "    max_p = max(p.shape[0] for p in pts)\n",
    "    pads = []\n",
    "    for p in pts:\n",
    "        if p.shape[0] < max_p:\n",
    "            pad = torch.zeros((max_p - p.shape[0], *p.shape[1:]))\n",
    "            p = torch.cat([p, pad], dim=0)\n",
    "        pads.append(p)\n",
    "    patches = torch.stack(pads, 0)\n",
    "    logging.info(f\"[Collate] stacked patches shape: {patches.shape}\")\n",
    "\n",
    "    ids = [b['input_ids'] for b in batch]\n",
    "    masks = [b['attention_mask'] for b in batch]\n",
    "    ids = nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    masks = nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "    logging.info(f\"[Collate] padded input_ids shape: {ids.shape}, padded attention_mask shape: {masks.shape}\")\n",
    "\n",
    "    return {\n",
    "        'full_imgs': imgs,\n",
    "        'patches': patches,\n",
    "        'input_ids': ids,\n",
    "        'attention_mask': masks,\n",
    "        'raw_reports': [b['raw_report'] for b in batch],\n",
    "        'cleaned_reports': [b['cleaned_report'] for b in batch]\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, gpt2_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        self.global_encoder = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n",
    "        self.global_encoder.reset_classifier(0)\n",
    "        self.global_proj = nn.Linear(self.global_encoder.num_features, 768)\n",
    "\n",
    "        self.patch_encoder = timm.create_model('resnet50', pretrained=True)\n",
    "        self.patch_encoder.fc = nn.Identity()\n",
    "        self.patch_proj = nn.Linear(self.patch_encoder.num_features, 768)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=768, num_heads=8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(768)\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel.from_pretrained(gpt2_model_name, add_cross_attention=True)\n",
    "\n",
    "    def _pool(self, feats):\n",
    "        return feats.mean(dim=[2, 3]) if feats.ndim > 2 else feats\n",
    "\n",
    "    def forward(self, imgs, patches, input_ids, attention_mask, decoder_labels=None):\n",
    "        # Global image\n",
    "        g_feats = self.global_encoder(imgs)\n",
    "        logging.info(f\"[Model] global_encoder output shape: {g_feats.shape}\")\n",
    "        g = self.global_proj(g_feats).unsqueeze(1)\n",
    "        logging.info(f\"[Model] global_proj + unsqueeze shape: {g.shape}\")\n",
    "\n",
    "        # Patch images\n",
    "        B, N, C, H, W = patches.shape\n",
    "        logging.info(f\"[Model] patches input shape: {patches.shape}\")\n",
    "        p = patches.view(B * N, C, H, W)\n",
    "        pf_feats = (self.patch_encoder.forward_features(p)\n",
    "                    if hasattr(self.patch_encoder, 'forward_features')\n",
    "                    else self.patch_encoder(p))\n",
    "        logging.info(f\"[Model] patch_encoder output shape: {pf_feats.shape}\")\n",
    "        pf_pooled = self._pool(pf_feats)\n",
    "        pf = self.patch_proj(pf_pooled).view(B, N, 768)\n",
    "        logging.info(f\"[Model] patch_proj + reshape shape: {pf.shape}\")\n",
    "\n",
    "        # Combine\n",
    "        cat = torch.cat([g, pf], dim=1)\n",
    "        logging.info(f\"[Model] concatenated features shape: {cat.shape}\")\n",
    "        comb, _ = self.attn(cat, cat, cat)\n",
    "        comb = self.norm(comb)\n",
    "        logging.info(f\"[Model] after attention & norm shape: {comb.shape}\")\n",
    "\n",
    "        # Decoder\n",
    "        out = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=comb,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "        logging.info(f\"[Model] decoder logits shape: {out.logits.shape}\")\n",
    "        return out\n",
    "\n",
    "# =============================================================================\n",
    "# Train / Eval helpers\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for b in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        logging.info(f\"[Train] batch full_imgs {imgs.shape}, patches {pts.shape}, input_ids {ids.shape}, mask {msk.shape}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            loss = out.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_gen, all_gt = [], []\n",
    "    for b in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs = b['full_imgs'].to(device)\n",
    "        pts = b['patches'].to(device)\n",
    "        ids = b['input_ids'].to(device)\n",
    "        msk = b['attention_mask'].to(device)\n",
    "\n",
    "        logging.info(f\"[Eval] batch full_imgs {imgs.shape}, patches {pts.shape}, input_ids {ids.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs, pts, ids, msk, decoder_labels=ids)\n",
    "            total_loss += out.loss.item()\n",
    "\n",
    "            bos_tokens = torch.tensor([[tokenizer.bos_token_id]] * imgs.size(0), device=device)\n",
    "            logging.info(f\"[Eval] bos_tokens shape: {bos_tokens.shape}\")\n",
    "\n",
    "            # Feature reuse\n",
    "            g_feats = model.global_encoder(imgs)\n",
    "            g = model.global_proj(g_feats).unsqueeze(1)\n",
    "            B, N, C, H, W = pts.shape\n",
    "            p = pts.view(B * N, C, H, W)\n",
    "            pf_feats = model.patch_encoder(p)\n",
    "            pf_pooled = model._pool(pf_feats)\n",
    "            pf = model.patch_proj(pf_pooled).view(B, N, 768)\n",
    "            cat = torch.cat([g, pf], dim=1)\n",
    "            comb, _ = model.attn(cat, cat, cat)\n",
    "            comb = model.norm(comb)\n",
    "\n",
    "            gen_ids = model.decoder.generate(\n",
    "                input_ids=bos_tokens,\n",
    "                encoder_hidden_states=comb,\n",
    "                early_stopping=True,\n",
    "                attention_mask=torch.ones_like(bos_tokens),\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=40,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.3,\n",
    "                no_repeat_ngram_size=2,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            logging.info(f\"[Eval] gen_ids shape: {gen_ids.shape}\")\n",
    "\n",
    "            gen_txt = [tokenizer.decode(g_, skip_special_tokens=True) for g_ in gen_ids]\n",
    "            gt_txt = [tokenizer.decode(i_, skip_special_tokens=True) for i_ in ids]\n",
    "            all_gen.extend(gen_txt)\n",
    "            all_gt.extend(gt_txt)\n",
    "\n",
    "    return total_loss / len(loader), all_gen, all_gt\n",
    "\n",
    "def compute_semantic_similarity(gen, gt):\n",
    "    stm = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    e1 = stm.encode(gen, convert_to_tensor=True)\n",
    "    e2 = stm.encode(gt, convert_to_tensor=True)\n",
    "    return nn.functional.cosine_similarity(e1, e2).mean().item()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, sems):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, sems, label=\"Semantic Similarity\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.DATASET = Cfg()\n",
    "cfg.DATASET.JSON = 'final_samples_both_only_v2.json'\n",
    "cfg.DATASET.USE_RAW = True\n",
    "cfg.DATASET.USE_PATCH = True\n",
    "cfg.DATASET.REPORT = True\n",
    "cfg.DATASET.TARGET_CLASSES = ['ra', 'oa', 'gout', 'normal', 'uncertain', 'ref.prev']\n",
    "cfg.DATASET.BALANCE = False\n",
    "cfg.DATASET.AUGMENT = False\n",
    "\n",
    "# Initialize and configure tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = FinalSamplesDataset(cfg)\n",
    "dataset.tokenizer = tokenizer\n",
    "dataset.eos_token = tokenizer.eos_token\n",
    "\n",
    "dist = Counter(e['class_label'] for e in dataset.data.values())\n",
    "logging.info(\"Dataset class distribution:\")\n",
    "for cls, cnt in dist.items():\n",
    "    logging.info(f\"  {cls}: {cnt}\")\n",
    "\n",
    "# Split\n",
    "n = len(dataset)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model, optimizer, scheduler, scaler\n",
    "model = MultiModalModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# Training\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, sems = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logging.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss, gen_txt, gt_txt = evaluate(model, val_loader, device)\n",
    "    sem = compute_semantic_similarity(gen_txt, gt_txt)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    sems.append(sem)\n",
    "\n",
    "    logging.info(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    logging.info(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    logging.info(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "\n",
    "    print(f\"  Train Loss          : {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss     : {val_loss:.4f}\")\n",
    "    print(f\"  Semantic Similarity : {sem:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(train_losses, val_losses, sems)\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_gen, test_gt = evaluate(model, test_loader, device)\n",
    "test_sem = compute_semantic_similarity(test_gen, test_gt)\n",
    "logging.info(\"\\n========== TEST RESULTS ==========\")\n",
    "logging.info(f\"Test Loss               : {test_loss:.4f}\")\n",
    "logging.info(f\"Test Semantic Similarity: {test_sem:.4f}\")\n",
    "\n",
    "print(\"\\n========== TEST RESULTS ==========\")\n",
    "print(f\"Test Loss               : {test_loss:.4f}\")\n",
    "print(f\"Test Semantic Similarity: {test_sem:.4f}\")\n",
    "\n",
    "# Random test examples\n",
    "logging.info(\"\\n===== RANDOM TEST EXAMPLES =====\")\n",
    "for idx in random.sample(range(len(test_ds)), min(10, len(test_ds))):\n",
    "    ex = test_ds[idx]\n",
    "    raw = ex['raw_report']\n",
    "    clean = ex['cleaned_report']\n",
    "    fi = ex['full_img'].unsqueeze(0).to(device)\n",
    "    pa = ex['patches'].unsqueeze(0).to(device)\n",
    "    \n",
    "    prompt = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "    g_feats = model.global_encoder(fi)\n",
    "    g = model.global_proj(g_feats).unsqueeze(1)\n",
    "    B, N, C, H, W = pa.shape\n",
    "    p = pa.view(B * N, C, H, W)\n",
    "    pf_feats = model.patch_encoder(p)\n",
    "    pf_pooled = model._pool(pf_feats)\n",
    "    pf = model.patch_proj(pf_pooled).view(B, N, 768)\n",
    "    cat = torch.cat([g, pf], dim=1)\n",
    "    comb, _ = model.attn(cat, cat, cat)\n",
    "    comb = model.norm(comb)\n",
    "\n",
    "    gen_ids = model.decoder.generate(\n",
    "        input_ids=prompt,\n",
    "        encoder_hidden_states=comb,\n",
    "        early_stopping=True,\n",
    "        attention_mask=torch.ones_like(prompt),\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=2,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    gen = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    logging.info(f\"\\n--- Example {idx} ---\")\n",
    "    logging.info(f\"Raw Report       : \\n{raw}\")\n",
    "    logging.info(f\"Cleaned Report   : \\n{clean}\")\n",
    "    logging.info(f\"Generated Report : \\n{gen}\")\n",
    "\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Raw Report       : \\n{raw}\")\n",
    "    print(f\"Cleaned Report   : \\n{clean}\")\n",
    "    print(f\"Generated Report : \\n{gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de31a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
