{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 17:27:00.946291: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 17:27:00.951949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-13 17:27:00.958791: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-13 17:27:00.960861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-13 17:27:00.966050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/tmp/ipykernel_46753/2380946077.py:53: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/2380946077.py:59: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/2380946077.py:66: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1:   0%|          | 0/18 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1/1: 100%|██████████| 18/18 [01:31<00:00,  5.06s/it, loss=1.0588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 2.2289\n",
      "Epoch 1 Validation Loss: 1.2166\n",
      "Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Generated Reports ===\n",
      "\n",
      "[Generated Report 1]:\n",
      "FINDING \n",
      "a) Rheumatoid arthritis (RA). b. RA involvement, both hands and feet: no significant difference at all on radiographs or MRI scans of joint space narrowing 2 years ago - 5th MTPR examination 4/5 1st 3rd TMT\n",
      "--------------------------------------------------\n",
      "[Generated Report 2]:\n",
      "The T-bone joint in both hands.\n",
      "Tibial osteopenia, and no significant change since the last study (possible RA involvement).  \n",
      " CONCLUSION - In addition to Rt 3rd MTP joints with moderate reduction of 5th RD3r5mTR\n",
      "--------------------------------------------------\n",
      "[Generated Report 3]:\n",
      "FINDING \n",
      "Cortical flexion, both. - no significant changes at T1st MTP joint and Rt 1st MTJT joints > 2nd RA involvement but with PPPOLEON formation (no change) < 5th DLR-TRG correlation\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import your model components and tokenizer ---\n",
    "from timm import create_model\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Use Albumentations for image augmentation (as in your reference code) ---\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "# --- Set up the tokenizer ---\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# GPT-2 does not have a pad token so we use EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load your data (pkl file) ---\n",
    "with open('updated_merge_json_200x300.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# --- Balance the dataset ---\n",
    "task_classes = ['ra', 'normal']\n",
    "class_counts = {cls: 0 for cls in task_classes}\n",
    "data_by_class = {cls: [] for cls in task_classes}\n",
    "\n",
    "for key, entry in data.items():\n",
    "    # use lower-case class label and only include if image file exists\n",
    "    class_label = entry.get('class', '').lower()\n",
    "    if class_label in class_counts and os.path.exists(entry['file_path']):\n",
    "        class_counts[class_label] += 1\n",
    "        data_by_class[class_label].append(entry)\n",
    "\n",
    "min_class_count = min(class_counts.values())\n",
    "balanced_data = []\n",
    "for cls in task_classes:\n",
    "    balanced_data.extend(random.sample(data_by_class[cls], min_class_count))\n",
    "\n",
    "# Optionally, you may multiply the balanced_data for augmentation purposes:\n",
    "augmented_data = balanced_data * 2\n",
    "\n",
    "# --- Define Albumentations transforms ---\n",
    "train_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# For patch images we use a similar transform (you can adjust if needed)\n",
    "patch_tfms = A.Compose([\n",
    "    A.Resize(112, 112),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# --- Define a new Dataset for report generation ---\n",
    "class ReportDataset(Dataset):\n",
    "    def __init__(self, data, img_tfms, patch_tfms, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.img_tfms = img_tfms\n",
    "        self.patch_tfms = patch_tfms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        \n",
    "        # Load the main image\n",
    "        image = Image.open(entry['file_path']).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        augmented = self.img_tfms(image=image)\n",
    "        image_tensor = augmented['image']\n",
    "        \n",
    "        # Process patch images (if any)\n",
    "        patches = entry.get('bbx', [])\n",
    "        if len(patches) > 0:\n",
    "            # Limit to up to 34 patches (as before)\n",
    "            patch_imgs = [Image.fromarray(patch) for patch in patches[:34]]\n",
    "            patch_tensors = []\n",
    "            for p in patch_imgs:\n",
    "                p = np.array(p)\n",
    "                aug_patch = self.patch_tfms(image=p)\n",
    "                patch_tensors.append(aug_patch['image'])\n",
    "            # Concatenate along the channel dimension: resulting shape (34*3, 112, 112)\n",
    "            combined_patches = torch.cat(patch_tensors, dim=0)\n",
    "        else:\n",
    "            combined_patches = torch.zeros(34*3, 112, 112)\n",
    "        \n",
    "        # Process the report text (diagnosis field)\n",
    "        # Replace unwanted tokens and append EOS token (using tokenizer.eos_token)\n",
    "        report_text = entry['diagnosis'].replace('_x000D_', ' ').strip()\n",
    "        caption = f\"{report_text}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize without padding here (we will pad in the collate function)\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Squeeze to remove batch dimension (now shape: [seq_len])\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Create shifted labels for LM training (shift left by one)\n",
    "        # For example: if input_ids = [a, b, c, eos] then labels = [b, c, eos, eos]\n",
    "        labels = input_ids.clone()\n",
    "        if input_ids.size(0) > 1:\n",
    "            labels[:-1] = input_ids[1:]\n",
    "        # (Optionally, you can set the last token’s label to -100 so that loss is not computed)\n",
    "        # labels[-1] = -100\n",
    "        \n",
    "        return image_tensor, combined_patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Define a collate function to pad variable-length sequences ---\n",
    "def collate_fn(batch):\n",
    "    images, patches, input_ids_list, attn_masks, labels_list = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    patches = torch.stack(patches, dim=0)\n",
    "    \n",
    "    # Use the tokenizer's pad method to pad the input_ids and labels\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {'input_ids': list(input_ids_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = batch_encoding['input_ids']\n",
    "    \n",
    "    batch_encoding_labels = tokenizer.pad(\n",
    "        {'input_ids': list(labels_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = batch_encoding_labels['input_ids']\n",
    "    \n",
    "    # Pad attention masks similarly\n",
    "    batch_attn = tokenizer.pad(\n",
    "        {'input_ids': list(attn_masks)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    attention_mask = batch_attn['input_ids']\n",
    "    \n",
    "    # Replace pad token positions in labels with -100 so that loss is not computed on them\n",
    "    labels[ input_ids == tokenizer.pad_token_id ] = -100\n",
    "    \n",
    "    return images, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Create Dataset and DataLoaders ---\n",
    "full_dataset = ReportDataset(augmented_data, train_tfms, patch_tfms, tokenizer, max_length=128)\n",
    "\n",
    "# You can split into train/val/test (here we use 80/10/10 split)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "batch_size = 16  # adjust as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- Use your model definition (TwoBranchWithGPT2) unchanged ---\n",
    "# (Below is your unchanged model code; make sure it is defined exactly as you need)\n",
    "class TwoBranchWithGPT2(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(TwoBranchWithGPT2, self).__init__()\n",
    "        # -- SWIN Models --\n",
    "        self.swin_global = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_global.head = nn.Identity()\n",
    "        self.swin_patch = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_patch.head = nn.Identity()\n",
    "\n",
    "        # -- ResNet Models --\n",
    "        from torchvision import models\n",
    "        resnet_global = models.resnet50(pretrained=pretrained)\n",
    "        resnet_patch  = models.resnet50(pretrained=pretrained)\n",
    "        resnet_global.fc = nn.Identity()\n",
    "        resnet_patch.fc  = nn.Identity()\n",
    "        self.resnet_global = resnet_global\n",
    "        self.resnet_patch  = resnet_patch\n",
    "\n",
    "        # -- Convert patch channels from 102 to 3 --\n",
    "        self.patch_channel_reduction = nn.Conv2d(in_channels=102, out_channels=3, kernel_size=1)\n",
    "\n",
    "        # -- Merge & project features to GPT2 hidden size (768) --\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(5632, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # -- Classification head (unused in caption training) --\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # -- GPT-2 with cross-attention --\n",
    "        gpt2_config = GPT2Config.from_pretrained(\"gpt2\", add_cross_attention=True)\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2_config)\n",
    "        self.gpt2.resize_token_embeddings(len(tokenizer))\n",
    "        # Initialize cross-attention layers\n",
    "        for name, param in self.gpt2.named_parameters():\n",
    "            if 'crossattention' in name:\n",
    "                param.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "        # -- Prefix Projector --\n",
    "        self.prefix_length = 10  # can experiment with this\n",
    "        self.prefix_projector = nn.Linear(768, 768 * self.prefix_length)\n",
    "\n",
    "    def encode_features(self, images, patches):\n",
    "        # Resize patches to image size and reduce channels\n",
    "        patches_resized = F.interpolate(patches, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        patches_reduced = self.patch_channel_reduction(patches_resized)\n",
    "\n",
    "        swin_global_features = self.swin_global.forward_features(images).mean(dim=[1, 2])\n",
    "        swin_patch_features = self.swin_patch.forward_features(patches_reduced).mean(dim=[1, 2])\n",
    "        resnet_global_features = self.resnet_global(images)\n",
    "        resnet_patch_features = self.resnet_patch(patches_reduced)\n",
    "\n",
    "        combined_features = torch.cat([swin_global_features, swin_patch_features,\n",
    "                                         resnet_global_features, resnet_patch_features], dim=1)\n",
    "        projected_features = self.feature_attention(combined_features)\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "    def forward(self, images, patches, input_ids=None, attention_mask=None):\n",
    "        projected_features = self.encode_features(images, patches)\n",
    "        # We still compute classification output (unused in caption LM training)\n",
    "        cls_output = self.classifier(projected_features)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = projected_features.size(0)\n",
    "            encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "            encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "            # Teacher forcing mode: pass LM input tokens along with cross-attention\n",
    "            gpt_outputs = self.gpt2(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states\n",
    "            )\n",
    "            return cls_output, gpt_outputs.logits\n",
    "        else:\n",
    "            return cls_output\n",
    "\n",
    "    def generate_reports(self, projected_features, tokenizer, max_length=50, \n",
    "                           do_sample=True, top_k=50, top_p=0.95, temperature=0.7):\n",
    "        device = projected_features.device\n",
    "        batch_size = projected_features.size(0)\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "        encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "\n",
    "        input_ids = torch.full((batch_size, 1), bos_id, device=device, dtype=torch.long)\n",
    "        encoder_attention_mask = torch.ones((batch_size, self.prefix_length), device=device, dtype=torch.long)\n",
    "\n",
    "        generated_ids = self.gpt2.generate(\n",
    "            input_ids=input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=do_sample,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.2,\n",
    "            bos_token_id=bos_id,\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(seq.tolist(), skip_special_tokens=True) for seq in generated_ids]\n",
    "        return generated_texts\n",
    "\n",
    "# --- Set up device, model, loss, optimizer, and scheduler ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoBranchWithGPT2(pretrained=True).to(device)\n",
    "\n",
    "# We use the text generation loss only (CrossEntropyLoss)\n",
    "criterion_txt = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 1  # adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, patches, input_ids, attention_mask, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # We call the model in teacher-forcing mode.\n",
    "        _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "        # Flatten logits and labels for loss computation.\n",
    "        loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, patches, input_ids, attention_mask, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            patches = patches.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "            loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# --- Testing / Inference: Generate Reports ---\n",
    "model.eval()\n",
    "all_generated_texts = []\n",
    "with torch.no_grad():\n",
    "    for images, patches, input_ids, attention_mask, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        projected_features = model.encode_features(images, patches)\n",
    "        gen_texts = model.generate_reports(projected_features, tokenizer, max_length=60)\n",
    "        all_generated_texts.extend(gen_texts)\n",
    "\n",
    "# Print a few sample generated reports\n",
    "print(\"\\n=== Sample Generated Reports ===\\n\")\n",
    "for i, report in enumerate(all_generated_texts[:3]):\n",
    "    print(f\"[Generated Report {i+1}]:\\n{report}\")\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46753/3463267922.py:53: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/3463267922.py:59: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/3463267922.py:66: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1:   0%|          | 0/18 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1/1: 100%|██████████| 18/18 [01:29<00:00,  5.00s/it, loss=1.4273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 2.2164\n",
      "Epoch 1 Validation Loss: 1.3563\n",
      "Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Generated Reports and Ground Truth Reports ===\n",
      "\n",
      "[Ground Truth Report 1]:\n",
      "[ Finding ] \n",
      "Lt. 3-5th MCP joint, extensor hood, synovitis. \n",
      "Lt. 2-5th PIP joints, synovial thickening. \n",
      "Rt. 3rd MCP, extensor hood, soft tissue swelling, suggestive of synovitis. \n",
      "   ==> RA involvement, suggested. \n",
      " \n",
      "Rt. 1st IP joint, synovitis and calcification. \n",
      "  --> CPPD vs. RA involvement. \n",
      " \n",
      "Rt. 5th/4th/3rd/2nd MTP joints,\n",
      "\n",
      "[Generated Report 1]:\n",
      "FINDING \n",
      "A rt. joint, both knees and elbows up to hip bone level at knee's side of head - RA involvement? 1st MTPM ? (OR/R)? 4th RTRT ???) 2nd MAO type 3rd MTKMT K\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 2]:\n",
      "[ Finding ] \n",
      "Hallux valgus. \n",
      "[ Diagnosis ] \n",
      " \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 2]:\n",
      "FINDING \n",
      "a bony abnormality. It is likely an underlying degenerative change in the frontal, posterior temporal lobes and occipital lobe/cortical joint space due to erosions on both sides of RA axis - possibly Rt 5th MTP junction?\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 3]:\n",
      "[FINDING       ] joint space narrowing, multiple TMT joints, both \n",
      "- RA involvement, more likely \n",
      "hallux valgus with degenerative change, both   [CONCLUSION    ] joint space narrowing, multiple TMT joints, both \n",
      "- RA involvement, more likely \n",
      "hallux valgus with degenerative change, both   [RECOMMENDATION] -\n",
      "\n",
      "[Generated Report 3]:\n",
      "\n",
      "Merry Christmas to all. I hope you are having a great holiday this year, and looking forward for the holidays ahead! - \n",
      "\n",
      "[1] http://www-strava.com/forums/?topic=2541&id=-522133521#post_\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import your model components and tokenizer ---\n",
    "from timm import create_model\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Use Albumentations for image augmentation (as in your reference code) ---\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "# --- Set up the tokenizer ---\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# GPT-2 does not have a pad token so we use EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load your data (pkl file) ---\n",
    "with open('updated_merge_json_200x300.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# --- Balance the dataset ---\n",
    "task_classes = ['ra', 'normal']\n",
    "class_counts = {cls: 0 for cls in task_classes}\n",
    "data_by_class = {cls: [] for cls in task_classes}\n",
    "\n",
    "for key, entry in data.items():\n",
    "    # use lower-case class label and only include if image file exists\n",
    "    class_label = entry.get('class', '').lower()\n",
    "    if class_label in class_counts and os.path.exists(entry['file_path']):\n",
    "        class_counts[class_label] += 1\n",
    "        data_by_class[class_label].append(entry)\n",
    "\n",
    "min_class_count = min(class_counts.values())\n",
    "balanced_data = []\n",
    "for cls in task_classes:\n",
    "    balanced_data.extend(random.sample(data_by_class[cls], min_class_count))\n",
    "\n",
    "# Optionally, you may multiply the balanced_data for augmentation purposes:\n",
    "augmented_data = balanced_data * 2\n",
    "\n",
    "# --- Define Albumentations transforms ---\n",
    "train_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# For patch images we use a similar transform (you can adjust if needed)\n",
    "patch_tfms = A.Compose([\n",
    "    A.Resize(112, 112),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# --- Define a new Dataset for report generation ---\n",
    "class ReportDataset(Dataset):\n",
    "    def __init__(self, data, img_tfms, patch_tfms, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.img_tfms = img_tfms\n",
    "        self.patch_tfms = patch_tfms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        \n",
    "        # Load the main image\n",
    "        image = Image.open(entry['file_path']).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        augmented = self.img_tfms(image=image)\n",
    "        image_tensor = augmented['image']\n",
    "        \n",
    "        # Process patch images (if any)\n",
    "        patches = entry.get('bbx', [])\n",
    "        if len(patches) > 0:\n",
    "            # Limit to up to 34 patches (as before)\n",
    "            patch_imgs = [Image.fromarray(patch) for patch in patches[:34]]\n",
    "            patch_tensors = []\n",
    "            for p in patch_imgs:\n",
    "                p = np.array(p)\n",
    "                aug_patch = self.patch_tfms(image=p)\n",
    "                patch_tensors.append(aug_patch['image'])\n",
    "            # Concatenate along the channel dimension: resulting shape (34*3, 112, 112)\n",
    "            combined_patches = torch.cat(patch_tensors, dim=0)\n",
    "        else:\n",
    "            combined_patches = torch.zeros(34*3, 112, 112)\n",
    "        \n",
    "        # Process the report text (diagnosis field)\n",
    "        # Replace unwanted tokens and append EOS token (using tokenizer.eos_token)\n",
    "        report_text = entry['diagnosis'].replace('_x000D_', ' ').strip()\n",
    "        caption = f\"{report_text}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize without padding here (we will pad in the collate function)\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Squeeze to remove batch dimension (now shape: [seq_len])\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Create shifted labels for LM training (shift left by one)\n",
    "        # For example: if input_ids = [a, b, c, eos] then labels = [b, c, eos, eos]\n",
    "        labels = input_ids.clone()\n",
    "        if input_ids.size(0) > 1:\n",
    "            labels[:-1] = input_ids[1:]\n",
    "        # (Optionally, you can set the last token’s label to -100 so that loss is not computed)\n",
    "        # labels[-1] = -100\n",
    "        \n",
    "        return image_tensor, combined_patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Define a collate function to pad variable-length sequences ---\n",
    "def collate_fn(batch):\n",
    "    images, patches, input_ids_list, attn_masks, labels_list = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    patches = torch.stack(patches, dim=0)\n",
    "    \n",
    "    # Use the tokenizer's pad method to pad the input_ids and labels\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {'input_ids': list(input_ids_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = batch_encoding['input_ids']\n",
    "    \n",
    "    batch_encoding_labels = tokenizer.pad(\n",
    "        {'input_ids': list(labels_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = batch_encoding_labels['input_ids']\n",
    "    \n",
    "    # Pad attention masks similarly\n",
    "    batch_attn = tokenizer.pad(\n",
    "        {'input_ids': list(attn_masks)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    attention_mask = batch_attn['input_ids']\n",
    "    \n",
    "    # Replace pad token positions in labels with -100 so that loss is not computed on them\n",
    "    labels[ input_ids == tokenizer.pad_token_id ] = -100\n",
    "    \n",
    "    return images, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Create Dataset and DataLoaders ---\n",
    "full_dataset = ReportDataset(augmented_data, train_tfms, patch_tfms, tokenizer, max_length=128)\n",
    "\n",
    "# You can split into train/val/test (here we use 80/10/10 split)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "batch_size = 16  # adjust as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- Use your model definition (TwoBranchWithGPT2) unchanged ---\n",
    "# (Below is your unchanged model code; make sure it is defined exactly as you need)\n",
    "class TwoBranchWithGPT2(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(TwoBranchWithGPT2, self).__init__()\n",
    "        # -- SWIN Models --\n",
    "        self.swin_global = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_global.head = nn.Identity()\n",
    "        self.swin_patch = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_patch.head = nn.Identity()\n",
    "\n",
    "        # -- ResNet Models --\n",
    "        from torchvision import models\n",
    "        resnet_global = models.resnet50(pretrained=pretrained)\n",
    "        resnet_patch  = models.resnet50(pretrained=pretrained)\n",
    "        resnet_global.fc = nn.Identity()\n",
    "        resnet_patch.fc  = nn.Identity()\n",
    "        self.resnet_global = resnet_global\n",
    "        self.resnet_patch  = resnet_patch\n",
    "\n",
    "        # -- Convert patch channels from 102 to 3 --\n",
    "        self.patch_channel_reduction = nn.Conv2d(in_channels=102, out_channels=3, kernel_size=1)\n",
    "\n",
    "        # -- Merge & project features to GPT2 hidden size (768) --\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(5632, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # -- Classification head (unused in caption training) --\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # -- GPT-2 with cross-attention --\n",
    "        gpt2_config = GPT2Config.from_pretrained(\"gpt2\", add_cross_attention=True)\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2_config)\n",
    "        self.gpt2.resize_token_embeddings(len(tokenizer))\n",
    "        # Initialize cross-attention layers\n",
    "        for name, param in self.gpt2.named_parameters():\n",
    "            if 'crossattention' in name:\n",
    "                param.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "        # -- Prefix Projector --\n",
    "        self.prefix_length = 10  # can experiment with this\n",
    "        self.prefix_projector = nn.Linear(768, 768 * self.prefix_length)\n",
    "\n",
    "    def encode_features(self, images, patches):\n",
    "        # Resize patches to image size and reduce channels\n",
    "        patches_resized = F.interpolate(patches, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        patches_reduced = self.patch_channel_reduction(patches_resized)\n",
    "\n",
    "        swin_global_features = self.swin_global.forward_features(images).mean(dim=[1, 2])\n",
    "        swin_patch_features = self.swin_patch.forward_features(patches_reduced).mean(dim=[1, 2])\n",
    "        resnet_global_features = self.resnet_global(images)\n",
    "        resnet_patch_features = self.resnet_patch(patches_reduced)\n",
    "\n",
    "        combined_features = torch.cat([swin_global_features, swin_patch_features,\n",
    "                                         resnet_global_features, resnet_patch_features], dim=1)\n",
    "        projected_features = self.feature_attention(combined_features)\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "    def forward(self, images, patches, input_ids=None, attention_mask=None):\n",
    "        projected_features = self.encode_features(images, patches)\n",
    "        # We still compute classification output (unused in caption LM training)\n",
    "        cls_output = self.classifier(projected_features)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = projected_features.size(0)\n",
    "            encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "            encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "            # Teacher forcing mode: pass LM input tokens along with cross-attention\n",
    "            gpt_outputs = self.gpt2(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states\n",
    "            )\n",
    "            return cls_output, gpt_outputs.logits\n",
    "        else:\n",
    "            return cls_output\n",
    "\n",
    "    def generate_reports(self, projected_features, tokenizer, max_length=50, \n",
    "                           do_sample=True, top_k=50, top_p=0.95, temperature=0.7):\n",
    "        device = projected_features.device\n",
    "        batch_size = projected_features.size(0)\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "        encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "\n",
    "        input_ids = torch.full((batch_size, 1), bos_id, device=device, dtype=torch.long)\n",
    "        encoder_attention_mask = torch.ones((batch_size, self.prefix_length), device=device, dtype=torch.long)\n",
    "\n",
    "        generated_ids = self.gpt2.generate(\n",
    "            input_ids=input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=do_sample,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.2,\n",
    "            bos_token_id=bos_id,\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(seq.tolist(), skip_special_tokens=True) for seq in generated_ids]\n",
    "        return generated_texts\n",
    "\n",
    "# --- Set up device, model, loss, optimizer, and scheduler ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoBranchWithGPT2(pretrained=True).to(device)\n",
    "\n",
    "# We use the text generation loss only (CrossEntropyLoss)\n",
    "criterion_txt = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 1  # adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, patches, input_ids, attention_mask, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # We call the model in teacher-forcing mode.\n",
    "        _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "        # Flatten logits and labels for loss computation.\n",
    "        loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, patches, input_ids, attention_mask, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            patches = patches.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "            loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# --- Testing / Inference: Generate Reports and Print Ground Truth ---\n",
    "model.eval()\n",
    "all_generated_texts = []\n",
    "all_ground_truth_texts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, patches, input_ids, attention_mask, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        # Get the encoded image features for caption generation.\n",
    "        projected_features = model.encode_features(images, patches)\n",
    "        # Generate reports for the current batch.\n",
    "        gen_texts = model.generate_reports(projected_features, tokenizer, max_length=60)\n",
    "        all_generated_texts.extend(gen_texts)\n",
    "        \n",
    "        # Decode ground truth reports from input_ids.\n",
    "        # Note: We decode the first token sequence in each sample (removing padding tokens).\n",
    "        for seq in input_ids:\n",
    "            gt_text = tokenizer.decode(seq.tolist(), skip_special_tokens=True)\n",
    "            all_ground_truth_texts.append(gt_text)\n",
    "\n",
    "# Print a few sample generated reports along with the ground truth\n",
    "print(\"\\n=== Sample Generated Reports and Ground Truth Reports ===\\n\")\n",
    "num_samples_to_show = 3\n",
    "for i in range(num_samples_to_show):\n",
    "    print(f\"[Ground Truth Report {i+1}]:\\n{all_ground_truth_texts[i]}\\n\")\n",
    "    print(f\"[Generated Report {i+1}]:\\n{all_generated_texts[i]}\\n\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46753/1402398462.py:58: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/1402398462.py:64: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/1402398462.py:71: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/avaghasiya/.conda/envs/rsna/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10:   0%|          | 0/18 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1/10: 100%|██████████| 18/18 [01:31<00:00,  5.07s/it, loss=1.7133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 2.3529\n",
      "Epoch 1 Validation Loss: 1.4922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 18/18 [01:33<00:00,  5.17s/it, loss=1.1058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 1.3353\n",
      "Epoch 2 Validation Loss: 1.1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 18/18 [01:31<00:00,  5.08s/it, loss=0.9250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.9782\n",
      "Epoch 3 Validation Loss: 0.8755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 18/18 [01:29<00:00,  4.96s/it, loss=0.8317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 0.8350\n",
      "Epoch 4 Validation Loss: 0.8740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 18/18 [01:30<00:00,  5.05s/it, loss=0.7283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training Loss: 0.7457\n",
      "Epoch 5 Validation Loss: 0.7147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 18/18 [01:32<00:00,  5.13s/it, loss=0.5431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training Loss: 0.5995\n",
      "Epoch 6 Validation Loss: 0.6499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 18/18 [01:30<00:00,  5.05s/it, loss=0.4917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training Loss: 0.5261\n",
      "Epoch 7 Validation Loss: 0.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 18/18 [01:30<00:00,  5.04s/it, loss=0.4824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training Loss: 0.4826\n",
      "Epoch 8 Validation Loss: 0.5784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 18/18 [01:31<00:00,  5.10s/it, loss=0.3822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training Loss: 0.4830\n",
      "Epoch 9 Validation Loss: 0.5370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 18/18 [01:31<00:00,  5.06s/it, loss=0.4842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training Loss: 0.4505\n",
      "Epoch 10 Validation Loss: 0.4799\n",
      "Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Generated Reports and Ground Truth Reports ===\n",
      "\n",
      "[Ground Truth Report 1]:\n",
      "[ Finding ] \n",
      " \n",
      "[ Diagnosis ] \n",
      "Rt. knee joint effusion \n",
      "Lt. accessory navicular bone, type II. \n",
      " \n",
      "LT. 1st MTP joint, bony erosion with periarticular osteopenia. \n",
      " \n",
      "Rt. 3rd finger DIP joint, bony erosion with soft tissue swelling. \n",
      "    \n",
      "--> inflammatory arthritis, such as RA \n",
      "       D/Dx. infectious arthritis. \n",
      " \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 1]:\n",
      "FINDING \n",
      "no bony lesion. A possible inflammatory arthritis in the wrist of Ltantoaxial joint, both calcaneus and plantar area with suspicious erosion at base or accessory bone type II Rt 2nd MT shaft . - suggestive CTE involvement --- ==> RA\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 2]:\n",
      "[FINDING       ] suspicious erosion at Lt 5th MTP joint \n",
      "-> r/o RA involvement   [CONCLUSION    ] suspicious erosion at Lt 5th MTP joint \n",
      "-> r/o RA involvement   [RECOMMENDATION] -\n",
      "\n",
      "[Generated Report 2]:\n",
      "FINDING \n",
      "No bony abnormality. No significant interval change or slightly progressed RA involvement since last study, both hands and feet intact with no erosion at Lt 5th MTP joint . Degenerative changes suggested in TMT inflammatory arthritis such as rheumatoid osteopen\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 3]:\n",
      "[ Finding ] \n",
      " \n",
      "[ Diagnosis ] \n",
      "more union in fracture of right 4ht proximal phalangeal bone base. \n",
      "no change of other finding. \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 3]:\n",
      "FINDING \n",
      "1. RA involvement, both feet and hands with erosion at Lt 5th MTP joint 2nd MT shaft --> C5/6 vs Rt 1st IPL head > OA postop> state 3rd to 4-10m old gout from fracture\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 4]:\n",
      "[ Finding ] \n",
      "minimal OA, both knee joints. \n",
      "Rt. knee soft tissue swelling and calcifications. \n",
      " \n",
      "Rt. ulnar styloid process, bony erosions \n",
      "  --> R/O RA \n",
      " \n",
      "[ Conclusion ] \n",
      "minimal OA, both knee joints. \n",
      "Rt. knee soft tissue swelling and calcifications. \n",
      " \n",
      "Rt. ulnar styloid process, bony erosions \n",
      "  --> R/O RA \n",
      " \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 4]:\n",
      "FINDING \n",
      "mild degenerative change. r/o RA involvement, most likely as an accessory fracture of Lt 1st MTP joint with soft tissue swelling and suspicious erosion at Rt 2nd MT head . - more evidence suggested bony erosions in both calcaneus\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 5]:\n",
      "[FINDING       ] erosions at both 5th MTP joint \n",
      "-> RA involvement, more likely \n",
      "\n",
      "mild degenerative change   [CONCLUSION    ] erosions at both 5th MTP joint \n",
      "-> RA involvement, more likely \n",
      "\n",
      "mild degenerative change   [RECOMMENDATION] -\n",
      "\n",
      "[Generated Report 5]:\n",
      "FINDING \n",
      "no significant bony lesion on radiographs. Achiells tendon insertion site, C-spines and TMT joints suggestive of BRCA1 type II bone involvement in both extensor hood region for possible osteopenia - most likely with RA involvements\n",
      "\n",
      "--------------------------------------------------\n",
      "=== Evaluation of Generated Reports ===\n",
      "Average BLEU Score (smoothed): 0.0133\n",
      "Average ROUGE-L Score:         0.1232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BERTScore F1: 0.8199\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import your model components and tokenizer ---\n",
    "from timm import create_model\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Use Albumentations for image augmentation (as in your reference code) ---\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# --- Set up the tokenizer ---\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# GPT-2 does not have a pad token so we use EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load your data (pkl file) ---\n",
    "with open('updated_merge_json_200x300.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# --- Balance the dataset ---\n",
    "task_classes = ['ra', 'normal']\n",
    "class_counts = {cls: 0 for cls in task_classes}\n",
    "data_by_class = {cls: [] for cls in task_classes}\n",
    "\n",
    "for key, entry in data.items():\n",
    "    # use lower-case class label and only include if image file exists\n",
    "    class_label = entry.get('class', '').lower()\n",
    "    if class_label in class_counts and os.path.exists(entry['file_path']):\n",
    "        class_counts[class_label] += 1\n",
    "        data_by_class[class_label].append(entry)\n",
    "\n",
    "min_class_count = min(class_counts.values())\n",
    "balanced_data = []\n",
    "for cls in task_classes:\n",
    "    balanced_data.extend(random.sample(data_by_class[cls], min_class_count))\n",
    "\n",
    "# Optionally, you may multiply the balanced_data for augmentation purposes:\n",
    "augmented_data = balanced_data * 2\n",
    "\n",
    "# --- Define Albumentations transforms ---\n",
    "train_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# For patch images we use a similar transform (you can adjust if needed)\n",
    "patch_tfms = A.Compose([\n",
    "    A.Resize(112, 112),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# --- Define a new Dataset for report generation ---\n",
    "class ReportDataset(Dataset):\n",
    "    def __init__(self, data, img_tfms, patch_tfms, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.img_tfms = img_tfms\n",
    "        self.patch_tfms = patch_tfms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        \n",
    "        # Load the main image\n",
    "        image = Image.open(entry['file_path']).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        augmented = self.img_tfms(image=image)\n",
    "        image_tensor = augmented['image']\n",
    "        \n",
    "        # Process patch images (if any)\n",
    "        patches = entry.get('bbx', [])\n",
    "        if len(patches) > 0:\n",
    "            # Limit to up to 34 patches (as before)\n",
    "            patch_imgs = [Image.fromarray(patch) for patch in patches[:34]]\n",
    "            patch_tensors = []\n",
    "            for p in patch_imgs:\n",
    "                p = np.array(p)\n",
    "                aug_patch = self.patch_tfms(image=p)\n",
    "                patch_tensors.append(aug_patch['image'])\n",
    "            # Concatenate along the channel dimension: resulting shape (34*3, 112, 112)\n",
    "            combined_patches = torch.cat(patch_tensors, dim=0)\n",
    "        else:\n",
    "            combined_patches = torch.zeros(34*3, 112, 112)\n",
    "        \n",
    "        # Process the report text (diagnosis field)\n",
    "        # Replace unwanted tokens and append EOS token (using tokenizer.eos_token)\n",
    "        report_text = entry['diagnosis'].replace('_x000D_', ' ').strip()\n",
    "        caption = f\"{report_text}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize without padding here (we will pad in the collate function)\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Squeeze to remove batch dimension (now shape: [seq_len])\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Create shifted labels for LM training (shift left by one)\n",
    "        # For example: if input_ids = [a, b, c, eos] then labels = [b, c, eos, eos]\n",
    "        labels = input_ids.clone()\n",
    "        if input_ids.size(0) > 1:\n",
    "            labels[:-1] = input_ids[1:]\n",
    "        # (Optionally, you can set the last token’s label to -100 so that loss is not computed)\n",
    "        # labels[-1] = -100\n",
    "        \n",
    "        return image_tensor, combined_patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Define a collate function to pad variable-length sequences ---\n",
    "def collate_fn(batch):\n",
    "    images, patches, input_ids_list, attn_masks, labels_list = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    patches = torch.stack(patches, dim=0)\n",
    "    \n",
    "    # Use the tokenizer's pad method to pad the input_ids and labels\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {'input_ids': list(input_ids_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = batch_encoding['input_ids']\n",
    "    \n",
    "    batch_encoding_labels = tokenizer.pad(\n",
    "        {'input_ids': list(labels_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = batch_encoding_labels['input_ids']\n",
    "    \n",
    "    # Pad attention masks similarly\n",
    "    batch_attn = tokenizer.pad(\n",
    "        {'input_ids': list(attn_masks)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    attention_mask = batch_attn['input_ids']\n",
    "    \n",
    "    # Replace pad token positions in labels with -100 so that loss is not computed on them\n",
    "    labels[ input_ids == tokenizer.pad_token_id ] = -100\n",
    "    \n",
    "    return images, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Create Dataset and DataLoaders ---\n",
    "full_dataset = ReportDataset(augmented_data, train_tfms, patch_tfms, tokenizer, max_length=128)\n",
    "\n",
    "# You can split into train/val/test (here we use 80/10/10 split)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "batch_size = 16  # adjust as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- Use your model definition (TwoBranchWithGPT2) unchanged ---\n",
    "# (Below is your unchanged model code; make sure it is defined exactly as you need)\n",
    "class TwoBranchWithGPT2(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(TwoBranchWithGPT2, self).__init__()\n",
    "        # -- SWIN Models --\n",
    "        self.swin_global = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_global.head = nn.Identity()\n",
    "        self.swin_patch = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_patch.head = nn.Identity()\n",
    "\n",
    "        # -- ResNet Models --\n",
    "        from torchvision import models\n",
    "        resnet_global = models.resnet50(pretrained=pretrained)\n",
    "        resnet_patch  = models.resnet50(pretrained=pretrained)\n",
    "        resnet_global.fc = nn.Identity()\n",
    "        resnet_patch.fc  = nn.Identity()\n",
    "        self.resnet_global = resnet_global\n",
    "        self.resnet_patch  = resnet_patch\n",
    "\n",
    "        # -- Convert patch channels from 102 to 3 --\n",
    "        self.patch_channel_reduction = nn.Conv2d(in_channels=102, out_channels=3, kernel_size=1)\n",
    "\n",
    "        # -- Merge & project features to GPT2 hidden size (768) --\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(5632, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # -- Classification head (unused in caption training) --\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # -- GPT-2 with cross-attention --\n",
    "        gpt2_config = GPT2Config.from_pretrained(\"gpt2\", add_cross_attention=True)\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2_config)\n",
    "        self.gpt2.resize_token_embeddings(len(tokenizer))\n",
    "        # Initialize cross-attention layers\n",
    "        for name, param in self.gpt2.named_parameters():\n",
    "            if 'crossattention' in name:\n",
    "                param.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "        # -- Prefix Projector --\n",
    "        self.prefix_length = 10  # can experiment with this\n",
    "        self.prefix_projector = nn.Linear(768, 768 * self.prefix_length)\n",
    "\n",
    "    def encode_features(self, images, patches):\n",
    "        # Resize patches to image size and reduce channels\n",
    "        patches_resized = F.interpolate(patches, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        patches_reduced = self.patch_channel_reduction(patches_resized)\n",
    "\n",
    "        swin_global_features = self.swin_global.forward_features(images).mean(dim=[1, 2])\n",
    "        swin_patch_features = self.swin_patch.forward_features(patches_reduced).mean(dim=[1, 2])\n",
    "        resnet_global_features = self.resnet_global(images)\n",
    "        resnet_patch_features = self.resnet_patch(patches_reduced)\n",
    "\n",
    "        combined_features = torch.cat([swin_global_features, swin_patch_features,\n",
    "                                         resnet_global_features, resnet_patch_features], dim=1)\n",
    "        projected_features = self.feature_attention(combined_features)\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "    def forward(self, images, patches, input_ids=None, attention_mask=None):\n",
    "        projected_features = self.encode_features(images, patches)\n",
    "        # We still compute classification output (unused in caption LM training)\n",
    "        cls_output = self.classifier(projected_features)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = projected_features.size(0)\n",
    "            encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "            encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "            # Teacher forcing mode: pass LM input tokens along with cross-attention\n",
    "            gpt_outputs = self.gpt2(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states\n",
    "            )\n",
    "            return cls_output, gpt_outputs.logits\n",
    "        else:\n",
    "            return cls_output\n",
    "\n",
    "    def generate_reports(self, projected_features, tokenizer, max_length=50, \n",
    "                           do_sample=True, top_k=50, top_p=0.95, temperature=0.7):\n",
    "        device = projected_features.device\n",
    "        batch_size = projected_features.size(0)\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "        encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "\n",
    "        input_ids = torch.full((batch_size, 1), bos_id, device=device, dtype=torch.long)\n",
    "        encoder_attention_mask = torch.ones((batch_size, self.prefix_length), device=device, dtype=torch.long)\n",
    "\n",
    "        generated_ids = self.gpt2.generate(\n",
    "            input_ids=input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=do_sample,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.2,\n",
    "            bos_token_id=bos_id,\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(seq.tolist(), skip_special_tokens=True) for seq in generated_ids]\n",
    "        return generated_texts\n",
    "\n",
    "# --- Set up device, model, loss, optimizer, and scheduler ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoBranchWithGPT2(pretrained=True).to(device)\n",
    "\n",
    "# We use the text generation loss only (CrossEntropyLoss)\n",
    "criterion_txt = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 10  # adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, patches, input_ids, attention_mask, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # We call the model in teacher-forcing mode.\n",
    "        _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "        # Flatten logits and labels for loss computation.\n",
    "        loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, patches, input_ids, attention_mask, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            patches = patches.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "            loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# --- Testing / Inference: Generate Reports and Print Ground Truth ---\n",
    "model.eval()\n",
    "all_generated_texts = []\n",
    "all_ground_truth_texts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, patches, input_ids, attention_mask, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        # Get the encoded image features for caption generation.\n",
    "        projected_features = model.encode_features(images, patches)\n",
    "        # Generate reports for the current batch.\n",
    "        gen_texts = model.generate_reports(projected_features, tokenizer, max_length=60)\n",
    "        all_generated_texts.extend(gen_texts)\n",
    "        \n",
    "        # Decode ground truth reports from input_ids.\n",
    "        # Note: We decode the first token sequence in each sample (removing padding tokens).\n",
    "        for seq in input_ids:\n",
    "            gt_text = tokenizer.decode(seq.tolist(), skip_special_tokens=True)\n",
    "            all_ground_truth_texts.append(gt_text)\n",
    "\n",
    "# Print a few sample generated reports along with the ground truth\n",
    "print(\"\\n=== Sample Generated Reports and Ground Truth Reports ===\\n\")\n",
    "num_samples_to_show = 5\n",
    "for i in range(num_samples_to_show):\n",
    "    print(f\"[Ground Truth Report {i+1}]:\\n{all_ground_truth_texts[i]}\\n\")\n",
    "    print(f\"[Generated Report {i+1}]:\\n{all_generated_texts[i]}\\n\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "def evaluate_generated_texts(ground_truth_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Evaluate generated texts using BLEU (with smoothing) and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    smoothing_fn = SmoothingFunction().method1  # Use smoothing to avoid zero counts for higher n-grams\n",
    "\n",
    "    for ref_text, gen_text in zip(ground_truth_texts, generated_texts):\n",
    "        # Tokenize by splitting on whitespace\n",
    "        ref_tokens = ref_text.split()\n",
    "        gen_tokens = gen_text.split()\n",
    "        # Calculate BLEU score for this sample with smoothing\n",
    "        bleu = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smoothing_fn)\n",
    "        bleu_scores.append(bleu)\n",
    "        try:\n",
    "            scores = rouge.get_scores(gen_text, ref_text)\n",
    "            rouge_l_scores.append(scores[0][\"rouge-l\"][\"f\"])\n",
    "        except ValueError:\n",
    "            rouge_l_scores.append(0.0)\n",
    "\n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "    avg_rouge = np.mean(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "    return avg_bleu, avg_rouge\n",
    "\n",
    "# Assuming you have collected all_ground_truth_texts and all_generated_texts during inference:\n",
    "avg_bleu, avg_rouge = evaluate_generated_texts(all_ground_truth_texts, all_generated_texts)\n",
    "\n",
    "print(\"=== Evaluation of Generated Reports ===\")\n",
    "print(f\"Average BLEU Score (smoothed): {avg_bleu:.4f}\")\n",
    "print(f\"Average ROUGE-L Score:         {avg_rouge:.4f}\")\n",
    "\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def evaluate_with_bertscore(generated_texts, ground_truth_texts):\n",
    "    results = bertscore_metric.compute(predictions=generated_texts, references=ground_truth_texts, lang=\"en\")\n",
    "    avg_f1 = np.mean(results[\"f1\"])\n",
    "    return avg_f1\n",
    "\n",
    "avg_bertscore = evaluate_with_bertscore(all_generated_texts, all_ground_truth_texts)\n",
    "print(f\"Average BERTScore F1: {avg_bertscore:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46753/47278737.py:58: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/47278737.py:64: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "/tmp/ipykernel_46753/47278737.py:71: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/20:   0%|          | 0/18 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1/20: 100%|██████████| 18/18 [01:30<00:00,  5.00s/it, loss=1.6995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 2.3149\n",
      "Epoch 1 Validation Loss: 1.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 18/18 [01:31<00:00,  5.11s/it, loss=1.3265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 1.2821\n",
      "Epoch 2 Validation Loss: 0.7419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 18/18 [01:30<00:00,  5.05s/it, loss=0.9980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.9197\n",
      "Epoch 3 Validation Loss: 0.4890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 18/18 [01:31<00:00,  5.10s/it, loss=0.7477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 0.7879\n",
      "Epoch 4 Validation Loss: 0.4499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 18/18 [01:31<00:00,  5.08s/it, loss=0.5928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training Loss: 0.6756\n",
      "Epoch 5 Validation Loss: 0.3632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 18/18 [01:32<00:00,  5.15s/it, loss=0.6343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training Loss: 0.5615\n",
      "Epoch 6 Validation Loss: 0.3296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 18/18 [01:30<00:00,  5.04s/it, loss=0.5875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training Loss: 0.4910\n",
      "Epoch 7 Validation Loss: 0.3130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 18/18 [01:31<00:00,  5.10s/it, loss=0.5441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training Loss: 0.4467\n",
      "Epoch 8 Validation Loss: 0.2957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 18/18 [01:32<00:00,  5.11s/it, loss=0.5347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training Loss: 0.4639\n",
      "Epoch 9 Validation Loss: 0.3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 18/18 [01:31<00:00,  5.09s/it, loss=0.3903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training Loss: 0.3968\n",
      "Epoch 10 Validation Loss: 0.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 18/18 [01:32<00:00,  5.11s/it, loss=0.3418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training Loss: 0.3535\n",
      "Epoch 11 Validation Loss: 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 18/18 [01:32<00:00,  5.15s/it, loss=0.3635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training Loss: 0.3019\n",
      "Epoch 12 Validation Loss: 0.2516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 18/18 [01:31<00:00,  5.08s/it, loss=0.2650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training Loss: 0.2691\n",
      "Epoch 13 Validation Loss: 0.2288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 18/18 [01:31<00:00,  5.09s/it, loss=0.2223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training Loss: 0.2437\n",
      "Epoch 14 Validation Loss: 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 18/18 [01:31<00:00,  5.09s/it, loss=0.2419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training Loss: 0.2302\n",
      "Epoch 15 Validation Loss: 0.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 18/18 [01:32<00:00,  5.12s/it, loss=0.2365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training Loss: 0.2165\n",
      "Epoch 16 Validation Loss: 0.2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 18/18 [01:30<00:00,  5.01s/it, loss=0.1750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training Loss: 0.2025\n",
      "Epoch 17 Validation Loss: 0.2137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 18/18 [01:32<00:00,  5.15s/it, loss=0.3156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training Loss: 0.2205\n",
      "Epoch 18 Validation Loss: 0.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 18/18 [01:30<00:00,  5.05s/it, loss=0.2361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training Loss: 0.2138\n",
      "Epoch 19 Validation Loss: 0.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 18/18 [01:31<00:00,  5.10s/it, loss=0.1771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training Loss: 0.2116\n",
      "Epoch 20 Validation Loss: 0.2275\n",
      "Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Generated Reports and Ground Truth Reports ===\n",
      "\n",
      "[Ground Truth Report 1]:\n",
      "[ Finding ] \n",
      " \n",
      "[ Diagnosis ] \n",
      "1. Two separate ossicles with sclerotic margin in right medial hallux sessamoid bone. \n",
      " - Old fracture or bipartite sessamoid bone. \n",
      " REC) Clinical correlation. \n",
      "2. Mild soft tissue swelling in medial portion of right foot. \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 1]:\n",
      " Finding ] \n",
      "no bony lesion. [ Diagnosis - IID or XXL joint, left] no significant body abnormality on radiographs of rt 2nd MT shaft since last study in 2018-11/14 . ---> possible RA involvement with multiple inflammatory arthritis such as\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 2]:\n",
      "[FINDING       ] joint space narrowing, possible erosions, Rt TMT joints \n",
      "-> r/o RA involvement \n",
      "\n",
      "degenerative change, talonavicular joint, Rt \n",
      "\n",
      "flat foot, Rt   [CONCLUSION    ] joint space narrowing, possible erosions, Rt TMT joints \n",
      "-> r/o RA involvement \n",
      "\n",
      "degenerative change, talonavicular joint, Rt \n",
      "\n",
      "flat foot, Rt   [RECOMMENDATION] -\n",
      "\n",
      "[Generated Report 2]:\n",
      "FINDING \n",
      "no bony lesion. R/O RA involvement, suggested - more likely [CONCLUSION] no B-Rt inflammatory arthritis in rhesus macaques or F5 mice  MTP joint , type 2     union state  -> probable erosions at\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 3]:\n",
      "[ Finding ] \n",
      "Rt. TMT joint, R/O RA. \n",
      " \n",
      "pes cavus both. \n",
      "[ Conclusion ] \n",
      "Rt. TMT joint, R/O RA. \n",
      " \n",
      "pes cavus both. \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 3]:\n",
      " Finding ] \n",
      "both hands and wrist, RA progression. Lt 5th MTP joint bony erosion with suspiscious high opacity lesions around Rt 1st PP or IPL head . - Old fracture at lat side of TMT bone for soft tissue swelling suggestive OF OA involvement in\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 4]:\n",
      "[ Finding ] \n",
      "no bony lesion. \n",
      "[ Conclusion ] \n",
      "no bony lesion. \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 4]:\n",
      " Finding ] \n",
      "no bony lesion. [ Diagnosis - II with joint space narrowing, bone erosions at both 3rd and 5th MTP joints, left] --- D/Dx since 2018-11) , RA involvement[ Recommend ]\n",
      "\n",
      "--------------------------------------------------\n",
      "[Ground Truth Report 5]:\n",
      "[ Finding ] \n",
      " \n",
      "[ Conclusion ] \n",
      "Both accessory navicula, type 1 \n",
      "[ Recommend ]\n",
      "\n",
      "[Generated Report 5]:\n",
      " Finding ] \n",
      "no bony lesion. [ Diagnosis - II of no significant body abnormality, OA --- BMD > 1st MTP joint] No change to Rt 5th and Lt 4THMVP joints since last study; RA involvement suggested.] Dental dams\n",
      "\n",
      "--------------------------------------------------\n",
      "=== Evaluation of Generated Reports ===\n",
      "Average BLEU Score (smoothed): 0.0236\n",
      "Average ROUGE-L Score:         0.2423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BERTScore F1: 0.8320\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Import your model components and tokenizer ---\n",
    "from timm import create_model\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# --- Use Albumentations for image augmentation (as in your reference code) ---\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# --- Set up the tokenizer ---\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# GPT-2 does not have a pad token so we use EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Load your data (pkl file) ---\n",
    "with open('updated_merge_json_200x300.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# --- Balance the dataset ---\n",
    "task_classes = ['ra', 'normal']\n",
    "class_counts = {cls: 0 for cls in task_classes}\n",
    "data_by_class = {cls: [] for cls in task_classes}\n",
    "\n",
    "for key, entry in data.items():\n",
    "    # use lower-case class label and only include if image file exists\n",
    "    class_label = entry.get('class', '').lower()\n",
    "    if class_label in class_counts and os.path.exists(entry['file_path']):\n",
    "        class_counts[class_label] += 1\n",
    "        data_by_class[class_label].append(entry)\n",
    "\n",
    "min_class_count = min(class_counts.values())\n",
    "balanced_data = []\n",
    "for cls in task_classes:\n",
    "    balanced_data.extend(random.sample(data_by_class[cls], min_class_count))\n",
    "\n",
    "# Optionally, you may multiply the balanced_data for augmentation purposes:\n",
    "augmented_data = balanced_data * 2\n",
    "\n",
    "# --- Define Albumentations transforms ---\n",
    "train_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_tfms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# For patch images we use a similar transform (you can adjust if needed)\n",
    "patch_tfms = A.Compose([\n",
    "    A.Resize(112, 112),\n",
    "    A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], always_apply=True),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# --- Define a new Dataset for report generation ---\n",
    "class ReportDataset(Dataset):\n",
    "    def __init__(self, data, img_tfms, patch_tfms, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.img_tfms = img_tfms\n",
    "        self.patch_tfms = patch_tfms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        \n",
    "        # Load the main image\n",
    "        image = Image.open(entry['file_path']).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        augmented = self.img_tfms(image=image)\n",
    "        image_tensor = augmented['image']\n",
    "        \n",
    "        # Process patch images (if any)\n",
    "        patches = entry.get('bbx', [])\n",
    "        if len(patches) > 0:\n",
    "            # Limit to up to 34 patches (as before)\n",
    "            patch_imgs = [Image.fromarray(patch) for patch in patches[:34]]\n",
    "            patch_tensors = []\n",
    "            for p in patch_imgs:\n",
    "                p = np.array(p)\n",
    "                aug_patch = self.patch_tfms(image=p)\n",
    "                patch_tensors.append(aug_patch['image'])\n",
    "            # Concatenate along the channel dimension: resulting shape (34*3, 112, 112)\n",
    "            combined_patches = torch.cat(patch_tensors, dim=0)\n",
    "        else:\n",
    "            combined_patches = torch.zeros(34*3, 112, 112)\n",
    "        \n",
    "        # Process the report text (diagnosis field)\n",
    "        # Replace unwanted tokens and append EOS token (using tokenizer.eos_token)\n",
    "        report_text = entry['diagnosis'].replace('_x000D_', ' ').strip()\n",
    "        caption = f\"{report_text}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize without padding here (we will pad in the collate function)\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Squeeze to remove batch dimension (now shape: [seq_len])\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Create shifted labels for LM training (shift left by one)\n",
    "        # For example: if input_ids = [a, b, c, eos] then labels = [b, c, eos, eos]\n",
    "        labels = input_ids.clone()\n",
    "        if input_ids.size(0) > 1:\n",
    "            labels[:-1] = input_ids[1:]\n",
    "        # (Optionally, you can set the last token’s label to -100 so that loss is not computed)\n",
    "        # labels[-1] = -100\n",
    "        \n",
    "        return image_tensor, combined_patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Define a collate function to pad variable-length sequences ---\n",
    "def collate_fn(batch):\n",
    "    images, patches, input_ids_list, attn_masks, labels_list = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    patches = torch.stack(patches, dim=0)\n",
    "    \n",
    "    # Use the tokenizer's pad method to pad the input_ids and labels\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {'input_ids': list(input_ids_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = batch_encoding['input_ids']\n",
    "    \n",
    "    batch_encoding_labels = tokenizer.pad(\n",
    "        {'input_ids': list(labels_list)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = batch_encoding_labels['input_ids']\n",
    "    \n",
    "    # Pad attention masks similarly\n",
    "    batch_attn = tokenizer.pad(\n",
    "        {'input_ids': list(attn_masks)},\n",
    "        padding='longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    attention_mask = batch_attn['input_ids']\n",
    "    \n",
    "    # Replace pad token positions in labels with -100 so that loss is not computed on them\n",
    "    labels[ input_ids == tokenizer.pad_token_id ] = -100\n",
    "    \n",
    "    return images, patches, input_ids, attention_mask, labels\n",
    "\n",
    "# --- Create Dataset and DataLoaders ---\n",
    "full_dataset = ReportDataset(augmented_data, train_tfms, patch_tfms, tokenizer, max_length=128)\n",
    "\n",
    "# You can split into train/val/test (here we use 80/10/10 split)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "batch_size = 16  # adjust as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- Use your model definition (TwoBranchWithGPT2) unchanged ---\n",
    "# (Below is your unchanged model code; make sure it is defined exactly as you need)\n",
    "class TwoBranchWithGPT2(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(TwoBranchWithGPT2, self).__init__()\n",
    "        # -- SWIN Models --\n",
    "        self.swin_global = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_global.head = nn.Identity()\n",
    "        self.swin_patch = create_model('swin_tiny_patch4_window7_224', pretrained=pretrained)\n",
    "        self.swin_patch.head = nn.Identity()\n",
    "\n",
    "        # -- ResNet Models --\n",
    "        from torchvision import models\n",
    "        resnet_global = models.resnet50(pretrained=pretrained)\n",
    "        resnet_patch  = models.resnet50(pretrained=pretrained)\n",
    "        resnet_global.fc = nn.Identity()\n",
    "        resnet_patch.fc  = nn.Identity()\n",
    "        self.resnet_global = resnet_global\n",
    "        self.resnet_patch  = resnet_patch\n",
    "\n",
    "        # -- Convert patch channels from 102 to 3 --\n",
    "        self.patch_channel_reduction = nn.Conv2d(in_channels=102, out_channels=3, kernel_size=1)\n",
    "\n",
    "        # -- Merge & project features to GPT2 hidden size (768) --\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(5632, 768),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # -- Classification head (unused in caption training) --\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # -- GPT-2 with cross-attention --\n",
    "        gpt2_config = GPT2Config.from_pretrained(\"gpt2\", add_cross_attention=True)\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=gpt2_config)\n",
    "        self.gpt2.resize_token_embeddings(len(tokenizer))\n",
    "        # Initialize cross-attention layers\n",
    "        for name, param in self.gpt2.named_parameters():\n",
    "            if 'crossattention' in name:\n",
    "                param.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "        # -- Prefix Projector --\n",
    "        self.prefix_length = 10  # can experiment with this\n",
    "        self.prefix_projector = nn.Linear(768, 768 * self.prefix_length)\n",
    "\n",
    "    def encode_features(self, images, patches):\n",
    "        # Resize patches to image size and reduce channels\n",
    "        patches_resized = F.interpolate(patches, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        patches_reduced = self.patch_channel_reduction(patches_resized)\n",
    "\n",
    "        swin_global_features = self.swin_global.forward_features(images).mean(dim=[1, 2])\n",
    "        swin_patch_features = self.swin_patch.forward_features(patches_reduced).mean(dim=[1, 2])\n",
    "        resnet_global_features = self.resnet_global(images)\n",
    "        resnet_patch_features = self.resnet_patch(patches_reduced)\n",
    "\n",
    "        combined_features = torch.cat([swin_global_features, swin_patch_features,\n",
    "                                         resnet_global_features, resnet_patch_features], dim=1)\n",
    "        projected_features = self.feature_attention(combined_features)\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        return projected_features\n",
    "\n",
    "    def forward(self, images, patches, input_ids=None, attention_mask=None):\n",
    "        projected_features = self.encode_features(images, patches)\n",
    "        # We still compute classification output (unused in caption LM training)\n",
    "        cls_output = self.classifier(projected_features)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = projected_features.size(0)\n",
    "            encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "            encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "            # Teacher forcing mode: pass LM input tokens along with cross-attention\n",
    "            gpt_outputs = self.gpt2(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states\n",
    "            )\n",
    "            return cls_output, gpt_outputs.logits\n",
    "        else:\n",
    "            return cls_output\n",
    "\n",
    "    def generate_reports(self, projected_features, tokenizer, max_length=50, \n",
    "                           do_sample=True, top_k=50, top_p=0.95, temperature=0.7):\n",
    "        device = projected_features.device\n",
    "        batch_size = projected_features.size(0)\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        eos_id = tokenizer.eos_token_id\n",
    "\n",
    "        projected_features = F.normalize(projected_features, dim=-1)\n",
    "        encoder_hidden_states = self.prefix_projector(projected_features)\n",
    "        encoder_hidden_states = encoder_hidden_states.view(batch_size, self.prefix_length, 768)\n",
    "\n",
    "        input_ids = torch.full((batch_size, 1), bos_id, device=device, dtype=torch.long)\n",
    "        encoder_attention_mask = torch.ones((batch_size, self.prefix_length), device=device, dtype=torch.long)\n",
    "\n",
    "        generated_ids = self.gpt2.generate(\n",
    "            input_ids=input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=do_sample,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=1.2,\n",
    "            bos_token_id=bos_id,\n",
    "            eos_token_id=eos_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_texts = [tokenizer.decode(seq.tolist(), skip_special_tokens=True) for seq in generated_ids]\n",
    "        return generated_texts\n",
    "\n",
    "# --- Set up device, model, loss, optimizer, and scheduler ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TwoBranchWithGPT2(pretrained=True).to(device)\n",
    "\n",
    "# We use the text generation loss only (CrossEntropyLoss)\n",
    "criterion_txt = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 20  # adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, patches, input_ids, attention_mask, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # We call the model in teacher-forcing mode.\n",
    "        _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "        # Flatten logits and labels for loss computation.\n",
    "        loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, patches, input_ids, attention_mask, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            patches = patches.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            _, gpt_logits = model(images, patches, input_ids, attention_mask)\n",
    "            loss = criterion_txt(gpt_logits.reshape(-1, gpt_logits.size(-1)), labels.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# --- Testing / Inference: Generate Reports and Print Ground Truth ---\n",
    "model.eval()\n",
    "all_generated_texts = []\n",
    "all_ground_truth_texts = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, patches, input_ids, attention_mask, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        patches = patches.to(device)\n",
    "        # Get the encoded image features for caption generation.\n",
    "        projected_features = model.encode_features(images, patches)\n",
    "        # Generate reports for the current batch.\n",
    "        gen_texts = model.generate_reports(projected_features, tokenizer, max_length=60)\n",
    "        all_generated_texts.extend(gen_texts)\n",
    "        \n",
    "        # Decode ground truth reports from input_ids.\n",
    "        # Note: We decode the first token sequence in each sample (removing padding tokens).\n",
    "        for seq in input_ids:\n",
    "            gt_text = tokenizer.decode(seq.tolist(), skip_special_tokens=True)\n",
    "            all_ground_truth_texts.append(gt_text)\n",
    "\n",
    "# Print a few sample generated reports along with the ground truth\n",
    "print(\"\\n=== Sample Generated Reports and Ground Truth Reports ===\\n\")\n",
    "num_samples_to_show = 5\n",
    "for i in range(num_samples_to_show):\n",
    "    print(f\"[Ground Truth Report {i+1}]:\\n{all_ground_truth_texts[i]}\\n\")\n",
    "    print(f\"[Generated Report {i+1}]:\\n{all_generated_texts[i]}\\n\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "def evaluate_generated_texts(ground_truth_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Evaluate generated texts using BLEU (with smoothing) and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    smoothing_fn = SmoothingFunction().method1  # Use smoothing to avoid zero counts for higher n-grams\n",
    "\n",
    "    for ref_text, gen_text in zip(ground_truth_texts, generated_texts):\n",
    "        # Tokenize by splitting on whitespace\n",
    "        ref_tokens = ref_text.split()\n",
    "        gen_tokens = gen_text.split()\n",
    "        # Calculate BLEU score for this sample with smoothing\n",
    "        bleu = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smoothing_fn)\n",
    "        bleu_scores.append(bleu)\n",
    "        try:\n",
    "            scores = rouge.get_scores(gen_text, ref_text)\n",
    "            rouge_l_scores.append(scores[0][\"rouge-l\"][\"f\"])\n",
    "        except ValueError:\n",
    "            rouge_l_scores.append(0.0)\n",
    "\n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "    avg_rouge = np.mean(rouge_l_scores) if rouge_l_scores else 0.0\n",
    "    return avg_bleu, avg_rouge\n",
    "\n",
    "# Assuming you have collected all_ground_truth_texts and all_generated_texts during inference:\n",
    "avg_bleu, avg_rouge = evaluate_generated_texts(all_ground_truth_texts, all_generated_texts)\n",
    "\n",
    "print(\"=== Evaluation of Generated Reports ===\")\n",
    "print(f\"Average BLEU Score (smoothed): {avg_bleu:.4f}\")\n",
    "print(f\"Average ROUGE-L Score:         {avg_rouge:.4f}\")\n",
    "\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def evaluate_with_bertscore(generated_texts, ground_truth_texts):\n",
    "    results = bertscore_metric.compute(predictions=generated_texts, references=ground_truth_texts, lang=\"en\")\n",
    "    avg_f1 = np.mean(results[\"f1\"])\n",
    "    return avg_f1\n",
    "\n",
    "avg_bertscore = evaluate_with_bertscore(all_generated_texts, all_ground_truth_texts)\n",
    "print(f\"Average BERTScore F1: {avg_bertscore:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
